{
  "hash": "24140454d3605550f0290d5846cc9d9f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Advance Linear Algebra with Python - Part I\"\nauthor: \"Nodar Okroshiashvili\"\ndate: \"2021-04-21\"\ndate-modified: \"2025-10-01\"\ncategories: [Mathematics, Python]\ntags: [Mathematics, Eigenvalues and Eigenvectors, Gauss and Gauss-Jordan Elimination, Image and Kernel, Python]\n\nabstract: |\n    Part 5 of the linear algebra series with Python, that covers operations such as Basis, Ranks, Gauss and Gauss-Jordan elimination, eigenvalues and eigenvectors.\n\nimage: \"linear_algebra.jpg\"\n---\n\n# Introduction\n\n\n1. [Introduction to Linear Algebra with Python](linear_algebra_1.qmd)\n2. [Basic Linear Algebra with Python](linear_algebra_2.qmd)\n3. Intermediate linear algebra\n   1. [Intermediate Linear Algebra with Python - Part I](linear_algebra_3.qmd)\n   2. [Intermediate Linear Algebra with Python - Part II](linear_algebra_4.qmd)\n4. **Advanced linear algebra**\n   1. **Advance Linear Algebra with Python - Part I**\n   2. Advance Linear Algebra with Python - Part II\n\n\nIn this post I start with introducing advanced parts of the vector operations and then continue with advanced matrix operations.\n\n\n## Vector\n\n\n### Basis Vectors\n\nIn the [basics](https://dsfabric.org/articles/mathematics/basics-of-linear-algebra.html), we saw what is a unit vector.\nTo refresh, the unit vector is the vector with length 1 and the formula is\n\n$$\n\\hat{X} = \\frac{X}{\\|X\\|}\n$$\n\nFor farther explanation, unit vectors can be used to represent the axes of a [Cartesian coordinate system](https://en.wikipedia.org/wiki/Cartesian_coordinate_system).\nFor example in a three-dimensional Cartesian coordinate system such vectors are:\n\n$$\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{k} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n$$\n\nwhich represents, $x$, $y$, and $z$ axes, respectively. For two dimensional space we have\n\n$$\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n$$\n\nLet deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions.\nImagine, we have vector space or collection of vectors $\\vec{V}$ over the Cartesian coordinate system.\nThis space includes all two-dimensional vectors, or in other words, vectors with only two elements, $x$, and $y$.\n\n> A **basis**, call it $B$, of vector space $V$ over the Cartesian coordinate system is a linearly independent subset of $V$ that spans whole vector space $V$. To be precise, basis $B$ to be the basis it must satisfy two conditions:\n\n* Linearly independence property - states that all vectors in $B$ are linearly independent\n\n* The spanning property - states that $B$ spans whole $V$\n\nWe can combine these two conditions in one sentence. $B$ is the basis if its all elements are linearly independent and every\nelement of $V$ is a linear combination of elements of $B$.\n\nFrom these conditions, we can conclude that unit vectors $\\hat{i}$ and $\\hat{j}$ are the basis of $\\mathbb{R^2}$.\nThis kind of bases are also called **standard basis** or **natural basis**. The standard basis are denoted\nby $e_{1}$, $e_{2}$, $e_{3}$ and so on. I will be consistent and use the later notation for standard basis\nand $\\hat{i}$, $\\hat{j}$ and $\\hat{k}$ for unit vectors.\n\nThese standard basis vectors are the basis in the sense that any other vector in $V$ can be expressed uniquely\nas a linear combination of these unit vectors. For example, every vector $v$ in two-dimensional space can be written as\n\n$$\nx\\ e_{1} + y\\ e_{2}\n$$\n\nwhere $e_{1}$ and $e_{2}$ are unit vectors and $x$ and $y$ are scalar components or elements of the vector $v$.\n\nNow, to generalize the idea for higher dimensions we just have to apply the same logic as above,\nfor $\\mathbb{R^3}$ and more. In $\\mathbb{R^3}$ we have standard basis vectors $e_{1}$, $e_{2}$, $e_{3}$,\nand generally for $\\mathbb{R^n}$ we have standard basis vector space\n\n$$\nE =\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2} \\\\\n\\cdots \\\\\ne_{n}\n\\end{bmatrix}\n$$\n\nTo generalize the definition of the basis further, let consider the following:\n\n**If elements $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ of $V$ generate $V$ and in addition they are linearly independent, then $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ is called a basis of $V$. We shall say that the elements $v_{1}, v_{2},\\cdots,v_{n}$ constitute or form a basis of V.**\nVector space $V$ can have several basis.\n\nAt this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I\nsolved some examples by hand. I'll show you how to compute basis after explaining row-echelon and reduced row-echelon\nforms and you'll understand it. However, it's not enough only to know how to row-reduce the given matrix.\nIt's necessary to know which basis you want. Either column space or row space basis or the basis for nullspace.\nThese notions are explained below and after that, we can find the basis for each of them.\n\n\n## Matrix\n\n\n### Gaussian Elimination of a Matrix\n\nIn linear algebra, Gaussian Elimination is the method to solve the system of linear equations.\nThis method is the sequence of operations performed on the coefficient matrix of the system.\nExcept for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix.\n\nAnd what is the sequence of operations?\n\nUnder this notion, elementary row operations are meant. We've covered it in the previous post but for the refresher, ERO's are:\n\n* Interchange rows\n\n* Multiply each element in a row by a non-zero number\n\n* Multiply a row by a non-zero number and add the result to another row\n\nPerforming Gaussian elimination results in the matrix in **Row Echelon Form**.\nThe matrix is said to be in row echelon form if it satisfies the following conditions:\n\n* The first non-zero element in each row, called the leading entry, is a 1\n\n* Each leading entry is in a column, which is the right side of the leading entry in the previous row\n\n* Below the leading entry in a column, all other entries are zero\n\nTo catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true),\nwe have the system of linear equations in the following form:\n\n$$\n\\begin{cases}\nx + 2y - z = 5\\\\\n3x + y - 2z = 9\\\\\n-x + 4y + 2z = 0\n\\end{cases}\n$$\n\nBased on these equations we can form the following matrix\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n$$\n\nThis matrix is called **coefficient matrix** as it contains the coefficients of the linear equations.\nHaving the coefficient matrix, we can rewrite our system in the following form:\n\n$$\nAx = b\n$$\n\nWhere $A$ is the coefficient matrix, $x$ is the vector of the unknowns, and $b$ is the vector of the right-hand side components\n\nTo solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO's. This matrix is:\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix} = [A | b]\n$$\n\nwhich is called **augmented matrix**, which in turn gives us the possibility to perform ELO's, in other words,\nwe do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the\nresulted matrix gives the solution to our system of equations.\n\nLet do it by hand. We have the initial system\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n-x + 4y + 2z = 0\n\\end{cases}\n$$\n\nThen, using ERO's\n\n1. $R3 \\rightarrow R3 + R1$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n6y + z = 5\n\\end{cases}\n$$\n\n2. $R2 \\rightarrow R2 - 3R1$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & -5 & 1 & |& -6 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n-5y + z = -6 \\\\\n6y + z = 5\n\\end{cases}\n$$\n\n3. $R2 \\rightarrow R2 + R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n6y + z = 5\n\\end{cases}\n$$\n\n4. $R3 \\rightarrow R3 - 6R2$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & -11 & |& 11\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n-11z = 11\n\\end{cases}\n$$\n\n5. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\quad (A)\\\\\ny + 2z = -1 \\quad (B)\\\\\nz = -1 \\quad (C)\n\\end{cases}\n$$\n\n6. Back substitution\n\n$$\n\\begin{cases}\n(C) \\quad z = -1 \\\\\n(B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\\n(A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2\n\\end{cases}\n$$\n\n7. Solution\n\n$$\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n$$\n\nThis is the solution of the initial system, as well as the last system and every intermediate system.\nThe matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions.\n\n**Note that, starting with a particular matrix, a different sequence of ERO's can lead to different row echelon form**\n\n\n### Gauss-Jordan Elimination of a Matrix\n\nGaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to\nreduce it to row echelon form. Once it's done we perform back substitution to find the solution. However, we can\ncontinue performing ERO's to reduce coefficient matrix farther, to produce **Reduced Row Echelon Form**.\nThe matrix is in reduced row echelon form if it satisfies the following conditions:\n\n* It is in row echelon form\n\n* The leading entry in each row is the only non-zero entry in its column\n\nGauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the\ntop down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon\nForm matrix is the result of Gauss-Jordan Elimination process.\n\nWe can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had\n\n1. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\nz = -1\n\\end{cases}\n$$\n\nNow, from bottom to up we perform the following ERO's\n\n6. $R2 \\rightarrow R2 - 2R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n$$\n\n7. $R1 \\rightarrow R1 + R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & 0 & |& 4 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y = 4 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n$$\n\n8. $R1 \\rightarrow R1 - 2R2$\n\n$$\n\\begin{bmatrix}\n1 & 0 & 0 & |& 2 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx = 2 \\\\\ny = 1 \\\\\nz = -1\n\\end{cases}\n$$\n\nThe solution is\n\n$$\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n$$\n\nand this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix $A$.\n\n\n### The Inverse of a Matrix Using Gauss-Jordan Elimination\n\nSuppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned\nin the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix.\nTo be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is\n\n$$\nA =\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n$$\n\nTo find the inverse of $A$, we need to augment $A$ by the identity matrix $I$ which has the same dimensions as $A$.\nIt is a must the identity to have the same dimensions. After augmentation we have\n\n$$\n[A | I] =\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0 \\\\\n3 & 1 & -2 & |& 0 & 1 & 0 \\\\\n-1 & 4 & 2 & |& 0 & 0 & 1\n\\end{bmatrix}\n$$\n\nWe have to perform elementary row operations in the same way as we did in the above example. Particularly,\n\n1. $R3 \\rightarrow R3 + R1$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n3 & 1 & -2 & |& 0 & 1 & 0\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n$$\n\n2. $R2 \\rightarrow R2 - 3R1$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & -5 & 1 & |& -3 & 1 & -3\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n$$\n\n3. $R2 \\rightarrow R2 + R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n$$\n\n4. $R3 \\rightarrow R3 - 6R2$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & -11 & |& 13 & -6 & 13\n\\end{bmatrix}\n$$\n\n5. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n$$\n\n6. $R2 \\rightarrow R2 - 2R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n$$\n\n7. $R1 \\rightarrow R1 + R3$\n\n$$\n\\begin{bmatrix}\n1 & 2 & 0 & |& -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n$$\n\n8. $R1 \\rightarrow R1 - 2R2$\n\n$$\n\\begin{bmatrix}\n1 & 0 & 0 & |& -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n$$\n\nOur inverse of $A$ is\n\n$$\nA^{-1} =\n\\begin{bmatrix}\n-\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n\\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n-\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n$$\n\n\n### Image of a Matrix\n\nLet $A$ be $m\\times n$ matrix. Space spanned by its column vectors are called range, image, or column space of a matrix $A$.\nThe row space is defined similarly. I only consider column space as all the logic is the same for row space.\n\nThe precise definition is the following:\n\nLet $A$ be an $m\\times n$ matrix, with column vectors $v_{1}, v_{2}, \\cdots, v_{n}$. A linear combination of these\nvectors is any vector of the following form: $c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}$, where $c_{1}, c_{2}, \\cdots , c_{n}$ are scalars.\nThe set of all possible linear combinations of $v_{1}, v_{2}, \\cdots , v_{n}$ is called the column space of $A$.\n\nFor example:\n\n$$\nA =\n\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n2 & 0\n\\end{bmatrix}\n$$\n\nColumn vectors are:\n\n$$\nv_{1} =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix}\n\\quad\nv_{2} =\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix}\n$$\n\nA linear combination of $v_{1}$ and $v_{2}$ is any vector of the form\n\n$$\nc_{1}\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix} + c_{2}\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\nc_{1}\\\\\nc_{2}\\\\\n2c_{1}\n\\end{bmatrix}\n$$\n\nThe set of all such vectors is the column space of $A$.\n\n\n### Kernel of a Matrix\n\nIn linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system:\n\n$$A\\cdot X = 0$$\n\nwhere $A$ is a $m\\times n$ matrix and $X$ is a $m\\times 1$ vector and is denoted by $Ker(A)$.\n\nFor more clarity, let consider the numerical example. Lat our matrix $A$ be the following:\n\n$$\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n$$\n\nand our $X$ is\n\n$$\nX =\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix}\n$$\n\nWe have to form the following system:\n\n$$\nA\\cdot X =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n$$\n\nAfter that, we have to put this system into row-echelon or reduced row-echelon form.\nLet skip detailed calculation and present only results, which is the last matrix.\n\n$$\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3\\\\\n-4 & -2 & 2 & -2\\\\\n-1 & 7 & 3 & 2\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n-4 & -2 & 2 & -2\\\\\n2 & 7 & 1 & 3\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & -30 & -10 & -10\\\\\n0 & 21 & 7 & 7\\\\\n0 & -12 & -4 & -4\n\\end{bmatrix}\n$$\n$$\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n\nNow, to find the kernel of the original matrix $A$, we have to solve the following system of equations:\n\n$$\n\\begin{cases}\nx_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\\n        3x_{2} + x_{3} + x_{4} = 0\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4}\n\\end{cases}\n$$\n\nFrom this solution we conclude that the kernel of $A$ is\n\n$$\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n$$\n\nWhere, $x_{3}$ and $x_{4}$ are free variables and can be any number in $R$\n\nNote, that both original matrix $A$ and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space.\n\n::: {#5cbff8fe .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.linalg import null_space\nfrom sympy import Matrix\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#07a11f7c .cell execution_count=2}\n``` {.python .cell-code}\nA = np.array([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\n# This matrix is normalized, meaning that it has unit length\nkernel_A = null_space(A)\nprint(\"Normalized Kernel\", kernel_A, sep=\"\\n\")\n\n# To find unnormalized kernel we have to do the following:\nB = Matrix([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\nkernel_B = B.nullspace()\nprint(\"Unnormalized Kernel\", kernel_B[0], sep=\"\\n\")\n\n\n# In unnormilized case, we clearly see that Sympy automatically choose values for our free variables.\n# In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1\n# Resulted vector(s) are basis for the null space for our matrix A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormalized Kernel\n[[-0.58983053  0.07100086]\n [ 0.14709576  0.39348879]\n [-0.73692629 -0.32248793]\n [ 0.29563901 -0.85797843]]\nUnnormalized Kernel\nMatrix([[2/3], [-1/3], [1], [0]])\n```\n:::\n:::\n\n\n### Rank of a Matrix\n\nIn the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant\nof sub-matrix of the original matrix shows its non-degenerateness. In other words, nonzero determinant gives us information\nabout the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix.\nAfter reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that\nindeed there are other ways to find the determinant of a matrix as well as the rank of the matrix.\nTo keep [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself), here I only consider a numerical example. The code is provided in the intermediate tutorial.\n\nSuppose we have matrix $A$ in the following form:\n\n$$\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 &- 3\n\\end{bmatrix}\n$$\n\nPerform Elementary Row Operations we get reduced-echelon form:\n\n$$\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 & -3\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n3 & 2 & -1\\\\\n2 & -3 & -5\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & -10 & -10\\\\\n0 & -11 & -11\n\\end{bmatrix}\n$$\n$$\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & -11 & -11\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n\nFrom the last matrix we see that the nonzero determinant only exists in $2\\times2$ sub-matrices, hence rank of the matrix $A$ is 2.\n\n\n### Find the Basis of a Matrix\n\nNow we are able to find the basis for column space and row space as well as the basis for the kernel.\nThe columns of a matrix $A$ span the column space but they may not form a basis if the column vectors are linearly dependent.\nIf this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix $A$ to reduced row-echelon form.\n\nFor example:\n\n$$\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n$$\n\nRow reduced form of $A$ is:\n\n$$\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n\nWe see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix $A$ form the basis, which is:\n\n$$\n\\begin{bmatrix}\n2 \\\\-4\\\\-1\\\\-2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n7\\\\-2 \\\\ 7 \\\\ 2\n\\end{bmatrix}\n$$\n\nTo find the basis for row space, let consider different matrix and again let it be $A$.\n\n$$\nA =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n2 & 7 & 4 \\\\\n1 & 5 & 2\n\\end{bmatrix}\n$$\n\nTo reduce $A$ to reduced row-echelon form we have:\n\n$$\nB =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n\nAs in column space case, we see that linearly independent, nonzero row vectors are\n\n$$\n\\begin{bmatrix}\n1 \\\\0\\\\2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n0\\\\1 \\\\ 0\n\\end{bmatrix}\n$$\n\nTo find the basis for kernel let consider our old example. In this case our matrix is:\n\n$$\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n$$\n\nAnd its row reduced form is\n\n$$\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n\nWe solved this and got the following result:\n\n$$\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n$$\n\nNow to have basis for null space just plug values for $x_{3} = 1$ and $x_{4} = 0$, resulted vector is\n\n$$\n\\begin{bmatrix}\n\\frac{2}{3} \\\\\n-\\frac{1}{3} \\\\\n1 \\\\\n0\n\\end{bmatrix}\n$$\n\nThe resulted vector is one set of the basis for kernel space. The values for $x_{3}$ and $x_{4}$ are up to you as they are free variables.\n\n\n### Transformations\n\nMatrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications,\nincluding the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of\nmachine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive.\nFirstly, define linear transformation:\n\n> Linear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations\n\n#### Linear Transformation\n\nYou can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input\nvector to produce a vector output. Specifically, matrix multiplications of vectors are *linear transformations* that\ntransform the input vector into the output vector.\n\nFor example, consider a matrix $A$ and vector $v$\n\n$$\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n$$\n\nDefine transformation $T$ to be:\n\n$$\nT(\\vec{v}) = A \\vec{v}\n$$\n\nThis transformation is simply dot or inner product and give the following result:\n\n$$\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9\n\\end{bmatrix}\n$$\n\nIn this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional\nvector and produces a new 2-dimensional vector. Formally we can write this in the following way:\n\n$$\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{2}\n$$\n\nThe transformation does not necessarily have to be $n \\times n$. The dimension of the output vector and the input vector may differ. Rewrite our matrix $A$ and vector $v$.\n\n$$\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2 \\\\\n1 & 1\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n$$\n\nApply above transformation gives,\n\n$$\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9 \\\\\n3\n\\end{bmatrix}\n$$\n\nNow, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as\n\n$$\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{3}\n$$\n\n\n#### Transformations of Magnitude and Amplitude\n\nWhen we multiply a vector by a matrix we transform it in at least one of the following two ways\n\n* Scale the length (Magnitude)\n\n* Change the direction (Amplitude)\n\n*Change in length (Magnitude), but not change in direction (Amplitude)*\n\n$$\nA =\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n$$\n\ntransformation gives,\n\n$$\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n0\n\\end{bmatrix}\n$$\n\nIn this case, the resulted vector changed in length but not changed in direction.\n\n::: {#c657ec50 .cell execution_count=3}\n``` {.python .cell-code}\nv = np.array([1, 0])\nA = np.array([[2, 0], [0, 2]])\n\nt = np.dot(A, v)\nprint(\"Resulted vector is: t = \", t)\n\n# Original vector v is green and transformed vector t is blue.\n# Vector t has same direction as v but greater magnitude\nvecs = np.array([t, v])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulted vector is: t =  [2 0]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](linear_algebra_5_files/figure-html/cell-4-output-2.png){width=590 height=411}\n:::\n:::\n\n\n*Change in direction (Amplitude), but not change in length (Magnitude)*\n\n$$\nA =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n$$\n\ntransformation gives,\n\n$$\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n$$\n\nThis time, resulted vector changed in direction but has the same length.\n\n::: {#92c19954 .cell execution_count=4}\n``` {.python .cell-code}\nv = np.array([1, 0])\nA = np.array([[0, -1], [1, 0]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector change the direction but has the same length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulted vector is: t =  [0 1]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](linear_algebra_5_files/figure-html/cell-5-output-2.png){width=590 height=411}\n:::\n:::\n\n\n*Change in direction (Amplitude) and in length (Magnitude)*\n\n$$\nA =\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n$$\n\ntransformation gives,\n\n$$\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}\n$$\n\nThis time the resulted vector changed in the direction as well as the length.\n\n::: {#c8d1439c .cell execution_count=5}\n``` {.python .cell-code}\nv = np.array([1, 0])\nA = np.array([[2, 1], [1, 2]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector changed the direction, as well as the length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulted vector is: t =  [2 1]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](linear_algebra_5_files/figure-html/cell-6-output-2.png){width=590 height=411}\n:::\n:::\n\n\n#### Affine Transformation\n\nAn Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as *bias*.\n\n$$\nT(\\vec{v}) = A\\vec{v} + \\vec{b}\n$$\n\nConsider following example\n\n$$\nT(\\vec{v}) = A\\vec{v} + \\vec{b} =\n\\begin{bmatrix}\n5 & 2\\\\\n3 & 1\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2\\\\\n-6\n\\end{bmatrix} =\n\\begin{bmatrix}\n5\\\\\n-2\n\\end{bmatrix}\n$$\n\nThis kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation.\n\n::: {#bb6da948 .cell execution_count=6}\n``` {.python .cell-code}\nv = np.array([1, 1])\nA = np.array([[5, 2], [3, 1]])\nb = np.array([-2, -6])\n\nt = A @ v + b\nprint(\"Resulted vector is: t = \", t)\n\n# The resulted vector t is blue\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulted vector is: t =  [ 5 -2]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](linear_algebra_5_files/figure-html/cell-7-output-2.png){width=590 height=411}\n:::\n:::\n\n\n### Eigenvalues\n\nLet consider matrix $A$\n\n$$\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n$$\n\nNow, let multiply this matrix with vector\n$$\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n$$\n\nWe have the following:\n\n$$\nA \\cdot v =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n2 \\cdot v\n$$\n\nThat's the beautiful relationship yes? To prove this is not the only one vector, which can do this try this\nvector $\\vec{v} = [0\\quad 1\\quad 2]$ instead of old $v$. You should get $11\\cdot \\vec{v}$\n\nThis beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case $2$ and $11$ are eigenvalues of the matrix $A$.\n\nLet formalize the notion of eigenvalue and eigenvector:\n\n> Let $A$ be an $n\\times n$ **square** matrix. If $\\lambda$ is a scalar and $v$ is non-zero vector in $\\mathbb{R^n}$ such that $Av = \\lambda v$ then we say that $\\lambda$ is an *eigenvalue* and $v$ is *eigenvector*\n\nI believe you are interested in how to find eigenvalues. Consider again our matrix $A$ and follow steps to find eigenvalues.\nGiven that our matrix $A$ is a square matrix, the condition that characterizes an eigenvalue $\\lambda$ is the existence of a\nnonzero vector $v$ such that $Av = \\lambda v$. We can rewrite this equation in the following way:\n\n$$\nAv = \\lambda v\n\\\\\nAv - \\lambda v = 0\n\\\\\nAv - \\lambda I v = 0\n\\\\\n(A - \\lambda I)v = 0\n$$\n\nThe final form of this equation makes it clear that $v$ is the solution of a square, homogeneous system.\nTo have the nonzero solution(we required it in above definition), then the determinant of\nthe **coefficient matrix** - $(A - \\lambda I)$ must be zero. This is achieved when the columns of the coefficient matrix are\nlinearly dependent. In other words, to find eigenvalues we have to choose $\\lambda$ such that to solve the following equation:\n\n$$\ndet(A - \\lambda I) = 0\n$$\n\nThis equation is called **characteristic equation**\n\nFor more clarity, let solve it with a particular example. We have square matrix $A$ and follow the above equation gives us:\n\n$$\ndet(A - \\lambda I) = det\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\lambda \\cdot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\Bigg) =\ndet\\Bigg(\n\\begin{bmatrix}\n2 - \\lambda & 0 & 0 \\\\\n0 & 3 - \\lambda & 4 \\\\\n0 & 4 & 9 - \\lambda\n\\end{bmatrix}\n\\Bigg)\n\\Rightarrow\n$$\n$$\n\\Rightarrow\n(2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] =\n-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\n$$\n\nThe equation $-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22$ is called  **characteristic polynomial** of the matrix $A$\nand will be of degree $n$ if $A$ is $n\\times n$\n\nThe zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix $A$.\nIn this case the roots are $2$, $1$, and $11$. Surprise! Our matrix $A$ have three eigenvalues and two of them are\nalready known for us from above example.\n\nEigenvalues of a square matrix $A$ have some nice features:\n\n* The determinant of $A$ equals to the product of the eigenvalues\n\n* The trace of $A$ (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues\n\n* If $A$ is symmetric matrix, then all of its eigenvalues are real\n\n* If $A$ is invertible (The determinant of $A$ is not zero) and $\\lambda_{1}, \\cdots, \\lambda_{n}$ are its eigenvalues, then the eigenvalues of $A^{-1}$ are $1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}$\n\nFrom first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero.\n\n\n### Eigenvectors\n\nIt's time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue.\n\n> Any nonzero vector $v$ which satisfies characteristic equation is said to be an eigenvector of $A$ corresponding to $\\lambda$\n\nContinue above example and see what are eigenvectors corresponding to eigenvalues $\\lambda = 2$, $\\lambda = 1$, and $\\lambda = 11$, respectively.\n\nEigenvector for $\\lambda = 1$\n\n$$\n(A - 1I)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\Bigg)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 4 \\\\\n0 & 4 & 8\n\\end{bmatrix}\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n$$\n\nRewrite this as a system of equations, we'll get\n\n$$\n\\begin{cases}\nv_{1} = 0\\\\\n2v_{2} + 4v_{3} = 0\\\\\n4v_{2} + 8v{3} = 0\n\\end{cases}\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2v_{3}\\\\\nv_{3} = 1\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2\\\\\nv_{3} = 1\n\\end{cases}\n$$\n\nSo, our eigenvector corresponding to eigenvalue $\\lambda = 1$ is\n\n$$\nv_{\\lambda = 1} =\n\\begin{bmatrix}\n0 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n$$\n\nFinding eigenvectors for $\\lambda = 2$ and $\\lambda = 11$ is up to you.\n\n::: {#d5dde394 .cell execution_count=7}\n``` {.python .cell-code}\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\n\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Note that this eigenvectors seems different from my calculation. However they are not different.\n# They are normalized to have unit length\nprint(\"Eigenvalues are: \", eigenvalues)\nprint(\"Eigenvectors are: \", eigenvectors, sep=\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEigenvalues are:  [11.  1.  2.]\nEigenvectors are: \n[[ 0.          0.          1.        ]\n [ 0.4472136   0.89442719  0.        ]\n [ 0.89442719 -0.4472136   0.        ]]\n```\n:::\n:::\n\n\n### Spectrum and Spectral Radius\n\nThe **Spectral Radius** of a square matrix $A$ is the largest absolute values of its eigenvalues and is denoted by $\\rho(A)$. More formally,\n\n> Spectral radius of a $n \\times n$ matrix $A$ is:\n\n$$\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda\n\\mid \\ :\n\\lambda \\ is \\ an \\ eigenvalue \\ of \\ A\n\\Big\\}\n$$\n\nStated otherwise, we have\n\n$$\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda_{1}\n\\mid,\n\\cdots,\n\\mid \\lambda_{n}\n\\mid\n\\Big\\}\n$$\n\nIt's noteworthy that the set of all eigenvalues\n\n$$\n\\Big\\{ \\lambda : \\lambda \\in \\lambda(A)\n\\Big\\}\n$$\n\nis called the **Spectrum**\n\nFrom above example we had three eigenvalues, $\\lambda = 2$, $\\lambda = 1$ and $\\lambda = 11$ which are spectrum\nof $A$ and spectral radius for our matrix $A$ is $\\lambda = 11$\n\n\n# Conclusion for part I\n\nIn conclusion, part one turned out to be relatively heavy, as it contains lots of calculations.\nAll these calculations are crucial for understanding the concepts and the theory behind them.\nI hope you enjoyed this post and learned something new.\n\nThe second part of this advanced concepts is devoted solely for matrix decompositions.\n\n\n#### References\n* [Linear Algebra Done Right](http://linear.axler.net/)\n* [Linear Algebra Topics](https://en.wikipedia.org/wiki/List_of_linear_algebra_topics)\n\n",
    "supporting": [
      "linear_algebra_5_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}