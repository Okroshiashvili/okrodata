{
  "hash": "0a6814718bc1981c0f43eb1130a4b5e2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Intermediate Linear Algebra with Python - Part I\"\nauthor: \"Nodar Okroshiashvili\"\ndate: \"2021-03-17\"\ndate-modified: \"2025-10-01\"\ncategories: [Mathematics, Python]\ntags: [Mathematics, Linear Algebra, Vectors, Vector Operations, Vector Span, Vector Independence, Python]\n\nabstract: |\n    Part 3 of the linear algebra series with Python, that solely covers the vectors and vector operations in more detail.\n\nimage: \"linear_algebra.jpg\"\n---\n\n# Introduction\n\n\n1. [Introduction to Linear Algebra with Python](linear_algebra_1.qmd)\n2. [Basic Linear Algebra with Python](linear_algebra_2.qmd)\n3. **Intermediate linear algebra**\n   1. **Intermediate Linear Algebra with Python - Part I**\n   2. Intermediate Linear Algebra with Python - Part II\n4. Advanced linear algebra\n   1. Advance Linear Algebra with Python - Part I\n   2. Advance Linear Algebra with Python - Part II\n\n\nIn this post I will introduce you to the notion of vector, its characteristics, and operations defined on vectors. I will also show you how to use Python to do these operations.\n\n\n## Vector\n\n\n### Cross Product\n\nIn the case of the dot product between two vectors, we saw that the result is a scalar. In the case of a cross product\nthe result is a vector, so the cross product is also called the vector product. The resulted vector is a vector that is at\nright angles to both the other vectors in 3D Euclidean space. This means that the cross-product only really makes sense\nwhen working with vectors that contain three components.\n\nThere are two formulas to calculate the cross product. One is algebraic and second is geometric.\nMore precisely, the first formula catches the algebraic intuition of cross product and second catches\nthe geometric intuition of the cross product.\n\nIf we have two vectors $A$ and $B$ in such a way:\n\n$$\nA =\n\\begin{bmatrix}\n    a_{1} \\\\\n    a_{2} \\\\\n    a_{3}\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    b_{1} \\\\\n    b_{2} \\\\\n    b_{3}\n\\end{bmatrix}\n$$\n\nThe algebraic formula is:\n\n$$\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(a_{2} \\cdot b_{3}) - (a_{3} \\cdot b_{2}) \\\\\n(a_{3} \\cdot b_{1}) - (a_{1} \\cdot b_{3}) \\\\\n(a_{1} \\cdot b_{2}) - (a_{2} \\cdot b_{1})\n\\end{bmatrix}\n$$\n\nAt the first glance, the formula is not easy to remember. Let try another formula which uses some advanced components.\n\n$$\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n\\vec{i} & \\vec{j} & \\vec{k} \\\\\na_{1} & a_{2} & a_{3} \\\\\nb_{1} & b_{2} & b_{3}\n\\end{bmatrix}\n$$\n\nwhere $\\vec{i}$, $\\vec{j}$ and $\\vec{k}$ are bases vectors and I will explain in advance part of these series.\n\nTo find the cross product we have to find the determinant of above matrix in the following way:\n\n$$\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\na_{2} & a_{3} \\\\\nb_{2} & b_{3}\n\\end{bmatrix} \\vec{i} \\quad - \\quad\n\\begin{bmatrix}\na_{1} & a_{3} \\\\\nb_{1} & b_{3}\n\\end{bmatrix} \\vec{j} \\quad + \\quad\n\\begin{bmatrix}\na_{1} & a_{2} \\\\\nb_{1} & b_{2}\n\\end{bmatrix} \\vec{k}\n$$\n\nSuppose we have two vectors:\n\n$$\nA =\n\\begin{bmatrix}\n    2 \\\\\n    3 \\\\\n    1\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    1 \\\\\n    2 \\\\\n    -2\n\\end{bmatrix}\n$$\n\nThen cross product is:\n\n$$\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(3 \\cdot (-2)) - (1 \\cdot 2) \\\\\n(1 \\cdot 1) - (2 \\cdot (-2)) \\\\\n(2 \\cdot 2) - (3 \\cdot 1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n(-6) - 2 \\\\\n1 - (-4) \\\\\n4 - 3\n\\end{bmatrix} =\n\\begin{bmatrix}\n-8 \\\\\n5 \\\\\n1\n\\end{bmatrix}\n$$\n\nThe geometric formula is:\n\n$$\n\\vec{C} =\n\\vec{A} \\times \\vec{B} =\n\\|\\vec{A}\\|\\cdot\\|\\vec{B}\\|\\cdot\\sin{\\theta} \\cdot \\hat{n}\n$$\n\nwhere, $\\theta$ is the angle between $\\vec{A}$ and $\\vec{B}$. Also, $\\hat{n}$ is a unit vector perpendicular to\nboth $\\vec{A}$ and $\\vec{B}$, such that $\\vec{A}$, $\\vec{B}$ and $\\hat{n}$ form a [right-handed system](https://en.wikipedia.org/wiki/Right-hand_rule)\n\nThere is also geometric application of the cross product. If we three vectors $\\vec{a}$, $\\vec{b}$ and $\\vec{c}$\nwhich forms the three dimensional figure such as:\n\n![right-handed system](right_handed.png)\n\nThen the area of the parallelogram ( two dimensional front of this object ) is given by:\n\n$$\n{\\rm{Area}} = \\left\\| {\\vec{a} \\times \\vec{b}} \\right\\|\n$$\n\nThis is the dot product of the result of cross product between vectors $\\vec{a}$ and $\\vec{b}$\n\nThe volume of the parallelepiped ( the whole three dimensional object ) is given by:\n\n$$\n{\\rm{Volume}} = \\left| {\\vec{a} \\centerdot \\left( {\\vec{b} \\times \\vec{c}} \\right)} \\right|\n$$\n\nhere, absolute value bars are necessary since the result could be negative and volume must be positive.\n\n::: {#5e480472 .cell execution_count=1}\n``` {.python .cell-code}\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n```\n:::\n\n\n::: {#3bd602fa .cell execution_count=2}\n``` {.python .cell-code}\nA = np.array([2, 3, 1])\nB = np.array([1, 2, -2])\n\nprint(\"A =\", A)\nprint(\"B = \", B)\n\nprint(\"Cross Product is:\", np.cross(A, B), sep=\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA = [2 3 1]\nB =  [ 1  2 -2]\nCross Product is:\n[-8  5  1]\n```\n:::\n:::\n\n\n### Span\n\nsuppose we have set of vectors:\n\n$$\n{\\alpha _1},...,{\\alpha _n} \\in A\n$$\n\nthen we can define the space $S$ spanned by:\n\n$$\n{\\alpha _1},...,{\\alpha _n}\n$$\n\nas\n\n$$\nS\\left( {{\\alpha_{1}},...,{\\alpha_{n}}} \\right) = \\left\\{ {\\sum_{i = 1}^{n} {{c_{i}}{\\alpha_{i}}\\;|\\;{c_{i}} \\in \\mathbb{R}} } \\right\\}\n$$\n\nwhich is the set of all linear combinations of the vectors in this subspace. The set is a subspace of $A$:\n\n$$\nS\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) \\subset A\n$$\n\nIn other words, if we have set of vectors\n\n$$\nA := \\{a_1, \\ldots, a_k\\}\n\\in\n\\mathbb R^n\n$$\n\nit's natural to think about the new vectors we can create from these vectors by performing linear operations.\nNew vectors created in this way are called linear combinations of $A$. Particularly, $X \\in \\mathbb{R}^n$ is a\nlinear combination of $A$ if\n\n$$\nX =\n\\beta_1 a_1 + \\cdots + \\beta_k a_k\n\\text{ for some scalars } \\beta_1, \\ldots, \\beta_k\n$$\n\nhere, $\\beta_1, \\ldots, \\beta_k$ are called coefficients of the linear combination.\n\nThe set of linear combinations of $A$ is the span of $A$ and is written as $Span(A)$\n\nFor example, if\n\n$$\n\\vec{A} = \\{e_1, e_2, e_3\\}\n\\quad\n\\text{such that}\n\\quad\ne_1 :=\n\\begin{bmatrix}\n     1 \\\\\n     0 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_2 :=\n\\begin{bmatrix}\n     0 \\\\\n     1 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_3 :=\n\\quad\n\\begin{bmatrix}\n     0 \\\\\n     0 \\\\\n     1\n\\end{bmatrix}\n$$\n\nthen the span of $A$ is all of $\\mathbb{R}^3$, because, for any $X=(x_1, x_2, x_3)\\in \\mathbb{R}^3$, we can write\n\n$$\nX =\nx_1 e_1 + x_2 e_2 + x_3 e_3\n$$\n\nThis means that by using $A$ or vectors $e_1$, $e_2$ and $e_3$ we can generate any vector in $\\mathbb{R}^3$ by performing linear operations.\n\nThis figure below shows the span of $A = \\{a_1, a_2\\}$ in $\\mathbb{R}^3$.\nThe span is a 2 dimensional plane passing through these two points and the origin.\n\n::: {#d1aaeefd .cell execution_count=3}\n``` {.python .cell-code}\n# Linear function to generate a plane\ndef f(x, y):\n    return (0.2 * x) + (0.1 * y)\n\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set(xlim=(-5, 5), ylim=(-5, 5), zlim=(-5, 5), xticks=(0,), yticks=(0,), zticks=(0,))\n\nz = np.linspace(-5, 5, 3)\ny = np.zeros(3)\nx = np.zeros(3)\n\nax.plot(x, y, z, \"k-\", lw=2, alpha=0.5)\nax.plot(z, x, y, \"k-\", lw=2, alpha=0.5)\nax.plot(y, z, x, \"k-\", lw=2, alpha=0.5)\n\n# Set vector coordinates\nx_coords = np.array((3, 3))\ny_coords = np.array((4, -4))\nz = f(x_coords, y_coords)\n\nfor i in (0, 1):\n    ax.text(x_coords[i], y_coords[i], z[i], f\"$a_{i+1}$\", fontsize=14)\n\n# We need to draw lines from origin to the vectors\nfor i in (0, 1):\n    x = (0, x_coords[i])\n    y = (0, y_coords[i])\n    z = (0, f(x_coords[i], y_coords[i]))\n    ax.plot(x, y, z, \"b-\", lw=1.5, alpha=0.6)\n\n\n# As we already draw axes and vectors, it's time to plot the plane\nxr2 = np.linspace(-5, 5, 50)\nyr2 = np.linspace(-5, 5, 50)\n\nx2, y2 = np.meshgrid(xr2, yr2)\nz2 = f(x2, y2)\n\nax.plot_surface(x2, y2, z2, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=True, alpha=0.2)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](linear_algebra_3_files/figure-html/cell-4-output-1.png){width=611 height=611}\n:::\n:::\n\n\n### Linear Independence and Dependence\n\nAbove, I mentioned a linear combination. In order to define linear dependence and independence let farther clarify what is a linear combination. If we have a set of vectors\n\n$$\n\\vec{A} = \\{a_1, \\ldots, a_k\\}\n$$\n\nwhich all have the same dimension, then\n\n> A **linear combination** of the vectors in $A$ is any vector of the form $c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k}$, where $c_{1}, c_{2}, \\ldots, c_{k}$ are arbitrary scalars\n\nFor example, if $A = \\{[1, 2], [2, 1]\\}$, then\n\n$$\n2 a_{1} - a_{2} = 2 ([1, 2]) - [2, 1] = [0, 3]\n$$\n\nis linear combination of the vectors in $A$.\n\n> A set $A$ of m-dimensional vectors is **linearly independent** if the only linear combination of vectors in $A$ that equals $0$ is the trivial linear combination\n\nThis formal definition seems a little bit confusing. Let consider the example to catch the idea.\n\nThe set of vectors\n\n$$\nA =\n\\{[1, 0], [0, 1]\\}\n$$\n\nis linearly independent. Let prove this claim. We need to find constants $c_{1}$ and $c_{2}$ satisfying\n\n$$\nc_{1} ([1, 0]) + c_{2} ([0, 1]) = [0, 0]\n$$\n\nsolving this system of equations gives that $[c_{1}, c_{2}] = [0, 0] \\rightarrow c_{1} = c_{2} = 0$, in turn this implies that $A$ is linearly independent.\n\nThe following statements are equivalent for a linear independence of $A$:\n\n* No vector in $A$ can be formed as a linear combination of the other vectors. This means that if we have three vectors in $A$, any of them cannot be expressed as the linear combination of the other two.\n\n\n* If $c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k} = 0$, then $c_{1} = c_{2} = \\cdots = c_{k} = 0$\n\n> A set $A$ of m-dimensional vectors is **linearly dependent** if there is a nontrivial linear combination of the vectors in $A$ that adds up to $0$\n\nThe set of vectors\n\n$$\nA =\n\\{[1, 2], [2, 4]\\}\n$$\n\nis linearly dependent set of vectors. Let see how.\n\n$$\nc_{1} ([1, 2]) + c_{2} ([2, 4]) = [0, 0]\n$$\n\nThere is a nontrivial linear combination with $c_{1} = 2$ and $c_{2} = 1$ that yields $0$. This implies $A$ is the linearly\ndependent set of vectors. It's easy in this case to spot linear dependence by first glance, as the second vector is 2 times the first vector, which indicates linear dependence.\n\n::: {#23ea3a13 .cell execution_count=4}\n``` {.python .cell-code}\nmatrix = np.array([[1, 2], [2, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(0, 1)\n```\n:::\n:::\n\n\nThis says that the vectors at index 0 and 1 are linearly independent. Let consider a linearly dependent set of vectors to see the result of the above code clearly.\n\n::: {#4b75105c .cell execution_count=5}\n``` {.python .cell-code}\nmatrix = np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 1, 0], [1, 0, 0, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(0, 1, 3)\n```\n:::\n:::\n\n\nThis says that vectors at index 0, 1, and 3 are linearly independent, while vector at index 2 is linearly dependent.\n\n\n# Conclusion for part I\n\nHere, I only covered half of the materials that I initially had intended to cover.\nI split those materials into two parts, mainly because to improve readability and maintain consistency.\n\nIn the second part, I review matrices and operations on matrices.\n\n\n#### References\n* [Linear Algebra Done Right](http://linear.axler.net/)\n* [Linear Algebra Topics](https://en.wikipedia.org/wiki/List_of_linear_algebra_topics)\n\n",
    "supporting": [
      "linear_algebra_3_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}