{
  "hash": "2fea7743055186086aa089c3e8590e70",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Singular Spectrum Analysis with Python\"\nauthor: \"Nodar Okroshiashvili\"\ndate: \"2021-08-10\"\ncategories: [Time Series Analysis]\ntags: [Singular spectrum analysis, Python, Time Series Analysis]\nkeywords: [time series, singular spectrum analysis, python, numpy, scipy, matplotlib, pandas, statsmodels, time series decomposition, singular value decomposition]\n---\n\n# Introduction\n\nIn this article, I will talk about `Singular Spectrum Analysis (SSA)` and its implementation in Python.\nFor the sake of simplicity, I will use the univariate time series and will implement basic SSA without forecasting capability. However, it is not hard to extend this method to multivariate time series and add forecasting capability as well.\n\n\nIn Time Series Analysis (TSA), the technique called `Singular Spectrum Analysis` (SSA) has been developed to use it for time series decomposition, forecasting, and noise reduction. It is a nonparametric technique, combining some elements of classical time series analysis, multivariate statistics, dynamical systems, and signal processing. Since it is nonparametric, it works with arbitrary statistical processes, whether linear or nonlinear, stationary or non-stationary, Gaussian or non-Gaussian.\n\n\n## Singular Spectrum Analysis\n\nLet dive into the technical details of the SSA. It consists of the two main steps: **decomposition** and **reconstruction**. Each above step, in turn, consists of the two sub-steps: **embedding followed by singular value decomposition (SVD)** and **eigentriple grouping followed by diagonal averaging**.\n\n\n### Decomposition\n\nAt that stage, we have to perform embedding followed by SVD. Let consider following time series: $\\mathbb{X} = (x_{1}, \\cdots, x_{N})$ of length $N$. Now let pick a window size $L$ and embed the time series into a trajectory matrix of dimension $L \\times K$, where $K = N - L + 1$:\n\nWindow length $L$ is the only hyper-parameter of the SSA. It is important to choose the right window length, because if the window length is too small, then the result will be noisy, and if the window length is too large, then the result will be too smooth. So, the window length should be chosen carefully.\n\nThe **trajectory matrix** of the series $\\mathbb{X}$ is defined as follows:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\cdots & x_K \\\\\nx_2 & x_3 & x_4 & \\cdots & x_{K+1} \\\\\nx_3 & x_4 & x_5 & \\cdots & x_{K+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n   x_L & x_{L+1} & x_{L+2} & \\cdots & x_N\n\\end{bmatrix}\n$$\n\nWe can see that, the first column of the trajectory matrix is the original time series up to a length $L$, the second column is the original time series shifted by one time step, and so on. We also see that our matrix $\\mathbf{X}$ has equal elements $x_{ij}$ on the anti-diagonals $i + j = const$. Hence, the matrix $\\mathbf{X}$ is called **[Hankel matrix](https://en.wikipedia.org/wiki/Hankel_matrix)**.\n\nInstead of implementing Hankel matrix from scratch, we can use `scipy` to hankelize any given time series. The function below takes time series and window length as input and returns Hankel matrix.\n\n::: {#01c2af36 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nimport seaborn as sns\n```\n:::\n\n\n::: {#f4aa7cc9 .cell execution_count=2}\n``` {.python .cell-code}\ndef embed(time_series, window_length):\n    \"\"\"\n    Embed the time series into a Hankel matrix. This is the trajectory matrix.\n    \"\"\"\n    K = len(time_series) - window_length + 1\n    trajectory_matrix = scipy.linalg.hankel(time_series, np.zeros(window_length)).T[:, : K]\n    return trajectory_matrix\n```\n:::\n\n\nLet consider following time series as an example:\n\n::: {#42179545 .cell execution_count=3}\n``` {.python .cell-code}\ntime_series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nwindow_length = 4\n\ntrajectory_matrix = embed(time_series, window_length)\n\ntrajectory_matrix\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n       [ 2.,  3.,  4.,  5.,  6.,  7.,  8.],\n       [ 3.,  4.,  5.,  6.,  7.,  8.,  9.],\n       [ 4.,  5.,  6.,  7.,  8.,  9., 10.]])\n```\n:::\n:::\n\n\nThe second step is to perform SVD on the trajectory matrix. First, we multiply the trajectory matrix by its transpose to form the matrix $\\mathbf{S} = XX^{T}$ and then perform SVD on the resulting matrix.\n\n[For more information how SVD works check my previous blog.](../mathematics/linear_algebra_6.qmd#singular-value-decomposition)\n\nAfter performing SVD, we get eigenvalues, sorted in decreasing order and left singular vectors of the matrix $\\mathbf{S}$. We're not interested in the right singular vectors, so we can discard them. For the next step, we also need to calculate the rank $d$ of the trajectory matrix $\\mathbf{X}$.\n\nThe function below takes trajectory matrix as input and returns rank, eigenvalues, and left singular vectors.\n\n::: {#a27a1b9e .cell execution_count=4}\n``` {.python .cell-code}\ndef decompose(trajectory_matrix):\n    \"\"\"\n    Decompose the trajectory matrix into its singular values and vectors.\n    \"\"\"\n    S = np.matmul(trajectory_matrix, trajectory_matrix.T)\n    singular_vectors, singular_values, _ = scipy.linalg.svd(S)\n    singular_values = np.sqrt(singular_values)\n    rank = np.linalg.matrix_rank(trajectory_matrix)\n    return rank, singular_values, singular_vectors\n```\n:::\n\n\n::: {#abf6af2f .cell execution_count=5}\n``` {.python .cell-code}\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\nprint(\"Rank: \", rank)\nprint(\"Singular values: \", singular_values, sep=\"\\n\")\nprint(\"Singular vectors: \", singular_vectors, sep=\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRank:  2\nSingular values: \n[31.46491008  1.98983259  0.00000016  0.00000009]\nSingular vectors: \n[[-0.37304882 -0.7488889   0.54567073 -0.04736515]\n [-0.45239454 -0.30877044 -0.69225707  0.46987248]\n [-0.53174025  0.13134802 -0.25249804 -0.79764951]\n [-0.61108597  0.57146648  0.39908439  0.37514218]]\n```\n:::\n:::\n\n\n### Reconstruction\n\nAt that stage, we have to perform eigentriple grouping followed by diagonal averaging. The SVD step of the trajectory matrix is not finished. I split it into two parts for better understanding. The first part was actual decomposition, and the second part is grouping to form elementary matrices.\n\nThe function below takes trajectory matrix, rank, eigenvalues, and left singular vectors as input and returns grouped matrices. The rank of each matrix is 1, hence the name **elementary matrices**. The collection $(\\sqrt{\\lambda_{i}}, U_{i}, V_i{})$ is called **eigentriple** of the SVD. Not to be confused with $V_{i}$. It is not the right singular vector, but the following matrix $V_{i} = \\\\ \\mathbf{X}^{T}U_{i}/{\\sqrt{\\lambda_{i}}}$\n\n::: {#7653ac08 .cell execution_count=6}\n``` {.python .cell-code}\ndef group(trajectory_matrix, rank, singular_values, singular_vectors):\n    \"\"\"\n    Group the singular values and vectors into matrices.\n    \"\"\"\n    V = {i: np.matmul(trajectory_matrix.T, singular_vectors[:, i]) / singular_values[i] for i in range(rank)}\n    # The rank of each matrix is 1\n    X = {\n        i: np.matmul(singular_values[i] * singular_vectors[:, i].reshape(-1, 1), V[i].reshape(-1, 1).T)\n        for i in range(rank)\n    }\n    return X\n```\n:::\n\n\n::: {#3a32c669 .cell execution_count=7}\n``` {.python .cell-code}\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n{0: array([[ 1.98365074,  2.71791138,  3.45217202,  4.18643267,  4.92069331,\n          5.65495395,  6.38921459],\n        [ 2.40556386,  3.29599826,  4.18643267,  5.07686707,  5.96730148,\n          6.85773588,  7.74817029],\n        [ 2.82747698,  3.87408514,  4.92069331,  5.96730148,  7.01390964,\n          8.06051781,  9.10712598],\n        [ 3.2493901 ,  4.45217202,  5.65495395,  6.85773588,  8.06051781,\n          9.26329974, 10.46608167]]),\n 1: array([[-0.98365074, -0.71791138, -0.45217202, -0.18643267,  0.07930669,\n          0.34504605,  0.61078541],\n        [-0.40556386, -0.29599826, -0.18643267, -0.07686707,  0.03269852,\n          0.14226412,  0.25182971],\n        [ 0.17252302,  0.12591486,  0.07930669,  0.03269852, -0.01390964,\n         -0.06051781, -0.10712598],\n        [ 0.7506099 ,  0.54782798,  0.34504605,  0.14226412, -0.06051781,\n         -0.26329974, -0.46608167]])}\n```\n:::\n:::\n\n\nAt the diagonal averaging step, each matrix $\\mathbf{X_{I_{j}}}$ of the grouped decomposition is henkelized and then the obtained Hankel matrix is transformed into a new series of length $N$. Diagonal averaging applied to the resulted matrix $\\mathbf{X_{I_{k}}}$ produces a reconstructed series $\\mathbb{\\tilde{X^{(k)}}} = (\\tilde{x}^{(k)}_{1}, \\cdots, \\tilde{x}^{(k)}_{N})$. Therefore, the initial series is decomposed into a sum of $m$ reconstructed subseries:\n\n$$\nx_{n} = \\sum_{k=1}^{m} \\tilde{x}^{(k)}_{n}, \\quad n = 1, \\cdots, N\n$$\n\nThe function below takes grouped matrices as input and returns reconstructed time series.\n\n::: {#f2a5828a .cell execution_count=8}\n``` {.python .cell-code}\ndef diagonal_averaging(X, m):\n    \"\"\"\n    Perform anti-diagonal averaging of the given hankel matrix\n    \"\"\"\n    if m > len(X):\n        raise ValueError(\"Number of singular values cannot be greater than the rank of the trajectory matrix.\")\n\n    result = []\n    matrix = np.sum([X[i] for i in range(m)], axis=(0))\n    rows, columns = matrix.shape\n    rows_star, columns_star = min(rows, columns), max(rows, columns)\n\n    for k in range(1 - columns_star, rows_star):\n        identity_matrix = np.eye(columns_star, k=k, dtype=\"bool\")[::-1][:rows_star, :]\n        identity_matrix_sum = np.sum(identity_matrix)\n        mask = np.ma.masked_array(matrix, mask=1 - identity_matrix)\n        average = mask.sum() / identity_matrix_sum\n        result.append(average)\n\n    return pd.DataFrame(result).rename(columns={0: \"reconstruction\"})\n```\n:::\n\n\nHere, I pick the first matrix of the grouped decomposition and perform diagonal averaging. The result is a reconstructed time series.\n\n::: {#432968d8 .cell execution_count=9}\n``` {.python .cell-code}\nreconstructed_time_series = diagonal_averaging(X, 1)\n\nreconstructed_time_series\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reconstruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.983651</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.561738</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.191882</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.874085</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.842606</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.811128</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.779649</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.956402</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9.185213</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10.466082</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nHere is the result for the first two matrices. We see that it produces exactly the same series as initial one.\n\n::: {#14dab6a5 .cell execution_count=10}\n``` {.python .cell-code}\nreconstructed_time_series = diagonal_averaging(X, 2)\n\nreconstructed_time_series\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reconstruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can also visualize how much each singular value contributes to the variance.\n\n::: {#959527f1 .cell execution_count=11}\n``` {.python .cell-code}\ndef plot_contributions(trajectory_matrix, singular_values):\n    \"\"\"View the contribution to variance of each singular value and its corresponding signal\"\"\"\n    lambdas = np.power(singular_values, 2)\n    norm = np.linalg.norm(trajectory_matrix)\n    contributions = pd.DataFrame((lambdas / (norm**2)).round(4), columns=[\"contribution\"])\n    # Filter out the contributions that are zero\n    contributions = contributions[contributions[\"contribution\"] > 0]\n    # Adjust the scale of the contributions\n    contributions[\"contribution\"] = (1 / contributions[\"contribution\"]).max() * 1.1 - (\n        1 / contributions[\"contribution\"]\n    )\n\n    # Plot the contributions\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(11, 4))\n    plt.axes().get_yaxis().set_ticks([])\n    sns.barplot(data=contributions, x=contributions.index, y=\"contribution\")\n    plt.title(\"Non-zero contributions of singular values to variance\")\n    plt.xlabel(\"Singular value\")\n    plt.ylabel(\"Contribution\")\n    plt.show()\n```\n:::\n\n\n::: {#7897b0f6 .cell execution_count=12}\n``` {.python .cell-code}\nplot_contributions(trajectory_matrix, singular_values)\n```\n\n::: {.cell-output .cell-output-display}\n![](singular_spectrum_analysis_files/figure-html/cell-13-output-1.png){width=838 height=380}\n:::\n:::\n\n\nFrom the above plot we see that the first singular value contributes the most to the variance. The second singular value contributes less.\n\n### Real Life Example\n\nI guess, the above example does not make much sense as it's not something real. Let consider real life data and go thru all the steps one by one. You can download [the data from here](https://www.kaggle.com/datasets/nodarokroshiashvili/time-series).\n\nLet read the data and replicate above steps.\n\n::: {#8c38a420 .cell execution_count=13}\n``` {.python .cell-code}\nts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\");\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_31497/3609878243.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  ts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](singular_spectrum_analysis_files/figure-html/cell-14-output-2.png){width=910 height=358}\n:::\n:::\n\n\nWe see that the data is characterized with some seasonality. Due to that reason, we will use a window size of 24.\n\n::: {#0266a95f .cell execution_count=14}\n``` {.python .cell-code}\ntrajectory_matrix = embed(ts, window_length=24)\n\ntrajectory_matrix\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([[ 6345., 20965., 51355., ..., 13655.,  4565.,  2840.],\n       [20965., 51355., 26435., ...,  4565.,  2840., 15380.],\n       [51355., 26435.,  9830., ...,  2840., 15380., 14340.],\n       ...,\n       [ 4725.,  9435.,  7495., ..., 17965.,  7505.,  2480.],\n       [ 9435.,  7495.,  7830., ...,  7505.,  2480.,  5725.],\n       [ 7495.,  7830., 13850., ...,  2480.,  5725., 17405.]],\n      shape=(24, 594))\n```\n:::\n:::\n\n\n::: {#e4af167f .cell execution_count=15}\n``` {.python .cell-code}\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n```\n:::\n\n\n::: {#339e7647 .cell execution_count=16}\n``` {.python .cell-code}\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n```\n:::\n\n\n::: {#7f8e66e4 .cell execution_count=17}\n``` {.python .cell-code}\nreconstructed = diagonal_averaging(X, m=12)\n\nts[\"reconstruction\"] = reconstructed[\"reconstruction\"].values\n```\n:::\n\n\n::: {#3f6b4b6c .cell execution_count=18}\n``` {.python .cell-code}\n# Plot original and reconstructed series\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\", label=\"Original\")\nsns.lineplot(data=ts, x=ts.index, y=\"reconstruction\", label=\"Reconstructed\");\n```\n\n::: {.cell-output .cell-output-display}\n![](singular_spectrum_analysis_files/figure-html/cell-19-output-1.png){width=910 height=358}\n:::\n:::\n\n\n# Conclusion\n\nThe result seems quite impressive. The implementation was not hard and the result is quite good. Adding forecasting capability and extending this method to multivariate time series should not be hard. For more detailed information, please refer to the references below.\n\n\n#### References\n* [Singular Spectrum Analysis for Time Series](https://www.researchgate.net/publication/260124592_Singular_Spectrum_Analysis_for_Time_Series)\n* [A Brief Introduction to Singular Spectrum Analysis](https://ssa.cf.ac.uk/ssa2010/a_brief_introduction_to_ssa.pdf)\n* [Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis)\n\n",
    "supporting": [
      "singular_spectrum_analysis_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}