[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Okro Data",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nSingular Spectrum Analysis with Python\n\n\n\nTime Series Analysis\n\nPython\n\n\n\nReview of Singular Spectrum Analysis and its implementation in Python\n\n\n\n\n\nAug 10, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nAdvance Linear Algebra with Python - Part II\n\n\n\nMathematics\n\nPython\n\n\n\nPart 6 of the linear algebra series with Python, that covers various forms of matrix decompositions.\n\n\n\n\n\nMay 13, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nAdvance Linear Algebra with Python - Part I\n\n\n\nMathematics\n\nPython\n\n\n\nPart 5 of the linear algebra series with Python, that covers operations such as Basis, Ranks, Gauss and Gauss-Jordan elimination, eigenvalues and eigenvectors.\n\n\n\n\n\nApr 21, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Linear Algebra with Python - Part II\n\n\n\nMathematics\n\nPython\n\n\n\nPart 4 of the linear algebra series with Python, that solely covers the matrices and matrix operations in more detail.\n\n\n\n\n\nMar 22, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Linear Algebra with Python - Part I\n\n\n\nMathematics\n\nPython\n\n\n\nPart 3 of the linear algebra series with Python, that solely covers the vectors and vector operations in more detail.\n\n\n\n\n\nMar 17, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Linear Algebra with Python\n\n\n\nMathematics\n\nPython\n\n\n\nPart 2 of the linear algebra series with Python, that covers the basics of vectors and matrices.\n\n\n\n\n\nMar 1, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Linear Algebra with Python\n\n\n\nMathematics\n\nPython\n\n\n\nPart 1 of the linear algebra series with Python, that covers the basic building blocks of linear algebra.\n\n\n\n\n\nFeb 22, 2021\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nInteger Sequences in Python\n\n\n\nMathematics\n\nPython\n\n\n\nThis post reviews some famous integer sequences with Python.\n\n\n\n\n\nDec 23, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nTaylor Series Expansion with Python\n\n\n\nMathematics\n\nPython\n\n\n\nTaylor and Maclaurin series with Python.\n\n\n\n\n\nOct 10, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Information Theory with Python\n\n\n\nGeneral\n\nPython\n\n\n\nThis post reviews some basic concepts of information theory with Python.\n\n\n\n\n\nJul 12, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Pandas - Part 4\n\n\n\nData Science\n\n\n\nPandas tips and tricks that I found useful in my data science journey.\n\n\n\n\n\nApr 29, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Pandas - Part 3\n\n\n\nData Science\n\n\n\nPandas tips and tricks that I found useful in my data science journey.\n\n\n\n\n\nApr 20, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Pandas - Part 2\n\n\n\nData Science\n\n\n\nPandas tips and tricks that I found useful in my data science journey.\n\n\n\n\n\nApr 14, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Pandas - Part 1\n\n\n\nData Science\n\n\n\nPandas tips and tricks that I found useful in my data science journey.\n\n\n\n\n\nApr 10, 2020\n\n\nNodar Okroshiashvili\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Single Good Market\n\n\n\nEconomics\n\nPython\n\n\n\nImplementation of one good market economy in Python, where supply and demand are linear functions of price and agents of this economy are price takers.\n\n\n\n\n\nJun 16, 2019\n\n\nNodar Okroshiashvili\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nodar Okroshiashvili",
    "section": "",
    "text": "I’m Data Software Engineer with 7+ years of experience architecting production-grade data applications and intelligent systems across fintech, e-commerce, cybersecurity, and document intelligence domains. Expert in Python ecosystem, cloud platforms, and real-time data processing with proven track record delivering end-to-end solutions from data collection to user-facing APIs. Successfully built custom search engines, credit scoring systems, anomaly detection platforms, and document intelligence solutions. Combines deep technical expertise with business domain understanding to solve complex problems through scalable and reliable data applications. Active open source contributor with experience collaborating on community-driven projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Nodar Okroshiashvili",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul, MN B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Nodar Okroshiashvili",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_1.html",
    "href": "posts/pandas/pandas_tips_and_tricks_1.html",
    "title": "Lost in Pandas - Part 1",
    "section": "",
    "text": "Many have been told and written about Pandas and its capabilities. I could not imagine data scientist or data analyst who had not heard about Pandas or had not used it at least once. We all use it. Every day, every week. It does not matter how many times. It’s a great tool. I use it all the time when I want to do data analysis, either it is simple calculations or complex data transformations, and it surprises me. Pandas is so simple in its form. However, imagine, how much you can do with some simple method chaining.\nSaying all of these, this blog aims to share my experience and amazment with Pandas.\n\n\nWe have data. This data comes from the HR department of the company. The data contains two columns, company name, and information about its employees. Each row of the employee information column is a list of lists. The lists inside the outer list can be duplicated. It also can have duplicate values, and inner lists have at most two values.\n\nDisclaimer: Any name, phone, email, and the title is a pure coincidence. Data is random and fake.\n\nHere is our data.\n\nfrom collections import defaultdict\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = {\n    \"company_name\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n    \"info\": [\n        [[\"Name\", \"David Jones\"], [\"Title\", \"CEO\"], [\"Phone\", \"207-685-1626\"], [\"Email\", \"djones@example.org\"]],\n        [\n            [\"Name\", \"Kate Brown\"],\n            [\"Title\", \"Senior Lawyer\"],\n            [\"Phone\", \"316-978-7791\"],\n            [\"Email\", \"Kate.Brown@example.edu\"],\n            [\"Name\", \"Darin White\"],\n            [\"Title\", \"Associate Vice President\"],\n            [\"Phone\", \"316-978-3887\"],\n            [\"Email\", \"Darin.White@example.edu\"],\n        ],\n        [\n            [\"Name\", \"Scott Lamb\"],\n            [\"Title\", \"Actuary\"],\n            [\"Phone\", \"316-978-3804\"],\n            [\"Email\", \"scott.lamb@example.edu\"],\n            [\"Name\", \"Scott Lamb\"],\n            [\"Title\", \"Senior Officer\"],\n            [\"Title\", \"Application Developer\"],\n            [\"Title\", \"Blockchain Architect\"],\n            [\"Title\", \"Director of External Affairs\"],\n            [\"Name\", \"Scott\"],\n            [\"Name\", \"Scott\"],\n            [\"Title\", \"Director of Medicine\"],\n            [\"Title\", \"Product Owner\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Domain Expert\"],\n            [\"Title\", \"Growth Hacker\"],\n            [\"Title\", \"Engineering Head\"],\n            [\"Title\", \"Event Manager\"],\n            [\"Name\", \"Joe\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Fundraising\"],\n            [\"Title\", \"VP of Customers\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Venture Capital Analyst\"],\n            [\"Title\", \"UX Designer\"],\n            [\"Name\", \"Mike\"],\n            [\"Name\", \"Susan\"],\n            [\"Name\", \"Bryan\"],\n            [\"Name\", \"Mia\"],\n            [\"Title\", \"Songwriter\"],\n        ],\n        [\n            [\"Name\", \"Rose Smith Rose Smith\"],\n            [\"Title\", \"Vice President\"],\n            [\"Title\", \"Finance and Operations Head\"],\n            [\"Phone\", \"316-978-3810\"],\n            [\"Email\", \"rose.smith@example.edu\"],\n            [\"Name\", \"Rose Smith\"],\n            [\"Title\", \"Foundation\"],\n            [\"Name\", \"Susan\"],\n            [\"Title\", \"Foundation\"],\n            [\"Title\", \"Accountant\"],\n            [\"Title\", \"Accountant\"],\n            [\"Title\", \"Executive\"],\n            [\"Title\", \"Director\"],\n            [\"Title\", \"Executive\"],\n            [\"Name\", \"Ray\"],\n            [\"Title\", \"Strategic Planning\"],\n            [\"Title\", \"Financial Analyst\"],\n            [\"Title\", \"Foundation\"],\n            [\"Title\", \"Foundation\"],\n            [\"Name\", \"Susan\"],\n            [\"Title\", \"member of the board\"],\n            [\"Title\", \"board of directors\"],\n            [\"Title\", \"president\"],\n            [\"Title\", \"board of directors\"],\n        ],\n        [\n            [\"Name\", \"Carl Clark\"],\n            [\"Title\", \"Chief \"],\n            [\"Title\", \"Operating Officer\"],\n            [\"Title\", \"PhD\"],\n            [\"Phone\", \"413-534-2745\"],\n            [\"Email\", \"Clark_Carl@example.com\"],\n        ],\n        [\n            [\"Title\", \"Board Member\"],\n            [\"Name\", \"Taylor Garcia\"],\n            [\"Phone\", \"307-733-2164\"],\n            [\"Phone\", \"307-733-4568\"],\n            [\"Email\", \"Garcia@example.org\"],\n        ],\n    ],\n}\n\nLet convert this dictionary into Pandas DataFrame and see what data we have.\n\ndf = pd.DataFrame(data)\n\ndf.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\n\n\n\n\n0\nA\n[[Name, David Jones], [Title, CEO], [Phone, 20...\n\n\n1\nB\n[[Name, Kate Brown], [Title, Senior Lawyer], [...\n\n\n2\nC\n[[Name, Scott Lamb], [Title, Actuary], [Phone,...\n\n\n3\nD\n[[Name, Rose Smith Rose Smith], [Title, Vice P...\n\n\n4\nE\n[[Name, Carl Clark], [Title, Chief ], [Title, ...\n\n\n\n\n\n\n\nWe see that the first column seems okay, but the second one not. Here we have one big list containing smaller two-element lists.\n\ndf[\"info\"].iloc[2]\n\n[['Name', 'Scott Lamb'],\n ['Title', 'Actuary'],\n ['Phone', '316-978-3804'],\n ['Email', 'scott.lamb@example.edu'],\n ['Name', 'Scott Lamb'],\n ['Title', 'Senior Officer'],\n ['Title', 'Application Developer'],\n ['Title', 'Blockchain Architect'],\n ['Title', 'Director of External Affairs'],\n ['Name', 'Scott'],\n ['Name', 'Scott'],\n ['Title', 'Director of Medicine'],\n ['Title', 'Product Owner'],\n ['Name', 'Mike'],\n ['Title', 'Domain Expert'],\n ['Title', 'Growth Hacker'],\n ['Title', 'Engineering Head'],\n ['Title', 'Event Manager'],\n ['Name', 'Joe'],\n ['Name', 'Mike'],\n ['Title', 'Fundraising'],\n ['Title', 'VP of Customers'],\n ['Name', 'Mike'],\n ['Title', 'Venture Capital Analyst'],\n ['Title', 'UX Designer'],\n ['Name', 'Mike'],\n ['Name', 'Susan'],\n ['Name', 'Bryan'],\n ['Name', 'Mia'],\n ['Title', 'Songwriter']]\n\n\nAs we figured out the data structure, let define the aim.\nWe need to unpack lists from the second column and flatten them in tabular format in the way to preserve the order. Meaning that, from the above example, Scott Lamb has to have title Actuary and not other titles are allowed. Long story short, we need to flatten list of list and make proper DataFrame from it.\n\n\n\nThe first idea that came to my mind was to use Pandas DataFrame .explode() method to unpack list of lists, which returned lists containing two elements. After that, I extracted these two elements into two different columns.\n\ndf_exploded = df.explode(\"info\")\n\ndf_exploded.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\n\n\n\n\n0\nA\n[Name, David Jones]\n\n\n0\nA\n[Title, CEO]\n\n\n0\nA\n[Phone, 207-685-1626]\n\n\n0\nA\n[Email, djones@example.org]\n\n\n1\nB\n[Name, Kate Brown]\n\n\n\n\n\n\n\n\n# Add two new columns\ndf_exploded.loc[:, \"tag\"] = df_exploded[\"info\"].map(lambda x: x[0])\n\ndf_exploded.loc[:, \"result\"] = df_exploded[\"info\"].map(lambda x: x[1])\n\ndf_exploded.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\ntag\nresult\n\n\n\n\n0\nA\n[Name, David Jones]\nName\nDavid Jones\n\n\n0\nA\n[Title, CEO]\nTitle\nCEO\n\n\n0\nA\n[Phone, 207-685-1626]\nPhone\n207-685-1626\n\n\n0\nA\n[Email, djones@example.org]\nEmail\ndjones@example.org\n\n\n1\nB\n[Name, Kate Brown]\nName\nKate Brown\n\n\n\n\n\n\n\nDespite unpacking the list of lists, that is not the format I wanted. So, I need to do an extra transformation.\n\ndf_exploded_final = (\n    df_exploded.groupby([\"company_name\", \"tag\"])[\"result\"]\n    .apply(lambda x: pd.Series(x.values))\n    .unstack(1)\n    .reset_index()\n    .drop([\"level_1\"], axis=1)\n)\n\ndf_exploded_final.head()\n\n\n\n\n\n\n\ntag\ncompany_name\nEmail\nName\nPhone\nTitle\n\n\n\n\n0\nA\ndjones@example.org\nDavid Jones\n207-685-1626\nCEO\n\n\n1\nB\nKate.Brown@example.edu\nKate Brown\n316-978-7791\nSenior Lawyer\n\n\n2\nB\nDarin.White@example.edu\nDarin White\n316-978-3887\nAssociate Vice President\n\n\n3\nC\nscott.lamb@example.edu\nScott Lamb\n316-978-3804\nActuary\n\n\n4\nC\nNaN\nScott Lamb\nNaN\nSenior Officer\n\n\n\n\n\n\n\nIt seems we did a good job. However, this approach is prone to errors. Namely, it does not preserve the order of the list values and may assign a different email to a different person. That was not what I need.\nSo, I decided to use another way to solve this problem. Notably, as the data contained millions of rows, it seemed impossible to be too precise, but I wanted to reduce the error of non-matching cases. To achieve this, I iterated over the values of info column and converted it to dict of lists, where keys are tags (identifiers) and values are actual employee information.\n\nout = []\n\nfor x in df[\"info\"].tolist():\n    groups = defaultdict(list)\n    for g, v in x:\n        groups[g].append(v)\n    out.append(dict(groups))\n\n\ndf.loc[:, \"new_info\"] = out\n\ndf[\"new_info\"].iloc[0]\n\n{'Name': ['David Jones'],\n 'Title': ['CEO'],\n 'Phone': ['207-685-1626'],\n 'Email': ['djones@example.org']}\n\n\nThat’s a step forward. After this, I was interested in counting the values for each key in dicts for each row. I made small changes in the above code and applied it to the new_info column.\n\nout = []\n\nfor x in df[\"new_info\"]:\n    groups = defaultdict(int)\n    for g, v in x.items():\n        groups[g] = len(list(filter(None, v)))\n    out.append(dict(groups))\n\n\ndf.loc[:, \"new_info_stats\"] = out\n\ndf[\"new_info_stats\"].iloc[0]\n\n{'Name': 1, 'Title': 1, 'Phone': 1, 'Email': 1}\n\n\nAs we calculated value counts for each dict, now we need to add three helper columns to the dataset for further usage. These helper columns will help to differentiate matching cases and non-matching cases.\n\ndf[\"_max\"] = df[\"new_info_stats\"].apply(lambda x: max(x.values()))\n\ndf[\"_min\"] = df[\"new_info_stats\"].apply(lambda x: min(x.values()))\n\ndf.loc[:, \"max_equal_min\"] = pd.Series(np.where((df[\"_max\"] == df[\"_min\"]), 1, 0))\n\nThe column max_equal_min is a dummy variable and helps us to differentiate matching and non-matching cases. The value 1 indicates we have a matching case and value 0 - non-matching case. According to this column, I split data into two parts. The first only contains matching examples, and the second will have only non-matching cases.\n\ndf_first = df[df[\"max_equal_min\"] &gt; 0].reset_index(drop=True)\n\ndf_second = df[df[\"max_equal_min\"] == 0].reset_index(drop=True)\n\nThe pre-processing of the first DataFrame is over and is ready to flatten. To do so, I iterate over new_info column and transform each row into Pandas DataFrame. After this step, data will be flat.\n\nnew_data = []\n\nfor j in df_first[\"new_info\"]:\n    new_data.append(pd.DataFrame(j))\n\n\ndf_first_final_i = (\n    pd.concat(new_data, axis=0, sort=False).drop_duplicates().dropna(subset=[\"Name\"]).reset_index(drop=True)\n)\n\ndf_first_final_i.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nDavid Jones\nCEO\n207-685-1626\ndjones@example.org\n\n\n1\nKate Brown\nSenior Lawyer\n316-978-7791\nKate.Brown@example.edu\n\n\n2\nDarin White\nAssociate Vice President\n316-978-3887\nDarin.White@example.edu\n\n\n\n\n\n\n\nWoohoo, it works! However, imagine having millions of rows how slow this approach will be. For this reason, I tried another method and found it much faster. Here it is.\n\ndf_first_final_ii = (\n    df_first[\"new_info\"].apply(pd.Series).apply(lambda x: x.explode()).drop_duplicates().reset_index(drop=True)\n)\n\ndf_first_final_ii.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nDavid Jones\nCEO\n207-685-1626\ndjones@example.org\n\n\n1\nKate Brown\nSenior Lawyer\n316-978-7791\nKate.Brown@example.edu\n\n\n2\nDarin White\nAssociate Vice President\n316-978-3887\nDarin.White@example.edu\n\n\n\n\n\n\n\nFaster and cleaner solution. But, what about the second DataFrame? It turned out that the above solution did not fit the second DataFrame and gave me an error. The error was ValueError: cannot reindex from a duplicate axis. Before finding the solution for this error, let take a look at the data.\n\ndf_second.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\nnew_info\nnew_info_stats\n_max\n_min\nmax_equal_min\n\n\n\n\n0\nC\n[[Name, Scott Lamb], [Title, Actuary], [Phone,...\n{'Name': ['Scott Lamb', 'Scott Lamb', 'Scott',...\n{'Name': 12, 'Title': 16, 'Phone': 1, 'Email': 1}\n16\n1\n0\n\n\n1\nD\n[[Name, Rose Smith Rose Smith], [Title, Vice P...\n{'Name': ['Rose Smith Rose Smith', 'Rose Smith...\n{'Name': 5, 'Title': 17, 'Phone': 1, 'Email': 1}\n17\n1\n0\n\n\n2\nE\n[[Name, Carl Clark], [Title, Chief ], [Title, ...\n{'Name': ['Carl Clark'], 'Title': ['Chief ', '...\n{'Name': 1, 'Title': 3, 'Phone': 1, 'Email': 1}\n3\n1\n0\n\n\n3\nF\n[[Title, Board Member], [Name, Taylor Garcia],...\n{'Title': ['Board Member'], 'Name': ['Taylor G...\n{'Title': 1, 'Name': 1, 'Phone': 2, 'Email': 1}\n2\n1\n0\n\n\n\n\n\n\n\nIn the second and third row, we have one Name and three Title and two Phone, respectively, for the new_info_stats column. This may be due to the data entry or extraction reason. Not 100% sure that this is the case, but the likelihood is very high. So, we have to handle this problem properly. One solution is to concatenate strings for the values of Title and Phone keys.\n\ndef process_info(record: dict) -&gt; dict:\n    if (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) &gt; 1\n        and len(record.get(\"Email\")) == 1\n        and len(record.get(\"Phone\")) == 1\n    ):\n        record[\"Title\"] = [\" \".join(record.get(\"Title\"))]\n\n    elif (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) == 1\n        and len(record.get(\"Email\")) &gt; 1\n        and len(record.get(\"Phone\")) == 1\n    ):\n        record[\"Email\"] = [\",\".join(record.get(\"Email\"))]\n\n    elif (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) == 1\n        and len(record.get(\"Email\")) == 1\n        and len(record.get(\"Phone\")) &gt; 1\n    ):\n        record[\"Phone\"] = [\",\".join(record.get(\"Phone\"))]\n\n    else:\n        pass\n    return record\n\n\ndf_second[\"new_info\"] = df_second[\"new_info\"].apply(process_info)\n\nThis is a simple logic to check if we are correctly concatenating string. After applying this function, the second DataFrame is ready to flatten. As I mentioned above the good old method did not give me the desired result for this case and then I came up to the following:\n\ndef flatten(df, column):\n    data = []\n    for i in df[column]:\n        data.append(pd.DataFrame(dict([(k, pd.Series(v)) for k, v in i.items()])))\n\n    new_df = (\n        pd.concat(data, axis=0, sort=False)\n        .drop_duplicates()\n        .dropna(subset=[\"Name\"])\n        .drop_duplicates(subset=[\"Name\", \"Title\", \"Phone\", \"Email\"])\n        .reset_index(drop=True)\n    )\n\n    return new_df\n\n\ndf_second_final = flatten(df_second, \"new_info\")\n\ndf_second_final.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nScott Lamb\nActuary\n316-978-3804\nscott.lamb@example.edu\n\n\n1\nScott Lamb\nSenior Officer\nNaN\nNaN\n\n\n2\nScott\nApplication Developer\nNaN\nNaN\n\n\n3\nScott\nBlockchain Architect\nNaN\nNaN\n\n\n4\nMike\nDirector of External Affairs\nNaN\nNaN\n\n\n\n\n\n\n\nApplied this function to the second DataFrame flattened it, and combining first and second DataFrames will give the final result."
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_1.html#problem-statement",
    "href": "posts/pandas/pandas_tips_and_tricks_1.html#problem-statement",
    "title": "Lost in Pandas - Part 1",
    "section": "",
    "text": "We have data. This data comes from the HR department of the company. The data contains two columns, company name, and information about its employees. Each row of the employee information column is a list of lists. The lists inside the outer list can be duplicated. It also can have duplicate values, and inner lists have at most two values.\n\nDisclaimer: Any name, phone, email, and the title is a pure coincidence. Data is random and fake.\n\nHere is our data.\n\nfrom collections import defaultdict\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = {\n    \"company_name\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n    \"info\": [\n        [[\"Name\", \"David Jones\"], [\"Title\", \"CEO\"], [\"Phone\", \"207-685-1626\"], [\"Email\", \"djones@example.org\"]],\n        [\n            [\"Name\", \"Kate Brown\"],\n            [\"Title\", \"Senior Lawyer\"],\n            [\"Phone\", \"316-978-7791\"],\n            [\"Email\", \"Kate.Brown@example.edu\"],\n            [\"Name\", \"Darin White\"],\n            [\"Title\", \"Associate Vice President\"],\n            [\"Phone\", \"316-978-3887\"],\n            [\"Email\", \"Darin.White@example.edu\"],\n        ],\n        [\n            [\"Name\", \"Scott Lamb\"],\n            [\"Title\", \"Actuary\"],\n            [\"Phone\", \"316-978-3804\"],\n            [\"Email\", \"scott.lamb@example.edu\"],\n            [\"Name\", \"Scott Lamb\"],\n            [\"Title\", \"Senior Officer\"],\n            [\"Title\", \"Application Developer\"],\n            [\"Title\", \"Blockchain Architect\"],\n            [\"Title\", \"Director of External Affairs\"],\n            [\"Name\", \"Scott\"],\n            [\"Name\", \"Scott\"],\n            [\"Title\", \"Director of Medicine\"],\n            [\"Title\", \"Product Owner\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Domain Expert\"],\n            [\"Title\", \"Growth Hacker\"],\n            [\"Title\", \"Engineering Head\"],\n            [\"Title\", \"Event Manager\"],\n            [\"Name\", \"Joe\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Fundraising\"],\n            [\"Title\", \"VP of Customers\"],\n            [\"Name\", \"Mike\"],\n            [\"Title\", \"Venture Capital Analyst\"],\n            [\"Title\", \"UX Designer\"],\n            [\"Name\", \"Mike\"],\n            [\"Name\", \"Susan\"],\n            [\"Name\", \"Bryan\"],\n            [\"Name\", \"Mia\"],\n            [\"Title\", \"Songwriter\"],\n        ],\n        [\n            [\"Name\", \"Rose Smith Rose Smith\"],\n            [\"Title\", \"Vice President\"],\n            [\"Title\", \"Finance and Operations Head\"],\n            [\"Phone\", \"316-978-3810\"],\n            [\"Email\", \"rose.smith@example.edu\"],\n            [\"Name\", \"Rose Smith\"],\n            [\"Title\", \"Foundation\"],\n            [\"Name\", \"Susan\"],\n            [\"Title\", \"Foundation\"],\n            [\"Title\", \"Accountant\"],\n            [\"Title\", \"Accountant\"],\n            [\"Title\", \"Executive\"],\n            [\"Title\", \"Director\"],\n            [\"Title\", \"Executive\"],\n            [\"Name\", \"Ray\"],\n            [\"Title\", \"Strategic Planning\"],\n            [\"Title\", \"Financial Analyst\"],\n            [\"Title\", \"Foundation\"],\n            [\"Title\", \"Foundation\"],\n            [\"Name\", \"Susan\"],\n            [\"Title\", \"member of the board\"],\n            [\"Title\", \"board of directors\"],\n            [\"Title\", \"president\"],\n            [\"Title\", \"board of directors\"],\n        ],\n        [\n            [\"Name\", \"Carl Clark\"],\n            [\"Title\", \"Chief \"],\n            [\"Title\", \"Operating Officer\"],\n            [\"Title\", \"PhD\"],\n            [\"Phone\", \"413-534-2745\"],\n            [\"Email\", \"Clark_Carl@example.com\"],\n        ],\n        [\n            [\"Title\", \"Board Member\"],\n            [\"Name\", \"Taylor Garcia\"],\n            [\"Phone\", \"307-733-2164\"],\n            [\"Phone\", \"307-733-4568\"],\n            [\"Email\", \"Garcia@example.org\"],\n        ],\n    ],\n}\n\nLet convert this dictionary into Pandas DataFrame and see what data we have.\n\ndf = pd.DataFrame(data)\n\ndf.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\n\n\n\n\n0\nA\n[[Name, David Jones], [Title, CEO], [Phone, 20...\n\n\n1\nB\n[[Name, Kate Brown], [Title, Senior Lawyer], [...\n\n\n2\nC\n[[Name, Scott Lamb], [Title, Actuary], [Phone,...\n\n\n3\nD\n[[Name, Rose Smith Rose Smith], [Title, Vice P...\n\n\n4\nE\n[[Name, Carl Clark], [Title, Chief ], [Title, ...\n\n\n\n\n\n\n\nWe see that the first column seems okay, but the second one not. Here we have one big list containing smaller two-element lists.\n\ndf[\"info\"].iloc[2]\n\n[['Name', 'Scott Lamb'],\n ['Title', 'Actuary'],\n ['Phone', '316-978-3804'],\n ['Email', 'scott.lamb@example.edu'],\n ['Name', 'Scott Lamb'],\n ['Title', 'Senior Officer'],\n ['Title', 'Application Developer'],\n ['Title', 'Blockchain Architect'],\n ['Title', 'Director of External Affairs'],\n ['Name', 'Scott'],\n ['Name', 'Scott'],\n ['Title', 'Director of Medicine'],\n ['Title', 'Product Owner'],\n ['Name', 'Mike'],\n ['Title', 'Domain Expert'],\n ['Title', 'Growth Hacker'],\n ['Title', 'Engineering Head'],\n ['Title', 'Event Manager'],\n ['Name', 'Joe'],\n ['Name', 'Mike'],\n ['Title', 'Fundraising'],\n ['Title', 'VP of Customers'],\n ['Name', 'Mike'],\n ['Title', 'Venture Capital Analyst'],\n ['Title', 'UX Designer'],\n ['Name', 'Mike'],\n ['Name', 'Susan'],\n ['Name', 'Bryan'],\n ['Name', 'Mia'],\n ['Title', 'Songwriter']]\n\n\nAs we figured out the data structure, let define the aim.\nWe need to unpack lists from the second column and flatten them in tabular format in the way to preserve the order. Meaning that, from the above example, Scott Lamb has to have title Actuary and not other titles are allowed. Long story short, we need to flatten list of list and make proper DataFrame from it."
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_1.html#how-i-approached-this-problem",
    "href": "posts/pandas/pandas_tips_and_tricks_1.html#how-i-approached-this-problem",
    "title": "Lost in Pandas - Part 1",
    "section": "",
    "text": "The first idea that came to my mind was to use Pandas DataFrame .explode() method to unpack list of lists, which returned lists containing two elements. After that, I extracted these two elements into two different columns.\n\ndf_exploded = df.explode(\"info\")\n\ndf_exploded.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\n\n\n\n\n0\nA\n[Name, David Jones]\n\n\n0\nA\n[Title, CEO]\n\n\n0\nA\n[Phone, 207-685-1626]\n\n\n0\nA\n[Email, djones@example.org]\n\n\n1\nB\n[Name, Kate Brown]\n\n\n\n\n\n\n\n\n# Add two new columns\ndf_exploded.loc[:, \"tag\"] = df_exploded[\"info\"].map(lambda x: x[0])\n\ndf_exploded.loc[:, \"result\"] = df_exploded[\"info\"].map(lambda x: x[1])\n\ndf_exploded.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\ntag\nresult\n\n\n\n\n0\nA\n[Name, David Jones]\nName\nDavid Jones\n\n\n0\nA\n[Title, CEO]\nTitle\nCEO\n\n\n0\nA\n[Phone, 207-685-1626]\nPhone\n207-685-1626\n\n\n0\nA\n[Email, djones@example.org]\nEmail\ndjones@example.org\n\n\n1\nB\n[Name, Kate Brown]\nName\nKate Brown\n\n\n\n\n\n\n\nDespite unpacking the list of lists, that is not the format I wanted. So, I need to do an extra transformation.\n\ndf_exploded_final = (\n    df_exploded.groupby([\"company_name\", \"tag\"])[\"result\"]\n    .apply(lambda x: pd.Series(x.values))\n    .unstack(1)\n    .reset_index()\n    .drop([\"level_1\"], axis=1)\n)\n\ndf_exploded_final.head()\n\n\n\n\n\n\n\ntag\ncompany_name\nEmail\nName\nPhone\nTitle\n\n\n\n\n0\nA\ndjones@example.org\nDavid Jones\n207-685-1626\nCEO\n\n\n1\nB\nKate.Brown@example.edu\nKate Brown\n316-978-7791\nSenior Lawyer\n\n\n2\nB\nDarin.White@example.edu\nDarin White\n316-978-3887\nAssociate Vice President\n\n\n3\nC\nscott.lamb@example.edu\nScott Lamb\n316-978-3804\nActuary\n\n\n4\nC\nNaN\nScott Lamb\nNaN\nSenior Officer\n\n\n\n\n\n\n\nIt seems we did a good job. However, this approach is prone to errors. Namely, it does not preserve the order of the list values and may assign a different email to a different person. That was not what I need.\nSo, I decided to use another way to solve this problem. Notably, as the data contained millions of rows, it seemed impossible to be too precise, but I wanted to reduce the error of non-matching cases. To achieve this, I iterated over the values of info column and converted it to dict of lists, where keys are tags (identifiers) and values are actual employee information.\n\nout = []\n\nfor x in df[\"info\"].tolist():\n    groups = defaultdict(list)\n    for g, v in x:\n        groups[g].append(v)\n    out.append(dict(groups))\n\n\ndf.loc[:, \"new_info\"] = out\n\ndf[\"new_info\"].iloc[0]\n\n{'Name': ['David Jones'],\n 'Title': ['CEO'],\n 'Phone': ['207-685-1626'],\n 'Email': ['djones@example.org']}\n\n\nThat’s a step forward. After this, I was interested in counting the values for each key in dicts for each row. I made small changes in the above code and applied it to the new_info column.\n\nout = []\n\nfor x in df[\"new_info\"]:\n    groups = defaultdict(int)\n    for g, v in x.items():\n        groups[g] = len(list(filter(None, v)))\n    out.append(dict(groups))\n\n\ndf.loc[:, \"new_info_stats\"] = out\n\ndf[\"new_info_stats\"].iloc[0]\n\n{'Name': 1, 'Title': 1, 'Phone': 1, 'Email': 1}\n\n\nAs we calculated value counts for each dict, now we need to add three helper columns to the dataset for further usage. These helper columns will help to differentiate matching cases and non-matching cases.\n\ndf[\"_max\"] = df[\"new_info_stats\"].apply(lambda x: max(x.values()))\n\ndf[\"_min\"] = df[\"new_info_stats\"].apply(lambda x: min(x.values()))\n\ndf.loc[:, \"max_equal_min\"] = pd.Series(np.where((df[\"_max\"] == df[\"_min\"]), 1, 0))\n\nThe column max_equal_min is a dummy variable and helps us to differentiate matching and non-matching cases. The value 1 indicates we have a matching case and value 0 - non-matching case. According to this column, I split data into two parts. The first only contains matching examples, and the second will have only non-matching cases.\n\ndf_first = df[df[\"max_equal_min\"] &gt; 0].reset_index(drop=True)\n\ndf_second = df[df[\"max_equal_min\"] == 0].reset_index(drop=True)\n\nThe pre-processing of the first DataFrame is over and is ready to flatten. To do so, I iterate over new_info column and transform each row into Pandas DataFrame. After this step, data will be flat.\n\nnew_data = []\n\nfor j in df_first[\"new_info\"]:\n    new_data.append(pd.DataFrame(j))\n\n\ndf_first_final_i = (\n    pd.concat(new_data, axis=0, sort=False).drop_duplicates().dropna(subset=[\"Name\"]).reset_index(drop=True)\n)\n\ndf_first_final_i.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nDavid Jones\nCEO\n207-685-1626\ndjones@example.org\n\n\n1\nKate Brown\nSenior Lawyer\n316-978-7791\nKate.Brown@example.edu\n\n\n2\nDarin White\nAssociate Vice President\n316-978-3887\nDarin.White@example.edu\n\n\n\n\n\n\n\nWoohoo, it works! However, imagine having millions of rows how slow this approach will be. For this reason, I tried another method and found it much faster. Here it is.\n\ndf_first_final_ii = (\n    df_first[\"new_info\"].apply(pd.Series).apply(lambda x: x.explode()).drop_duplicates().reset_index(drop=True)\n)\n\ndf_first_final_ii.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nDavid Jones\nCEO\n207-685-1626\ndjones@example.org\n\n\n1\nKate Brown\nSenior Lawyer\n316-978-7791\nKate.Brown@example.edu\n\n\n2\nDarin White\nAssociate Vice President\n316-978-3887\nDarin.White@example.edu\n\n\n\n\n\n\n\nFaster and cleaner solution. But, what about the second DataFrame? It turned out that the above solution did not fit the second DataFrame and gave me an error. The error was ValueError: cannot reindex from a duplicate axis. Before finding the solution for this error, let take a look at the data.\n\ndf_second.head()\n\n\n\n\n\n\n\n\ncompany_name\ninfo\nnew_info\nnew_info_stats\n_max\n_min\nmax_equal_min\n\n\n\n\n0\nC\n[[Name, Scott Lamb], [Title, Actuary], [Phone,...\n{'Name': ['Scott Lamb', 'Scott Lamb', 'Scott',...\n{'Name': 12, 'Title': 16, 'Phone': 1, 'Email': 1}\n16\n1\n0\n\n\n1\nD\n[[Name, Rose Smith Rose Smith], [Title, Vice P...\n{'Name': ['Rose Smith Rose Smith', 'Rose Smith...\n{'Name': 5, 'Title': 17, 'Phone': 1, 'Email': 1}\n17\n1\n0\n\n\n2\nE\n[[Name, Carl Clark], [Title, Chief ], [Title, ...\n{'Name': ['Carl Clark'], 'Title': ['Chief ', '...\n{'Name': 1, 'Title': 3, 'Phone': 1, 'Email': 1}\n3\n1\n0\n\n\n3\nF\n[[Title, Board Member], [Name, Taylor Garcia],...\n{'Title': ['Board Member'], 'Name': ['Taylor G...\n{'Title': 1, 'Name': 1, 'Phone': 2, 'Email': 1}\n2\n1\n0\n\n\n\n\n\n\n\nIn the second and third row, we have one Name and three Title and two Phone, respectively, for the new_info_stats column. This may be due to the data entry or extraction reason. Not 100% sure that this is the case, but the likelihood is very high. So, we have to handle this problem properly. One solution is to concatenate strings for the values of Title and Phone keys.\n\ndef process_info(record: dict) -&gt; dict:\n    if (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) &gt; 1\n        and len(record.get(\"Email\")) == 1\n        and len(record.get(\"Phone\")) == 1\n    ):\n        record[\"Title\"] = [\" \".join(record.get(\"Title\"))]\n\n    elif (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) == 1\n        and len(record.get(\"Email\")) &gt; 1\n        and len(record.get(\"Phone\")) == 1\n    ):\n        record[\"Email\"] = [\",\".join(record.get(\"Email\"))]\n\n    elif (\n        len(record.keys()) == 4\n        and len(record.get(\"Name\")) == 1\n        and len(record.get(\"Title\")) == 1\n        and len(record.get(\"Email\")) == 1\n        and len(record.get(\"Phone\")) &gt; 1\n    ):\n        record[\"Phone\"] = [\",\".join(record.get(\"Phone\"))]\n\n    else:\n        pass\n    return record\n\n\ndf_second[\"new_info\"] = df_second[\"new_info\"].apply(process_info)\n\nThis is a simple logic to check if we are correctly concatenating string. After applying this function, the second DataFrame is ready to flatten. As I mentioned above the good old method did not give me the desired result for this case and then I came up to the following:\n\ndef flatten(df, column):\n    data = []\n    for i in df[column]:\n        data.append(pd.DataFrame(dict([(k, pd.Series(v)) for k, v in i.items()])))\n\n    new_df = (\n        pd.concat(data, axis=0, sort=False)\n        .drop_duplicates()\n        .dropna(subset=[\"Name\"])\n        .drop_duplicates(subset=[\"Name\", \"Title\", \"Phone\", \"Email\"])\n        .reset_index(drop=True)\n    )\n\n    return new_df\n\n\ndf_second_final = flatten(df_second, \"new_info\")\n\ndf_second_final.head()\n\n\n\n\n\n\n\n\nName\nTitle\nPhone\nEmail\n\n\n\n\n0\nScott Lamb\nActuary\n316-978-3804\nscott.lamb@example.edu\n\n\n1\nScott Lamb\nSenior Officer\nNaN\nNaN\n\n\n2\nScott\nApplication Developer\nNaN\nNaN\n\n\n3\nScott\nBlockchain Architect\nNaN\nNaN\n\n\n4\nMike\nDirector of External Affairs\nNaN\nNaN\n\n\n\n\n\n\n\nApplied this function to the second DataFrame flattened it, and combining first and second DataFrames will give the final result."
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_3.html",
    "href": "posts/pandas/pandas_tips_and_tricks_3.html",
    "title": "Lost in Pandas - Part 3",
    "section": "",
    "text": "Introduction\nI’ll show you how to split one column with repetitive index and/or value into multiple rows with unique index.\n\nimport pandas as pd\n\n\ndata = {\n    \"ID\": [100, 100, 200, 200, 200, 300, 300, 400],\n    \"Product_Name\": [\"Apple\", \"Banana\", \"Cherry\", \"Apricot\", \"Apple\", \"Avocado\", \"Avocado\", \"Orange\"],\n}\n\ndf = pd.DataFrame(data)\n\ndf\n\n\n\n\n\n\n\n\nID\nProduct_Name\n\n\n\n\n0\n100\nApple\n\n\n1\n100\nBanana\n\n\n2\n200\nCherry\n\n\n3\n200\nApricot\n\n\n4\n200\nApple\n\n\n5\n300\nAvocado\n\n\n6\n300\nAvocado\n\n\n7\n400\nOrange\n\n\n\n\n\n\n\nWe see that our dataframe contains repetitive index (ID) and also not unique values for each unique index. Our aim is to convert Product_Name column into rows such as to keep index unique and in each new colum to be each product name. We need the following format:\n\n\n\nID\n0\n1\n2\n\n\n\n\n100\nApple\nBanana\nNaN\n\n\n200\nCherry\nApricot\nApple\n\n\n300\nAvocado\nAvocado\nNaN\n\n\n400\nOrange\nNaN\nNaN\n\n\n\nTo achieve our aim we can use cumcount for new columns names to MultiIndex by set_index and reshape by unstack. The second way to do this will be to create Series of lists and new DataFrame by contructor:\n\ndf.set_index([\"ID\", df.groupby(\"ID\").cumcount()])[\"Product_Name\"].unstack()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\nID\n\n\n\n\n\n\n\n100\nApple\nBanana\nNaN\n\n\n200\nCherry\nApricot\nApple\n\n\n300\nAvocado\nAvocado\nNaN\n\n\n400\nOrange\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Second way of doing the same as above\n\ns = df.groupby(\"ID\")[\"Product_Name\"].apply(list)\n\npd.DataFrame(s.values.tolist(), index=s.index)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\nID\n\n\n\n\n\n\n\n100\nApple\nBanana\nNone\n\n\n200\nCherry\nApricot\nApple\n\n\n300\nAvocado\nAvocado\nNone\n\n\n400\nOrange\nNone\nNone\n\n\n\n\n\n\n\nThat’s good. We found the solution, but what if we want to have just two columns. I mean to aggregate Product_Name column values in one cell for each unique index? That’s even easy to do. Use groupby then apply join and reset_index\n\ndf.groupby(\"ID\")[\"Product_Name\"].apply(\" \".join).reset_index(name=\"New_Product_Name\")\n\n\n\n\n\n\n\n\nID\nNew_Product_Name\n\n\n\n\n0\n100\nApple Banana\n\n\n1\n200\nCherry Apricot Apple\n\n\n2\n300\nAvocado Avocado\n\n\n3\n400\nOrange"
  },
  {
    "objectID": "posts/economics/single_market_economy.html",
    "href": "posts/economics/single_market_economy.html",
    "title": "Modeling Single Good Market",
    "section": "",
    "text": "In this blog, I want to review simple one good market economy, where supply and demand are linear functions of price and agents of this economy are price takers. Those agents are divided into two categories: buyers and sellers. The price of a product is determined by the interaction of demand and supply or in other words buyers and sellers. Here, the buyer is associated with demand and seller is associated with supply. Henceforth, in this market, there are no frictions or market imperfections (more on this later) and market is competitive. So, this should be a very simple environment, where buyer demands product and seller sells this product at a given price. Before continue, it’s worth to define some key terms and notions in our simple market model.\nFirst, let’s start by defining what is economics. In simple words, economics is the social science which studies production, consumption, and distribution of goods and services. I used, three main terms above, which constitutes our simple economy: demand, supply, and price.\n\nDemand is all the quantities of good or service that buyers are willing and able to buy at all possible prices.\n\nDemand is based on needs and wants. From the consumer point of view, we all know the difference but from an economist’s perspective, they are the same. Moreover, demand is based on the ability to pay, If the agent cannot afford to pay, they have no effective demand.\n\nThe law of demand states that a higher price leads to a lower quantity demanded and a lower price leads to a higher quantity demanded\n\nMake it simple, low of demand says that there is an inverse relationship between price and quantity demanded and this relationship determines downward sloping demand curve.\n\nSupply is all the possible quantities that sellers are willing and able to produce at all possible prices.\n\nWhen an economist talks about to supply, they mean the amount of some good or service a producer is willing to supply at each price. Price is what the producer receives for selling one unit of good or service.\n\nThe low of supply states that a higher price leads to a higher quantity and lower price leads to lower quantity supplied.\n\nLow of supply says that there is positive relationship between price and quantity supplied, leading to an upward sloping supply curve.\nI mentioned earlier that our market is competitive and in competitive market demand for good or service and supply will determine the price. This price is the equilibrium price.\n\nEquilibrium price is the price for which quantity demanded and quantity supplied are equal.\n\nIn our market settings, equilibrium occurs when the price has adjusted until the quantity supplied is equal to quantity demanded. When this equality is violated market experiences disequilibrium. When a market is experiencing a disequilibrium, there will be either a shortage or a surplus.\n\nWhen the price is above the equilibrium price, this produce surplus, which encourages sellers to lower their prices to eliminate the surplus.\n\n\nAt any price below the equilibrium price, the shortage will exist, which leads to the price of the good increasing.\n\n\n\nAt that moment we defined all the necessary terms and notions in our simple economy to continue our analysis and modeling. Now, it’s time to ask ourselves. Can we model mathematically this economy and see the dynamics of the buyers and sellers? Yes, we can do and we can go farther and make a simple Python script to make simulations in this economy.\nLet’s start our modeling part with defining functions for demand and supply. Logically, demand and supply are generated from some very complicated function and its real anatomy is unknown, but as we are in simple economy we can assume that they are linear functions of price:\nA liner demand curve:\n\\[\nQ =\na - b\\cdot p\n\\]\nA linear supply curve:\n\\[\nQ =\nc + d\\cdot p\n\\]\nHere, \\(p\\) is the price paid by the consumer and \\(Q\\) is the quantity. \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are demand and supply parameters.\nLet first solve this mathematically. We have the equation for quantity and the parameters \\(a\\) and \\(b\\) are predetermined, or we can choose their value. We know that in equilibrium, quantity demanded and quantity supplied are equal. So, by equating our equations and doing some simple algebra we’ll get equilibrium price. Let denote this price by \\(P^{*}\\).\n\\[\na - b\\cdot p = c + d\\cdot p \\Rightarrow\n\\\\\n\\\\ \\Rightarrow d\\cdot p + b\\cdot p = a - c\n\\\\\n\\\\ \\Rightarrow P^{*} =\n\\frac{a - c}{d + b}\n\\]\nBy having \\(P^{*}\\) we have equilibrium price. Now, let me introduce some new notions, which are very natural in this market economy, such as, consumer surplus, producer surplus, and social surplus.\n\nConsumer surplus is the gap between the price that consumers are willing to pay and equilibrium price.\n\n\nProducer surplus is the gap between the price for which producers are willing to sell a product and market equilibrium price.\n\n\nSocial surplus is the sum of consumer and producer surplus.\n\nTo calculate these values, first, we need to calculate inverse demand and supply function. Inverse demand function considers price as a function of quantity. We have quantity as a function of a price, hence we need to find its inverse.\nWe have a linear demand curve, given by\n\\[\nQ =\na - b\\cdot p\n\\]\nLet change the notation a little bit and denote demand function in the following way:\n\\[\nQ(p) =\na - b\\cdot p\n\\]\nTo find the inverse of this function we have to solve the above function for \\(p\\). Doing simple algebra, yields\n\\[\np = \\frac{a - Q}{b}\n\\]\nand this is the same to write\n\\[\np(Q) =\n\\frac{a - Q}{b}\n\\]\nDoing the same for supply, we easily find inverse supply function, given by\n\\[\np(Q) =\n\\frac{Q - c}{d}\n\\]\nConsumer surplus is the area under inverse demand function and producer surplus is the area above inverse supply function. This may not be clear but after plotting demand and supply curves it will be understandable, what is consumer and producer surplus and how to calculate them. For general sense, we need integrals of inverse demand and supply functions to calculate surplus. See the link for more information.\nAs we have all the necessary information and equations, we can start to code these equations in Python. I’m going to write a simple Python class, which will calculate equilibrium price and quantity, consumer and producer surplus.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\nfrom scipy.integrate import quad\n\nplt.style.use(\"ggplot\")\n\n\nclass Market:\n    \"\"\"A class to model a single good market with linear demand and supply functions.\"\"\"\n    def __init__(self, a, b, c, d):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        if a &lt; c:\n            raise ValueError(\"Insufficient demand!\")\n\n    def price(self):\n        \"\"\"Returns equilibrium price\"\"\"\n        return (self.a - self.c) / (self.d + self.b)\n\n    def quantity(self):\n        \"\"\"Returns equilibrium quantity\"\"\"\n        return self.a - self.b * self.price()\n\n    def inverse_demand(self, x):\n        \"\"\"Inverse demand function.\"\"\"\n        return (self.a - x) / self.b\n\n    def inverse_supply(self, x):\n        \"\"\"Inverse supply function.\"\"\"\n        return (x - self.c) / self.d\n\n    def consumer_surplus(self):\n        \"\"\"Calculates consumer surplus.\"\"\"\n        integrand = lambda x: (self.a - x) / self.b\n        area, error = quad(integrand, 0, self.quantity())\n        return area - self.price() * self.quantity()\n\n    def producer_surplus(self):\n        \"\"\"Calculates producer surplus.\"\"\"\n        integrand = lambda x: (x - self.c) / self.d\n        area, error = quad(integrand, 0, self.quantity())\n        return self.price() * self.quantity() - area\n\nLet, test our model with some base line parameters.\n\na = 15\nb = 0.5\nc = -2\nd = 0.5\n\nm = Market(a, b, c, d)\n\n\nprint(\"Equilibrium Price = \", m.price())\nprint(\"Equilibrium Quantity = \", m.quantity())\n\nEquilibrium Price =  17.0\nEquilibrium Quantity =  6.5\n\n\n\nprint('Consumer Surplus =', m.consumer_surplus())\nprint('Producer Surplus =', m.producer_surplus())\n\nConsumer Surplus = 42.25\nProducer Surplus = 42.25\n\n\nLet plot inverse demand and inverse supply curves and shade consumer and producer surplus area.\n\nq_max = m.quantity() * 2\nq_grid = np.linspace(0.0, q_max, 100)\n\npd = m.inverse_demand(q_grid)\nps = m.inverse_supply(q_grid)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(q_grid, pd, lw=2, alpha=0.6, label=\"Demand\", color=\"red\")\nax.plot(q_grid, ps, lw=2, alpha=0.6, label=\"Supply\", color=\"blue\")\n\nax.axhline(17, 0, 0.5, linewidth=2, color=\"black\", linestyle=\"dashed\")\nax.axvline(6.5, 0, 0.5, linewidth=2, color=\"black\", linestyle=\"dashed\")\n\npath1 = Path([[0, 17], [0, 30], [6.5, 17], [0, 17]])\npatch1 = PathPatch(path1, facecolor=\"silver\")\nax.add_patch(patch1)\n\npath2 = Path([[0, 4], [0, 17], [6.5, 17], [0, 4]])\npatch2 = PathPatch(path2, facecolor=\"plum\")\nax.add_patch(patch2)\n\nax.text(6.5, 17, \"Equilibrium Point\", fontsize=12)\nax.text(1, 22, \"Consumer Surplus\", fontsize=12)\nax.text(1, 14, \"Producer Surplus\", fontsize=12)\n\nax.text(0, 17, \"$P^{*}$\", fontsize=14)\nax.text(6.5, 0, \"$Q^{*}$\", fontsize=14)\n\nax.set_xlabel(\"Quantity\", labelpad=20)\nax.set_xlim(0, q_max)\nax.set_ylabel(\"Price\")\nax.legend(loc=\"best\")\n\n\n\n\n\n\n\n\nIn the above graph, we see the equilibrium point. We can ask the following question. What will happen to the equilibrium price and quantity if demand or supply curve shits? Let make a table and summarize the changes in equilibrium due to changes in supply and demand curve.\n\n\n\n\n\n\n\n\nChange\nChange in\\(P^{*}\\)\nChange in\\(Q^{*}\\)\n\n\n\n\nSupply increases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nSupply decreases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\nDemand increases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nDemand decreases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\nDemand increases, Supply increases\n\\(P\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nDemand increases, Supply decreases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\n\nDemand decreases, Supply increases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\n\nDemand decreases, Supply decreases\n\\(P\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\n\nFrom the above table, we see that in some cases due to the simultaneous changes in supply and demand we cannot determine equilibrium price or quantity movement. That’s the case when other factors are in consideration, but these factors are out of the scope for this blog."
  },
  {
    "objectID": "posts/economics/single_market_economy.html#model",
    "href": "posts/economics/single_market_economy.html#model",
    "title": "Modeling Single Good Market",
    "section": "",
    "text": "At that moment we defined all the necessary terms and notions in our simple economy to continue our analysis and modeling. Now, it’s time to ask ourselves. Can we model mathematically this economy and see the dynamics of the buyers and sellers? Yes, we can do and we can go farther and make a simple Python script to make simulations in this economy.\nLet’s start our modeling part with defining functions for demand and supply. Logically, demand and supply are generated from some very complicated function and its real anatomy is unknown, but as we are in simple economy we can assume that they are linear functions of price:\nA liner demand curve:\n\\[\nQ =\na - b\\cdot p\n\\]\nA linear supply curve:\n\\[\nQ =\nc + d\\cdot p\n\\]\nHere, \\(p\\) is the price paid by the consumer and \\(Q\\) is the quantity. \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are demand and supply parameters.\nLet first solve this mathematically. We have the equation for quantity and the parameters \\(a\\) and \\(b\\) are predetermined, or we can choose their value. We know that in equilibrium, quantity demanded and quantity supplied are equal. So, by equating our equations and doing some simple algebra we’ll get equilibrium price. Let denote this price by \\(P^{*}\\).\n\\[\na - b\\cdot p = c + d\\cdot p \\Rightarrow\n\\\\\n\\\\ \\Rightarrow d\\cdot p + b\\cdot p = a - c\n\\\\\n\\\\ \\Rightarrow P^{*} =\n\\frac{a - c}{d + b}\n\\]\nBy having \\(P^{*}\\) we have equilibrium price. Now, let me introduce some new notions, which are very natural in this market economy, such as, consumer surplus, producer surplus, and social surplus.\n\nConsumer surplus is the gap between the price that consumers are willing to pay and equilibrium price.\n\n\nProducer surplus is the gap between the price for which producers are willing to sell a product and market equilibrium price.\n\n\nSocial surplus is the sum of consumer and producer surplus.\n\nTo calculate these values, first, we need to calculate inverse demand and supply function. Inverse demand function considers price as a function of quantity. We have quantity as a function of a price, hence we need to find its inverse.\nWe have a linear demand curve, given by\n\\[\nQ =\na - b\\cdot p\n\\]\nLet change the notation a little bit and denote demand function in the following way:\n\\[\nQ(p) =\na - b\\cdot p\n\\]\nTo find the inverse of this function we have to solve the above function for \\(p\\). Doing simple algebra, yields\n\\[\np = \\frac{a - Q}{b}\n\\]\nand this is the same to write\n\\[\np(Q) =\n\\frac{a - Q}{b}\n\\]\nDoing the same for supply, we easily find inverse supply function, given by\n\\[\np(Q) =\n\\frac{Q - c}{d}\n\\]\nConsumer surplus is the area under inverse demand function and producer surplus is the area above inverse supply function. This may not be clear but after plotting demand and supply curves it will be understandable, what is consumer and producer surplus and how to calculate them. For general sense, we need integrals of inverse demand and supply functions to calculate surplus. See the link for more information.\nAs we have all the necessary information and equations, we can start to code these equations in Python. I’m going to write a simple Python class, which will calculate equilibrium price and quantity, consumer and producer surplus.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\nfrom scipy.integrate import quad\n\nplt.style.use(\"ggplot\")\n\n\nclass Market:\n    \"\"\"A class to model a single good market with linear demand and supply functions.\"\"\"\n    def __init__(self, a, b, c, d):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        if a &lt; c:\n            raise ValueError(\"Insufficient demand!\")\n\n    def price(self):\n        \"\"\"Returns equilibrium price\"\"\"\n        return (self.a - self.c) / (self.d + self.b)\n\n    def quantity(self):\n        \"\"\"Returns equilibrium quantity\"\"\"\n        return self.a - self.b * self.price()\n\n    def inverse_demand(self, x):\n        \"\"\"Inverse demand function.\"\"\"\n        return (self.a - x) / self.b\n\n    def inverse_supply(self, x):\n        \"\"\"Inverse supply function.\"\"\"\n        return (x - self.c) / self.d\n\n    def consumer_surplus(self):\n        \"\"\"Calculates consumer surplus.\"\"\"\n        integrand = lambda x: (self.a - x) / self.b\n        area, error = quad(integrand, 0, self.quantity())\n        return area - self.price() * self.quantity()\n\n    def producer_surplus(self):\n        \"\"\"Calculates producer surplus.\"\"\"\n        integrand = lambda x: (x - self.c) / self.d\n        area, error = quad(integrand, 0, self.quantity())\n        return self.price() * self.quantity() - area\n\nLet, test our model with some base line parameters.\n\na = 15\nb = 0.5\nc = -2\nd = 0.5\n\nm = Market(a, b, c, d)\n\n\nprint(\"Equilibrium Price = \", m.price())\nprint(\"Equilibrium Quantity = \", m.quantity())\n\nEquilibrium Price =  17.0\nEquilibrium Quantity =  6.5\n\n\n\nprint('Consumer Surplus =', m.consumer_surplus())\nprint('Producer Surplus =', m.producer_surplus())\n\nConsumer Surplus = 42.25\nProducer Surplus = 42.25\n\n\nLet plot inverse demand and inverse supply curves and shade consumer and producer surplus area.\n\nq_max = m.quantity() * 2\nq_grid = np.linspace(0.0, q_max, 100)\n\npd = m.inverse_demand(q_grid)\nps = m.inverse_supply(q_grid)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(q_grid, pd, lw=2, alpha=0.6, label=\"Demand\", color=\"red\")\nax.plot(q_grid, ps, lw=2, alpha=0.6, label=\"Supply\", color=\"blue\")\n\nax.axhline(17, 0, 0.5, linewidth=2, color=\"black\", linestyle=\"dashed\")\nax.axvline(6.5, 0, 0.5, linewidth=2, color=\"black\", linestyle=\"dashed\")\n\npath1 = Path([[0, 17], [0, 30], [6.5, 17], [0, 17]])\npatch1 = PathPatch(path1, facecolor=\"silver\")\nax.add_patch(patch1)\n\npath2 = Path([[0, 4], [0, 17], [6.5, 17], [0, 4]])\npatch2 = PathPatch(path2, facecolor=\"plum\")\nax.add_patch(patch2)\n\nax.text(6.5, 17, \"Equilibrium Point\", fontsize=12)\nax.text(1, 22, \"Consumer Surplus\", fontsize=12)\nax.text(1, 14, \"Producer Surplus\", fontsize=12)\n\nax.text(0, 17, \"$P^{*}$\", fontsize=14)\nax.text(6.5, 0, \"$Q^{*}$\", fontsize=14)\n\nax.set_xlabel(\"Quantity\", labelpad=20)\nax.set_xlim(0, q_max)\nax.set_ylabel(\"Price\")\nax.legend(loc=\"best\")\n\n\n\n\n\n\n\n\nIn the above graph, we see the equilibrium point. We can ask the following question. What will happen to the equilibrium price and quantity if demand or supply curve shits? Let make a table and summarize the changes in equilibrium due to changes in supply and demand curve.\n\n\n\n\n\n\n\n\nChange\nChange in\\(P^{*}\\)\nChange in\\(Q^{*}\\)\n\n\n\n\nSupply increases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nSupply decreases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\nDemand increases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nDemand decreases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\nDemand increases, Supply increases\n\\(P\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\\(Q\\) \\(\\Uparrow\\)\n\n\nDemand increases, Supply decreases\n\\(P\\) \\(\\Uparrow\\)\n\\(Q\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\n\nDemand decreases, Supply increases\n\\(P\\) \\(\\Downarrow\\)\n\\(Q\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\n\nDemand decreases, Supply decreases\n\\(P\\) \\(\\Updownarrow\\), \\(\\\\(indeterminate)\\)\n\\(Q\\) \\(\\Downarrow\\)\n\n\n\nFrom the above table, we see that in some cases due to the simultaneous changes in supply and demand we cannot determine equilibrium price or quantity movement. That’s the case when other factors are in consideration, but these factors are out of the scope for this blog."
  },
  {
    "objectID": "posts/mathematics/integer_sequences.html",
    "href": "posts/mathematics/integer_sequences.html",
    "title": "Integer Sequences in Python",
    "section": "",
    "text": "Introduction\nIn this article, I want to review some integer sequences from number theory. Particularly, I will review their mathematical formulation, then will implement them in Python and visualize.\nIn short, integer sequence is an ordered list of integers. Generally, in mathematics, the sequence is a collection of objects, where repetition is allowed, and object order does not matter. The number of objects a.k.a elements is the length of the sequence. The sequence can be finite or infinite, increasing or decreasing, convergent or divergent, bounded or unbounded.\nA sequence generally has a rule, which indicates how we can find the value of each element. An integer sequence can be declared either by the formula for its \\(n^{th}\\) element, or by giving the relationship between its elements. Moreover, sequences have the initial condition, which gives us the value of the first few terms of the sequence.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nplt.style.use(\"ggplot\")\n\n\ndef plot_sequence(sequence, x_range, title, x_label, y_label):\n    \"\"\"\n    Plot a sequence of integers.\n\n    Args:\n        sequence: Sequence of integers.\n        title: Title of the plot.\n        x_label: Label of the x-axis.\n        y_label: Label of the y-axis.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=list(range(x_range)), y=sequence)\n    ax.set_title(title)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    plt.plot(sequence, linewidth=4, color=\"black\")\n    plt.show()\n\n\nFibonacci Sequence\nLet first review the well-known Fibonacci Sequence. In the sequence each element is the sum of the two preceding elements starting from \\(0\\) and \\(1\\). The sequence is commonly denoted by \\(F_{n}\\) and the initial condition is given as: \\(F_{0} = 0\\) and \\(F_{1} = 1\\). The general formula is: \\[\nF_{n} = F_{n-1} + F_{n - 2} \\ \\ \\ \\ \\ \\ \\ \\ \\text{for} \\ n &gt; 1\n\\]\n\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\n\nfibonacci_sequence = [fibonacci(n) for n in range(7)]\n\nfibonacci_sequence\n\n[0, 1, 1, 2, 3, 5, 8]\n\n\n\nplot_sequence(fibonacci_sequence, 7, \"Fibonacci Sequence\", \"n\", \"Fibonacci(n)\")\n\n\n\n\n\n\n\n\n\n\nFactorial Sequence\nThe next sequence is Factorial. Factorial of a positive integer is denoted by \\(n!\\) and is the product to all positive integers less or equal to \\(n\\). The formula is: \\[\nn! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\n\\]\nFor integers \\(n \\geq 1\\) we have the following formula: \\(n! = \\prod_{i=1}^{n} i\\), which leads us to the following reccurence relation: \\(n! = n \\cdot (n-1)!\\). The initial condition is that \\(n!=1\\) for \\(n\\) equal to 1 and 0.\n\ndef factorial(n):\n    if n &lt; 2:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n\nfactorial_sequence = [factorial(n) for n in range(6)]\n\nfactorial_sequence\n\n[1, 1, 2, 6, 24, 120]\n\n\n\nplot_sequence(factorial_sequence, 6, \"Factorial Sequence\", \"n\", \"Factorial(n)\")\n\n\n\n\n\n\n\n\n\n\nPadovan Sequence\nPadovan Sequence is the sequence of integers with initial value given by: \\(P(0) = P(1) = P(2) = 1\\). The recurrence relation is defined by \\[\nP(n) = P(n - 2) + P(n - 3)\n\\]\n\ndef padovan(n):\n    if n &lt; 3:\n        return 1\n    else:\n        return padovan(n - 2) + padovan(n - 3)\n\n\npadovan_sequence = [padovan(n) for n in range(6)]\n\npadovan_sequence\n\n[1, 1, 1, 2, 2, 3]\n\n\n\nplot_sequence(padovan_sequence, 6, \"Padovan Sequence\", \"n\", \"Padovan(n)\")\n\n\n\n\n\n\n\n\n\n\nPerrin Number\nThe reccurent formula for Perrin Sequence with initial values \\(P(0) = 3\\), \\(P(1) = 0\\), and \\(P(2) = 2\\) is defined by \\[\nP(n) = P(n - 2) + P(n - 3)\n\\]\n\ndef perrin(n):\n    if n == 0:\n        return 3\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 2\n    else:\n        return perrin(n - 2) + perrin(n - 3)\n\n\nperrin_sequence = [perrin(n) for n in range(9)]\n\nperrin_sequence\n\n[3, 0, 2, 3, 2, 5, 5, 7, 10]\n\n\n\nplot_sequence(perrin_sequence, 9, \"Perrin Sequence\", \"n\", \"Perrin(n)\")\n\n\n\n\n\n\n\n\n\n\nSylvester’s Sequence\nIn the Sylvester’s sequence, each element is the product of previous terms, plus one. Due to this reason, Sylvester’s sequence is considered to have doubly exponential growth, when sequence increases a much higher rate than, say factorial sequence. The initial condition for the sequence is \\(S_{0} = 2\\). This is because the product of an empty set is \\(1\\). The sequence is given by: \\[\nS_{n} = 1 + \\prod_{i=0}^{n-1} s_{i}\n\\]\nRewriting it into recurrence formula, gives: \\[\nS_{i} = S_{i-1}\\cdot(S_{i-1} - 1) + 1\n\\]\n\ndef sylvester(n):\n    if n == 0:\n        return 2\n    else:\n        return sylvester(n - 1) * (sylvester(n - 1) - 1) + 1\n\n\nsylvester_sequence = [sylvester(n) for n in range(5)]\n\nsylvester_sequence\n\n[2, 3, 7, 43, 1807]\n\n\n\nplot_sequence(sylvester_sequence, 5, \"Sylvester Sequence\", \"n\", \"Sylvester(n)\")\n\n\n\n\n\n\n\n\n\n\nFermat Number\nFermat number or Fermat sequence, named after well-known Pierre de Fermat, forms the sequence of the following form: \\[\nF_{n} = 2^{2^{n}} + 1\n\\]\nwith the condition that \\(n \\geq 0\\). The recurrence relation is given by: \\[\nF_{n} = (F_{n-1} - 1)^{2} + 1\n\\]\nNote that, both formulae are valid. However, I use the first to implement it in Python.\n\ndef fermat(n):\n    return 2 ** (2**n) + 1\n\n\nfermat_sequence = [fermat(n) for n in range(6)]\n\nfermat_sequence\n\n[3, 5, 17, 257, 65537, 4294967297]\n\n\n\nplot_sequence(fermat_sequence, 6, \"Fermat Sequence\", \"n\", \"Fermat(n)\")\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nIn this post, I tried to review some integer sequences and implemented them in Python. The analysis of these sequences is a matter of another blog post. However, looking at the plots we can compare these sequences at least with their growth rate. The Fermat sequence has a pretty much higher growth rate than other sequences. Calculating \\(n^{th}\\) term of these sequences can be quite challenging.\n\nReferences\n\nSequences\nSequence\nList of OEIS sequences"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_5.html",
    "href": "posts/mathematics/linear_algebra_5.html",
    "title": "Advance Linear Algebra with Python - Part I",
    "section": "",
    "text": "Introduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn this post I start with introducing advanced parts of the vector operations and then continue with advanced matrix operations.\n\n\n\n\nIn the basics, we saw what is a unit vector. To refresh, the unit vector is the vector with length 1 and the formula is\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|}\n\\]\nFor farther explanation, unit vectors can be used to represent the axes of a Cartesian coordinate system. For example in a three-dimensional Cartesian coordinate system such vectors are:\n\\[\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{k} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nwhich represents, \\(x\\), \\(y\\), and \\(z\\) axes, respectively. For two dimensional space we have\n\\[\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nLet deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. Imagine, we have vector space or collection of vectors \\(\\vec{V}\\) over the Cartesian coordinate system. This space includes all two-dimensional vectors, or in other words, vectors with only two elements, \\(x\\), and \\(y\\).\n\nA basis, call it \\(B\\), of vector space \\(V\\) over the Cartesian coordinate system is a linearly independent subset of \\(V\\) that spans whole vector space \\(V\\). To be precise, basis \\(B\\) to be the basis it must satisfy two conditions:\n\n\nLinearly independence property - states that all vectors in \\(B\\) are linearly independent\nThe spanning property - states that \\(B\\) spans whole \\(V\\)\n\nWe can combine these two conditions in one sentence. \\(B\\) is the basis if its all elements are linearly independent and every element of \\(V\\) is a linear combination of elements of \\(B\\).\nFrom these conditions, we can conclude that unit vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis of \\(\\mathbb{R^2}\\). This kind of bases are also called standard basis or natural basis. The standard basis are denoted by \\(e_{1}\\), \\(e_{2}\\), \\(e_{3}\\) and so on. I will be consistent and use the later notation for standard basis and \\(\\hat{i}\\), \\(\\hat{j}\\) and \\(\\hat{k}\\) for unit vectors.\nThese standard basis vectors are the basis in the sense that any other vector in \\(V\\) can be expressed uniquely as a linear combination of these unit vectors. For example, every vector \\(v\\) in two-dimensional space can be written as\n\\[\nx\\ e_{1} + y\\ e_{2}\n\\]\nwhere \\(e_{1}\\) and \\(e_{2}\\) are unit vectors and \\(x\\) and \\(y\\) are scalar components or elements of the vector \\(v\\).\nNow, to generalize the idea for higher dimensions we just have to apply the same logic as above, for \\(\\mathbb{R^3}\\) and more. In \\(\\mathbb{R^3}\\) we have standard basis vectors \\(e_{1}\\), \\(e_{2}\\), \\(e_{3}\\), and generally for \\(\\mathbb{R^n}\\) we have standard basis vector space\n\\[\nE =\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2} \\\\\n\\cdots \\\\\ne_{n}\n\\end{bmatrix}\n\\]\nTo generalize the definition of the basis further, let consider the following:\nIf elements \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) of \\(V\\) generate \\(V\\) and in addition they are linearly independent, then \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) is called a basis of \\(V\\). We shall say that the elements \\(v_{1}, v_{2},\\cdots,v_{n}\\) constitute or form a basis of V. Vector space \\(V\\) can have several basis.\nAt this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I solved some examples by hand. I’ll show you how to compute basis after explaining row-echelon and reduced row-echelon forms and you’ll understand it. However, it’s not enough only to know how to row-reduce the given matrix. It’s necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. These notions are explained below and after that, we can find the basis for each of them.\n\n\n\n\n\n\nIn linear algebra, Gaussian Elimination is the method to solve the system of linear equations. This method is the sequence of operations performed on the coefficient matrix of the system. Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix.\nAnd what is the sequence of operations?\nUnder this notion, elementary row operations are meant. We’ve covered it in the previous post but for the refresher, ERO’s are:\n\nInterchange rows\nMultiply each element in a row by a non-zero number\nMultiply a row by a non-zero number and add the result to another row\n\nPerforming Gaussian elimination results in the matrix in Row Echelon Form. The matrix is said to be in row echelon form if it satisfies the following conditions:\n\nThe first non-zero element in each row, called the leading entry, is a 1\nEach leading entry is in a column, which is the right side of the leading entry in the previous row\nBelow the leading entry in a column, all other entries are zero\n\nTo catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), we have the system of linear equations in the following form:\n\\[\n\\begin{cases}\nx + 2y - z = 5\\\\\n3x + y - 2z = 9\\\\\n-x + 4y + 2z = 0\n\\end{cases}\n\\]\nBased on these equations we can form the following matrix\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n\\]\nThis matrix is called coefficient matrix as it contains the coefficients of the linear equations. Having the coefficient matrix, we can rewrite our system in the following form:\n\\[\nAx = b\n\\]\nWhere \\(A\\) is the coefficient matrix, \\(x\\) is the vector of the unknowns, and \\(b\\) is the vector of the right-hand side components\nTo solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO’s. This matrix is:\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix} = [A | b]\n\\]\nwhich is called augmented matrix, which in turn gives us the possibility to perform ELO’s, in other words, we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the resulted matrix gives the solution to our system of equations.\nLet do it by hand. We have the initial system\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n-x + 4y + 2z = 0\n\\end{cases}\n\\]\nThen, using ERO’s\n\n\\(R3 \\rightarrow R3 + R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R2 \\rightarrow R2 - 3R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & -5 & 1 & |& -6 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n-5y + z = -6 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R2 \\rightarrow R2 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R3 \\rightarrow R3 - 6R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & -11 & |& 11\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n-11z = 11\n\\end{cases}\n\\]\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\quad (A)\\\\\ny + 2z = -1 \\quad (B)\\\\\nz = -1 \\quad (C)\n\\end{cases}\n\\]\n\nBack substitution\n\n\\[\n\\begin{cases}\n(C) \\quad z = -1 \\\\\n(B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\\n(A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2\n\\end{cases}\n\\]\n\nSolution\n\n\\[\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n\\]\nThis is the solution of the initial system, as well as the last system and every intermediate system. The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions.\nNote that, starting with a particular matrix, a different sequence of ERO’s can lead to different row echelon form\n\n\n\nGaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to reduce it to row echelon form. Once it’s done we perform back substitution to find the solution. However, we can continue performing ERO’s to reduce coefficient matrix farther, to produce Reduced Row Echelon Form. The matrix is in reduced row echelon form if it satisfies the following conditions:\n\nIt is in row echelon form\nThe leading entry in each row is the only non-zero entry in its column\n\nGauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon Form matrix is the result of Gauss-Jordan Elimination process.\nWe can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\nz = -1\n\\end{cases}\n\\]\nNow, from bottom to up we perform the following ERO’s\n\n\\(R2 \\rightarrow R2 - 2R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n\\]\n\n\\(R1 \\rightarrow R1 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & |& 4 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y = 4 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n\\]\n\n\\(R1 \\rightarrow R1 - 2R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & |& 2 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx = 2 \\\\\ny = 1 \\\\\nz = -1\n\\end{cases}\n\\]\nThe solution is\n\\[\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n\\]\nand this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix \\(A\\).\n\n\n\nSuppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n\\]\nTo find the inverse of \\(A\\), we need to augment \\(A\\) by the identity matrix \\(I\\) which has the same dimensions as \\(A\\). It is a must the identity to have the same dimensions. After augmentation we have\n\\[\n[A | I] =\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0 \\\\\n3 & 1 & -2 & |& 0 & 1 & 0 \\\\\n-1 & 4 & 2 & |& 0 & 0 & 1\n\\end{bmatrix}\n\\]\nWe have to perform elementary row operations in the same way as we did in the above example. Particularly,\n\n\\(R3 \\rightarrow R3 + R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n3 & 1 & -2 & |& 0 & 1 & 0\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 - 3R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & -5 & 1 & |& -3 & 1 & -3\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R3 \\rightarrow R3 - 6R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & -11 & |& 13 & -6 & 13\n\\end{bmatrix}\n\\]\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 - 2R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R1 \\rightarrow R1 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & |& -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R1 \\rightarrow R1 - 2R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & |& -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\nOur inverse of \\(A\\) is\n\\[\nA^{-1} =\n\\begin{bmatrix}\n-\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n\\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n-\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\n\nLet \\(A\\) be \\(m\\times n\\) matrix. Space spanned by its column vectors are called range, image, or column space of a matrix \\(A\\). The row space is defined similarly. I only consider column space as all the logic is the same for row space.\nThe precise definition is the following:\nLet \\(A\\) be an \\(m\\times n\\) matrix, with column vectors \\(v_{1}, v_{2}, \\cdots, v_{n}\\). A linear combination of these vectors is any vector of the following form: \\(c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}\\), where \\(c_{1}, c_{2}, \\cdots , c_{n}\\) are scalars. The set of all possible linear combinations of \\(v_{1}, v_{2}, \\cdots , v_{n}\\) is called the column space of \\(A\\).\nFor example:\n\\[\nA =\n\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n2 & 0\n\\end{bmatrix}\n\\]\nColumn vectors are:\n\\[\nv_{1} =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix}\n\\quad\nv_{2} =\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix}\n\\]\nA linear combination of \\(v_{1}\\) and \\(v_{2}\\) is any vector of the form\n\\[\nc_{1}\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix} + c_{2}\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\nc_{1}\\\\\nc_{2}\\\\\n2c_{1}\n\\end{bmatrix}\n\\]\nThe set of all such vectors is the column space of \\(A\\).\n\n\n\nIn linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system:\n\\[A\\cdot X = 0\\]\nwhere \\(A\\) is a \\(m\\times n\\) matrix and \\(X\\) is a \\(m\\times 1\\) vector and is denoted by \\(Ker(A)\\).\nFor more clarity, let consider the numerical example. Lat our matrix \\(A\\) be the following:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nand our \\(X\\) is\n\\[\nX =\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix}\n\\]\nWe have to form the following system:\n\\[\nA\\cdot X =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\nAfter that, we have to put this system into row-echelon or reduced row-echelon form. Let skip detailed calculation and present only results, which is the last matrix.\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3\\\\\n-4 & -2 & 2 & -2\\\\\n-1 & 7 & 3 & 2\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n-4 & -2 & 2 & -2\\\\\n2 & 7 & 1 & 3\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & -30 & -10 & -10\\\\\n0 & 21 & 7 & 7\\\\\n0 & -12 & -4 & -4\n\\end{bmatrix}\n\\] \\[\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nNow, to find the kernel of the original matrix \\(A\\), we have to solve the following system of equations:\n\\[\n\\begin{cases}\nx_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\\n        3x_{2} + x_{3} + x_{4} = 0\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4}\n\\end{cases}\n\\]\nFrom this solution we conclude that the kernel of \\(A\\) is\n\\[\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n\\]\nWhere, \\(x_{3}\\) and \\(x_{4}\\) are free variables and can be any number in \\(R\\)\nNote, that both original matrix \\(A\\) and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space.\n\nimport numpy as np\nfrom scipy.linalg import null_space\nfrom sympy import Matrix\nimport matplotlib.pyplot as plt\n\n\nA = np.array([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\n# This matrix is normalized, meaning that it has unit length\nkernel_A = null_space(A)\nprint(\"Normalized Kernel\", kernel_A, sep=\"\\n\")\n\n# To find unnormalized kernel we have to do the following:\nB = Matrix([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\nkernel_B = B.nullspace()\nprint(\"Unnormalized Kernel\", kernel_B[0], sep=\"\\n\")\n\n\n# In unnormilized case, we clearly see that Sympy automatically choose values for our free variables.\n# In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1\n# Resulted vector(s) are basis for the null space for our matrix A\n\nNormalized Kernel\n[[-0.58983053  0.07100086]\n [ 0.14709576  0.39348879]\n [-0.73692629 -0.32248793]\n [ 0.29563901 -0.85797843]]\nUnnormalized Kernel\nMatrix([[2/3], [-1/3], [1], [0]])\n\n\n\n\n\nIn the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant of sub-matrix of the original matrix shows its non-degenerateness. In other words, nonzero determinant gives us information about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. To keep DRY, here I only consider a numerical example. The code is provided in the intermediate tutorial.\nSuppose we have matrix \\(A\\) in the following form:\n\\[\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 &- 3\n\\end{bmatrix}\n\\]\nPerform Elementary Row Operations we get reduced-echelon form:\n\\[\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 & -3\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n3 & 2 & -1\\\\\n2 & -3 & -5\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & -10 & -10\\\\\n0 & -11 & -11\n\\end{bmatrix}\n\\] \\[\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & -11 & -11\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nFrom the last matrix we see that the nonzero determinant only exists in \\(2\\times2\\) sub-matrices, hence rank of the matrix \\(A\\) is 2.\n\n\n\nNow we are able to find the basis for column space and row space as well as the basis for the kernel. The columns of a matrix \\(A\\) span the column space but they may not form a basis if the column vectors are linearly dependent. If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix \\(A\\) to reduced row-echelon form.\nFor example:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nRow reduced form of \\(A\\) is:\n\\[\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWe see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix \\(A\\) form the basis, which is:\n\\[\n\\begin{bmatrix}\n2 \\\\-4\\\\-1\\\\-2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n7\\\\-2 \\\\ 7 \\\\ 2\n\\end{bmatrix}\n\\]\nTo find the basis for row space, let consider different matrix and again let it be \\(A\\).\n\\[\nA =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n2 & 7 & 4 \\\\\n1 & 5 & 2\n\\end{bmatrix}\n\\]\nTo reduce \\(A\\) to reduced row-echelon form we have:\n\\[\nB =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nAs in column space case, we see that linearly independent, nonzero row vectors are\n\\[\n\\begin{bmatrix}\n1 \\\\0\\\\2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n0\\\\1 \\\\ 0\n\\end{bmatrix}\n\\]\nTo find the basis for kernel let consider our old example. In this case our matrix is:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nAnd its row reduced form is\n\\[\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWe solved this and got the following result:\n\\[\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n\\]\nNow to have basis for null space just plug values for \\(x_{3} = 1\\) and \\(x_{4} = 0\\), resulted vector is\n\\[\n\\begin{bmatrix}\n\\frac{2}{3} \\\\\n-\\frac{1}{3} \\\\\n1 \\\\\n0\n\\end{bmatrix}\n\\]\nThe resulted vector is one set of the basis for kernel space. The values for \\(x_{3}\\) and \\(x_{4}\\) are up to you as they are free variables.\n\n\n\nMatrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. Firstly, define linear transformation:\n\nLinear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations\n\n\n\nYou can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are linear transformations that transform the input vector into the output vector.\nFor example, consider a matrix \\(A\\) and vector \\(v\\)\n\\[\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\]\nDefine transformation \\(T\\) to be:\n\\[\nT(\\vec{v}) = A \\vec{v}\n\\]\nThis transformation is simply dot or inner product and give the following result:\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9\n\\end{bmatrix}\n\\]\nIn this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector. Formally we can write this in the following way:\n\\[\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{2}\n\\]\nThe transformation does not necessarily have to be \\(n \\times n\\). The dimension of the output vector and the input vector may differ. Rewrite our matrix \\(A\\) and vector \\(v\\).\n\\[\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2 \\\\\n1 & 1\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\]\nApply above transformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9 \\\\\n3\n\\end{bmatrix}\n\\]\nNow, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as\n\\[\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{3}\n\\]\n\n\n\nWhen we multiply a vector by a matrix we transform it in at least one of the following two ways\n\nScale the length (Magnitude)\nChange the direction (Amplitude)\n\nChange in length (Magnitude), but not change in direction (Amplitude)\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n0\n\\end{bmatrix}\n\\]\nIn this case, the resulted vector changed in length but not changed in direction.\n\nv = np.array([1, 0])\nA = np.array([[2, 0], [0, 2]])\n\nt = np.dot(A, v)\nprint(\"Resulted vector is: t = \", t)\n\n# Original vector v is green and transformed vector t is blue.\n# Vector t has same direction as v but greater magnitude\nvecs = np.array([t, v])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [2 0]\n\n\n\n\n\n\n\n\n\nChange in direction (Amplitude), but not change in length (Magnitude)\n\\[\nA =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nThis time, resulted vector changed in direction but has the same length.\n\nv = np.array([1, 0])\nA = np.array([[0, -1], [1, 0]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector change the direction but has the same length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [0 1]\n\n\n\n\n\n\n\n\n\nChange in direction (Amplitude) and in length (Magnitude)\n\\[\nA =\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}\n\\]\nThis time the resulted vector changed in the direction as well as the length.\n\nv = np.array([1, 0])\nA = np.array([[2, 1], [1, 2]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector changed the direction, as well as the length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [2 1]\n\n\n\n\n\n\n\n\n\n\n\n\nAn Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as bias.\n\\[\nT(\\vec{v}) = A\\vec{v} + \\vec{b}\n\\]\nConsider following example\n\\[\nT(\\vec{v}) = A\\vec{v} + \\vec{b} =\n\\begin{bmatrix}\n5 & 2\\\\\n3 & 1\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2\\\\\n-6\n\\end{bmatrix} =\n\\begin{bmatrix}\n5\\\\\n-2\n\\end{bmatrix}\n\\]\nThis kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation.\n\nv = np.array([1, 1])\nA = np.array([[5, 2], [3, 1]])\nb = np.array([-2, -6])\n\nt = A @ v + b\nprint(\"Resulted vector is: t = \", t)\n\n# The resulted vector t is blue\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [ 5 -2]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet consider matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\]\nNow, let multiply this matrix with vector \\[\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nWe have the following:\n\\[\nA \\cdot v =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n2 \\cdot v\n\\]\nThat’s the beautiful relationship yes? To prove this is not the only one vector, which can do this try this vector \\(\\vec{v} = [0\\quad 1\\quad 2]\\) instead of old \\(v\\). You should get \\(11\\cdot \\vec{v}\\)\nThis beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case \\(2\\) and \\(11\\) are eigenvalues of the matrix \\(A\\).\nLet formalize the notion of eigenvalue and eigenvector:\n\nLet \\(A\\) be an \\(n\\times n\\) square matrix. If \\(\\lambda\\) is a scalar and \\(v\\) is non-zero vector in \\(\\mathbb{R^n}\\) such that \\(Av = \\lambda v\\) then we say that \\(\\lambda\\) is an eigenvalue and \\(v\\) is eigenvector\n\nI believe you are interested in how to find eigenvalues. Consider again our matrix \\(A\\) and follow steps to find eigenvalues. Given that our matrix \\(A\\) is a square matrix, the condition that characterizes an eigenvalue \\(\\lambda\\) is the existence of a nonzero vector \\(v\\) such that \\(Av = \\lambda v\\). We can rewrite this equation in the following way:\n\\[\nAv = \\lambda v\n\\\\\nAv - \\lambda v = 0\n\\\\\nAv - \\lambda I v = 0\n\\\\\n(A - \\lambda I)v = 0\n\\]\nThe final form of this equation makes it clear that \\(v\\) is the solution of a square, homogeneous system. To have the nonzero solution(we required it in above definition), then the determinant of the coefficient matrix - \\((A - \\lambda I)\\) must be zero. This is achieved when the columns of the coefficient matrix are linearly dependent. In other words, to find eigenvalues we have to choose \\(\\lambda\\) such that to solve the following equation:\n\\[\ndet(A - \\lambda I) = 0\n\\]\nThis equation is called characteristic equation\nFor more clarity, let solve it with a particular example. We have square matrix \\(A\\) and follow the above equation gives us:\n\\[\ndet(A - \\lambda I) = det\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\lambda \\cdot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\Bigg) =\ndet\\Bigg(\n\\begin{bmatrix}\n2 - \\lambda & 0 & 0 \\\\\n0 & 3 - \\lambda & 4 \\\\\n0 & 4 & 9 - \\lambda\n\\end{bmatrix}\n\\Bigg)\n\\Rightarrow\n\\] \\[\n\\Rightarrow\n(2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] =\n-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\n\\]\nThe equation \\(-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\\) is called characteristic polynomial of the matrix \\(A\\) and will be of degree \\(n\\) if \\(A\\) is \\(n\\times n\\)\nThe zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix \\(A\\). In this case the roots are \\(2\\), \\(1\\), and \\(11\\). Surprise! Our matrix \\(A\\) have three eigenvalues and two of them are already known for us from above example.\nEigenvalues of a square matrix \\(A\\) have some nice features:\n\nThe determinant of \\(A\\) equals to the product of the eigenvalues\nThe trace of \\(A\\) (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues\nIf \\(A\\) is symmetric matrix, then all of its eigenvalues are real\nIf \\(A\\) is invertible (The determinant of \\(A\\) is not zero) and \\(\\lambda_{1}, \\cdots, \\lambda_{n}\\) are its eigenvalues, then the eigenvalues of \\(A^{-1}\\) are \\(1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}\\)\n\nFrom first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero.\n\n\n\nIt’s time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue.\n\nAny nonzero vector \\(v\\) which satisfies characteristic equation is said to be an eigenvector of \\(A\\) corresponding to \\(\\lambda\\)\n\nContinue above example and see what are eigenvectors corresponding to eigenvalues \\(\\lambda = 2\\), \\(\\lambda = 1\\), and \\(\\lambda = 11\\), respectively.\nEigenvector for \\(\\lambda = 1\\)\n\\[\n(A - 1I)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\Bigg)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 4 \\\\\n0 & 4 & 8\n\\end{bmatrix}\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nRewrite this as a system of equations, we’ll get\n\\[\n\\begin{cases}\nv_{1} = 0\\\\\n2v_{2} + 4v_{3} = 0\\\\\n4v_{2} + 8v{3} = 0\n\\end{cases}\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2v_{3}\\\\\nv_{3} = 1\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2\\\\\nv_{3} = 1\n\\end{cases}\n\\]\nSo, our eigenvector corresponding to eigenvalue \\(\\lambda = 1\\) is\n\\[\nv_{\\lambda = 1} =\n\\begin{bmatrix}\n0 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\nFinding eigenvectors for \\(\\lambda = 2\\) and \\(\\lambda = 11\\) is up to you.\n\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\n\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Note that this eigenvectors seems different from my calculation. However they are not different.\n# They are normalized to have unit length\nprint(\"Eigenvalues are: \", eigenvalues)\nprint(\"Eigenvectors are: \", eigenvectors, sep=\"\\n\")\n\nEigenvalues are:  [11.  1.  2.]\nEigenvectors are: \n[[ 0.          0.          1.        ]\n [ 0.4472136   0.89442719  0.        ]\n [ 0.89442719 -0.4472136   0.        ]]\n\n\n\n\n\nThe Spectral Radius of a square matrix \\(A\\) is the largest absolute values of its eigenvalues and is denoted by \\(\\rho(A)\\). More formally,\n\nSpectral radius of a \\(n \\times n\\) matrix \\(A\\) is:\n\n\\[\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda\n\\mid \\ :\n\\lambda \\ is \\ an \\ eigenvalue \\ of \\ A\n\\Big\\}\n\\]\nStated otherwise, we have\n\\[\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda_{1}\n\\mid,\n\\cdots,\n\\mid \\lambda_{n}\n\\mid\n\\Big\\}\n\\]\nIt’s noteworthy that the set of all eigenvalues\n\\[\n\\Big\\{ \\lambda : \\lambda \\in \\lambda(A)\n\\Big\\}\n\\]\nis called the Spectrum\nFrom above example we had three eigenvalues, \\(\\lambda = 2\\), \\(\\lambda = 1\\) and \\(\\lambda = 11\\) which are spectrum of \\(A\\) and spectral radius for our matrix \\(A\\) is \\(\\lambda = 11\\)"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_5.html#vector",
    "href": "posts/mathematics/linear_algebra_5.html#vector",
    "title": "Advance Linear Algebra with Python - Part I",
    "section": "",
    "text": "In the basics, we saw what is a unit vector. To refresh, the unit vector is the vector with length 1 and the formula is\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|}\n\\]\nFor farther explanation, unit vectors can be used to represent the axes of a Cartesian coordinate system. For example in a three-dimensional Cartesian coordinate system such vectors are:\n\\[\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{k} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nwhich represents, \\(x\\), \\(y\\), and \\(z\\) axes, respectively. For two dimensional space we have\n\\[\n\\hat{i} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\quad\n\\hat{j} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nLet deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. Imagine, we have vector space or collection of vectors \\(\\vec{V}\\) over the Cartesian coordinate system. This space includes all two-dimensional vectors, or in other words, vectors with only two elements, \\(x\\), and \\(y\\).\n\nA basis, call it \\(B\\), of vector space \\(V\\) over the Cartesian coordinate system is a linearly independent subset of \\(V\\) that spans whole vector space \\(V\\). To be precise, basis \\(B\\) to be the basis it must satisfy two conditions:\n\n\nLinearly independence property - states that all vectors in \\(B\\) are linearly independent\nThe spanning property - states that \\(B\\) spans whole \\(V\\)\n\nWe can combine these two conditions in one sentence. \\(B\\) is the basis if its all elements are linearly independent and every element of \\(V\\) is a linear combination of elements of \\(B\\).\nFrom these conditions, we can conclude that unit vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis of \\(\\mathbb{R^2}\\). This kind of bases are also called standard basis or natural basis. The standard basis are denoted by \\(e_{1}\\), \\(e_{2}\\), \\(e_{3}\\) and so on. I will be consistent and use the later notation for standard basis and \\(\\hat{i}\\), \\(\\hat{j}\\) and \\(\\hat{k}\\) for unit vectors.\nThese standard basis vectors are the basis in the sense that any other vector in \\(V\\) can be expressed uniquely as a linear combination of these unit vectors. For example, every vector \\(v\\) in two-dimensional space can be written as\n\\[\nx\\ e_{1} + y\\ e_{2}\n\\]\nwhere \\(e_{1}\\) and \\(e_{2}\\) are unit vectors and \\(x\\) and \\(y\\) are scalar components or elements of the vector \\(v\\).\nNow, to generalize the idea for higher dimensions we just have to apply the same logic as above, for \\(\\mathbb{R^3}\\) and more. In \\(\\mathbb{R^3}\\) we have standard basis vectors \\(e_{1}\\), \\(e_{2}\\), \\(e_{3}\\), and generally for \\(\\mathbb{R^n}\\) we have standard basis vector space\n\\[\nE =\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2} \\\\\n\\cdots \\\\\ne_{n}\n\\end{bmatrix}\n\\]\nTo generalize the definition of the basis further, let consider the following:\nIf elements \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) of \\(V\\) generate \\(V\\) and in addition they are linearly independent, then \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) is called a basis of \\(V\\). We shall say that the elements \\(v_{1}, v_{2},\\cdots,v_{n}\\) constitute or form a basis of V. Vector space \\(V\\) can have several basis.\nAt this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I solved some examples by hand. I’ll show you how to compute basis after explaining row-echelon and reduced row-echelon forms and you’ll understand it. However, it’s not enough only to know how to row-reduce the given matrix. It’s necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. These notions are explained below and after that, we can find the basis for each of them."
  },
  {
    "objectID": "posts/mathematics/linear_algebra_5.html#matrix",
    "href": "posts/mathematics/linear_algebra_5.html#matrix",
    "title": "Advance Linear Algebra with Python - Part I",
    "section": "",
    "text": "In linear algebra, Gaussian Elimination is the method to solve the system of linear equations. This method is the sequence of operations performed on the coefficient matrix of the system. Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix.\nAnd what is the sequence of operations?\nUnder this notion, elementary row operations are meant. We’ve covered it in the previous post but for the refresher, ERO’s are:\n\nInterchange rows\nMultiply each element in a row by a non-zero number\nMultiply a row by a non-zero number and add the result to another row\n\nPerforming Gaussian elimination results in the matrix in Row Echelon Form. The matrix is said to be in row echelon form if it satisfies the following conditions:\n\nThe first non-zero element in each row, called the leading entry, is a 1\nEach leading entry is in a column, which is the right side of the leading entry in the previous row\nBelow the leading entry in a column, all other entries are zero\n\nTo catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), we have the system of linear equations in the following form:\n\\[\n\\begin{cases}\nx + 2y - z = 5\\\\\n3x + y - 2z = 9\\\\\n-x + 4y + 2z = 0\n\\end{cases}\n\\]\nBased on these equations we can form the following matrix\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n\\]\nThis matrix is called coefficient matrix as it contains the coefficients of the linear equations. Having the coefficient matrix, we can rewrite our system in the following form:\n\\[\nAx = b\n\\]\nWhere \\(A\\) is the coefficient matrix, \\(x\\) is the vector of the unknowns, and \\(b\\) is the vector of the right-hand side components\nTo solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO’s. This matrix is:\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix} = [A | b]\n\\]\nwhich is called augmented matrix, which in turn gives us the possibility to perform ELO’s, in other words, we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the resulted matrix gives the solution to our system of equations.\nLet do it by hand. We have the initial system\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n-1 & 4 & 2 & |& 0\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n-x + 4y + 2z = 0\n\\end{cases}\n\\]\nThen, using ERO’s\n\n\\(R3 \\rightarrow R3 + R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n3 & 1 & -2 & |& 9 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n3x + y - 2z = 9 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R2 \\rightarrow R2 - 3R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & -5 & 1 & |& -6 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\n-5y + z = -6 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R2 \\rightarrow R2 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 6 & 1 & |& 5\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n6y + z = 5\n\\end{cases}\n\\]\n\n\\(R3 \\rightarrow R3 - 6R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & -11 & |& 11\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\n-11z = 11\n\\end{cases}\n\\]\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\quad (A)\\\\\ny + 2z = -1 \\quad (B)\\\\\nz = -1 \\quad (C)\n\\end{cases}\n\\]\n\nBack substitution\n\n\\[\n\\begin{cases}\n(C) \\quad z = -1 \\\\\n(B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\\n(A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2\n\\end{cases}\n\\]\n\nSolution\n\n\\[\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n\\]\nThis is the solution of the initial system, as well as the last system and every intermediate system. The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions.\nNote that, starting with a particular matrix, a different sequence of ERO’s can lead to different row echelon form\n\n\n\nGaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to reduce it to row echelon form. Once it’s done we perform back substitution to find the solution. However, we can continue performing ERO’s to reduce coefficient matrix farther, to produce Reduced Row Echelon Form. The matrix is in reduced row echelon form if it satisfies the following conditions:\n\nIt is in row echelon form\nThe leading entry in each row is the only non-zero entry in its column\n\nGauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon Form matrix is the result of Gauss-Jordan Elimination process.\nWe can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 2 & |& -1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny + 2z = -1 \\\\\nz = -1\n\\end{cases}\n\\]\nNow, from bottom to up we perform the following ERO’s\n\n\\(R2 \\rightarrow R2 - 2R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 5 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y - z = 5 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n\\]\n\n\\(R1 \\rightarrow R1 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & |& 4 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx + 2y = 4 \\\\\ny  = 1 \\\\\nz = -1\n\\end{cases}\n\\]\n\n\\(R1 \\rightarrow R1 - 2R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & |& 2 \\\\\n0 & 1 & 0 & |& 1 \\\\\n0 & 0 & 1 & |& -1\n\\end{bmatrix}\n\\equiv\n\\begin{cases}\nx = 2 \\\\\ny = 1 \\\\\nz = -1\n\\end{cases}\n\\]\nThe solution is\n\\[\nx = 2 \\\\\ny = 1 \\\\\nx = -1\n\\]\nand this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix \\(A\\).\n\n\n\nSuppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & -1 \\\\\n3 & 1 & -2 \\\\\n-1 & 4 & 2\n\\end{bmatrix}\n\\]\nTo find the inverse of \\(A\\), we need to augment \\(A\\) by the identity matrix \\(I\\) which has the same dimensions as \\(A\\). It is a must the identity to have the same dimensions. After augmentation we have\n\\[\n[A | I] =\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0 \\\\\n3 & 1 & -2 & |& 0 & 1 & 0 \\\\\n-1 & 4 & 2 & |& 0 & 0 & 1\n\\end{bmatrix}\n\\]\nWe have to perform elementary row operations in the same way as we did in the above example. Particularly,\n\n\\(R3 \\rightarrow R3 + R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n3 & 1 & -2 & |& 0 & 1 & 0\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 - 3R1\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & -5 & 1 & |& -3 & 1 & -3\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 6 & 1 & |& 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\\(R3 \\rightarrow R3 - 6R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & -11 & |& 13 & -6 & 13\n\\end{bmatrix}\n\\]\n\n\\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 2 & |& -2 & 1 & -2\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R2 \\rightarrow R2 - 2R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & |& 1 & 0 & 0\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R1 \\rightarrow R1 + R3\\)\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & |& -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\\(R1 \\rightarrow R1 - 2R2\\)\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & |& -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\nOur inverse of \\(A\\) is\n\\[\nA^{-1} =\n\\begin{bmatrix}\n-\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n\\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n-\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n\\end{bmatrix}\n\\]\n\n\n\nLet \\(A\\) be \\(m\\times n\\) matrix. Space spanned by its column vectors are called range, image, or column space of a matrix \\(A\\). The row space is defined similarly. I only consider column space as all the logic is the same for row space.\nThe precise definition is the following:\nLet \\(A\\) be an \\(m\\times n\\) matrix, with column vectors \\(v_{1}, v_{2}, \\cdots, v_{n}\\). A linear combination of these vectors is any vector of the following form: \\(c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}\\), where \\(c_{1}, c_{2}, \\cdots , c_{n}\\) are scalars. The set of all possible linear combinations of \\(v_{1}, v_{2}, \\cdots , v_{n}\\) is called the column space of \\(A\\).\nFor example:\n\\[\nA =\n\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n2 & 0\n\\end{bmatrix}\n\\]\nColumn vectors are:\n\\[\nv_{1} =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix}\n\\quad\nv_{2} =\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix}\n\\]\nA linear combination of \\(v_{1}\\) and \\(v_{2}\\) is any vector of the form\n\\[\nc_{1}\n\\begin{bmatrix}\n1\\\\\n0\\\\\n2\n\\end{bmatrix} + c_{2}\n\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\nc_{1}\\\\\nc_{2}\\\\\n2c_{1}\n\\end{bmatrix}\n\\]\nThe set of all such vectors is the column space of \\(A\\).\n\n\n\nIn linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system:\n\\[A\\cdot X = 0\\]\nwhere \\(A\\) is a \\(m\\times n\\) matrix and \\(X\\) is a \\(m\\times 1\\) vector and is denoted by \\(Ker(A)\\).\nFor more clarity, let consider the numerical example. Lat our matrix \\(A\\) be the following:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nand our \\(X\\) is\n\\[\nX =\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix}\n\\]\nWe have to form the following system:\n\\[\nA\\cdot X =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\nAfter that, we have to put this system into row-echelon or reduced row-echelon form. Let skip detailed calculation and present only results, which is the last matrix.\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3\\\\\n-4 & -2 & 2 & -2\\\\\n-1 & 7 & 3 & 2\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n-4 & -2 & 2 & -2\\\\\n2 & 7 & 1 & 3\\\\\n-2 & 2 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & -30 & -10 & -10\\\\\n0 & 21 & 7 & 7\\\\\n0 & -12 & -4 & -4\n\\end{bmatrix}\n\\] \\[\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\\\\\n0 & 3 & 1 & 1\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nNow, to find the kernel of the original matrix \\(A\\), we have to solve the following system of equations:\n\\[\n\\begin{cases}\nx_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\\n        3x_{2} + x_{3} + x_{4} = 0\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4}\n\\end{cases}\n\\]\nFrom this solution we conclude that the kernel of \\(A\\) is\n\\[\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n\\]\nWhere, \\(x_{3}\\) and \\(x_{4}\\) are free variables and can be any number in \\(R\\)\nNote, that both original matrix \\(A\\) and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space.\n\nimport numpy as np\nfrom scipy.linalg import null_space\nfrom sympy import Matrix\nimport matplotlib.pyplot as plt\n\n\nA = np.array([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\n# This matrix is normalized, meaning that it has unit length\nkernel_A = null_space(A)\nprint(\"Normalized Kernel\", kernel_A, sep=\"\\n\")\n\n# To find unnormalized kernel we have to do the following:\nB = Matrix([[2, 7, 1, 3], [-4, -2, 2, -2], [-1, 7, 3, 2], [-2, 2, 2, 0]])\n\nkernel_B = B.nullspace()\nprint(\"Unnormalized Kernel\", kernel_B[0], sep=\"\\n\")\n\n\n# In unnormilized case, we clearly see that Sympy automatically choose values for our free variables.\n# In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1\n# Resulted vector(s) are basis for the null space for our matrix A\n\nNormalized Kernel\n[[-0.58983053  0.07100086]\n [ 0.14709576  0.39348879]\n [-0.73692629 -0.32248793]\n [ 0.29563901 -0.85797843]]\nUnnormalized Kernel\nMatrix([[2/3], [-1/3], [1], [0]])\n\n\n\n\n\nIn the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant of sub-matrix of the original matrix shows its non-degenerateness. In other words, nonzero determinant gives us information about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. To keep DRY, here I only consider a numerical example. The code is provided in the intermediate tutorial.\nSuppose we have matrix \\(A\\) in the following form:\n\\[\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 &- 3\n\\end{bmatrix}\n\\]\nPerform Elementary Row Operations we get reduced-echelon form:\n\\[\nA =\n\\begin{bmatrix}\n3 & 2 & -1\\\\\n2 & -3 & -5\\\\\n-1 & -4 & -3\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n3 & 2 & -1\\\\\n2 & -3 & -5\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & -10 & -10\\\\\n0 & -11 & -11\n\\end{bmatrix}\n\\] \\[\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & -11 & -11\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & 4 & 3\\\\\n0 & 1 & 1\\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nFrom the last matrix we see that the nonzero determinant only exists in \\(2\\times2\\) sub-matrices, hence rank of the matrix \\(A\\) is 2.\n\n\n\nNow we are able to find the basis for column space and row space as well as the basis for the kernel. The columns of a matrix \\(A\\) span the column space but they may not form a basis if the column vectors are linearly dependent. If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix \\(A\\) to reduced row-echelon form.\nFor example:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nRow reduced form of \\(A\\) is:\n\\[\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWe see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix \\(A\\) form the basis, which is:\n\\[\n\\begin{bmatrix}\n2 \\\\-4\\\\-1\\\\-2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n7\\\\-2 \\\\ 7 \\\\ 2\n\\end{bmatrix}\n\\]\nTo find the basis for row space, let consider different matrix and again let it be \\(A\\).\n\\[\nA =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n2 & 7 & 4 \\\\\n1 & 5 & 2\n\\end{bmatrix}\n\\]\nTo reduce \\(A\\) to reduced row-echelon form we have:\n\\[\nB =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nAs in column space case, we see that linearly independent, nonzero row vectors are\n\\[\n\\begin{bmatrix}\n1 \\\\0\\\\2\n\\end{bmatrix}\n\\quad\n\\text{and}\n\\quad\n\\begin{bmatrix}\n0\\\\1 \\\\ 0\n\\end{bmatrix}\n\\]\nTo find the basis for kernel let consider our old example. In this case our matrix is:\n\\[\nA =\n\\begin{bmatrix}\n2 & 7 & 1 & 3 \\\\\n-4 & -2 & 2 & -2 \\\\\n-1 & 7 & 3 & 2 \\\\\n-2 & 2 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nAnd its row reduced form is\n\\[\nB =\n\\begin{bmatrix}\n-1 & 7 & 3 & 2\\\\\n0 & 3 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nWe solved this and got the following result:\n\\[\nKer(A) =\n\\begin{bmatrix}\nx_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{bmatrix}\n\\]\nNow to have basis for null space just plug values for \\(x_{3} = 1\\) and \\(x_{4} = 0\\), resulted vector is\n\\[\n\\begin{bmatrix}\n\\frac{2}{3} \\\\\n-\\frac{1}{3} \\\\\n1 \\\\\n0\n\\end{bmatrix}\n\\]\nThe resulted vector is one set of the basis for kernel space. The values for \\(x_{3}\\) and \\(x_{4}\\) are up to you as they are free variables.\n\n\n\nMatrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. Firstly, define linear transformation:\n\nLinear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations\n\n\n\nYou can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are linear transformations that transform the input vector into the output vector.\nFor example, consider a matrix \\(A\\) and vector \\(v\\)\n\\[\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\]\nDefine transformation \\(T\\) to be:\n\\[\nT(\\vec{v}) = A \\vec{v}\n\\]\nThis transformation is simply dot or inner product and give the following result:\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9\n\\end{bmatrix}\n\\]\nIn this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector. Formally we can write this in the following way:\n\\[\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{2}\n\\]\nThe transformation does not necessarily have to be \\(n \\times n\\). The dimension of the output vector and the input vector may differ. Rewrite our matrix \\(A\\) and vector \\(v\\).\n\\[\nA =\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 2 \\\\\n1 & 1\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n\\]\nApply above transformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n8 \\\\\n9 \\\\\n3\n\\end{bmatrix}\n\\]\nNow, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as\n\\[\nT: \\rm I\\!R^{2} \\to \\rm I\\!R^{3}\n\\]\n\n\n\nWhen we multiply a vector by a matrix we transform it in at least one of the following two ways\n\nScale the length (Magnitude)\nChange the direction (Amplitude)\n\nChange in length (Magnitude), but not change in direction (Amplitude)\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n0\n\\end{bmatrix}\n\\]\nIn this case, the resulted vector changed in length but not changed in direction.\n\nv = np.array([1, 0])\nA = np.array([[2, 0], [0, 2]])\n\nt = np.dot(A, v)\nprint(\"Resulted vector is: t = \", t)\n\n# Original vector v is green and transformed vector t is blue.\n# Vector t has same direction as v but greater magnitude\nvecs = np.array([t, v])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [2 0]\n\n\n\n\n\n\n\n\n\nChange in direction (Amplitude), but not change in length (Magnitude)\n\\[\nA =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\nThis time, resulted vector changed in direction but has the same length.\n\nv = np.array([1, 0])\nA = np.array([[0, -1], [1, 0]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector change the direction but has the same length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [0 1]\n\n\n\n\n\n\n\n\n\nChange in direction (Amplitude) and in length (Magnitude)\n\\[\nA =\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\\quad\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\ntransformation gives,\n\\[\nT(\\vec{v}) = A \\vec{v} =\n\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}\n\\]\nThis time the resulted vector changed in the direction as well as the length.\n\nv = np.array([1, 0])\nA = np.array([[2, 1], [1, 2]])\n\nt = A @ v\nprint(\"Resulted vector is: t = \", t)\n\n# Resulted vector changed the direction, as well as the length\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [2 1]\n\n\n\n\n\n\n\n\n\n\n\n\nAn Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as bias.\n\\[\nT(\\vec{v}) = A\\vec{v} + \\vec{b}\n\\]\nConsider following example\n\\[\nT(\\vec{v}) = A\\vec{v} + \\vec{b} =\n\\begin{bmatrix}\n5 & 2\\\\\n3 & 1\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2\\\\\n-6\n\\end{bmatrix} =\n\\begin{bmatrix}\n5\\\\\n-2\n\\end{bmatrix}\n\\]\nThis kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation.\n\nv = np.array([1, 1])\nA = np.array([[5, 2], [3, 1]])\nb = np.array([-2, -6])\n\nt = A @ v + b\nprint(\"Resulted vector is: t = \", t)\n\n# The resulted vector t is blue\nvecs = np.array([v, t])\nplt.axis(\"equal\")\nplt.grid()\nplt.quiver(0, 0, *t, color=[\"blue\"], scale=10)\nplt.quiver(0, 0, *v, color=[\"green\"], scale=10)\nplt.show()\n\nResulted vector is: t =  [ 5 -2]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet consider matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\]\nNow, let multiply this matrix with vector \\[\n\\vec{v} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nWe have the following:\n\\[\nA \\cdot v =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n0 \\\\\n0\n\\end{bmatrix} =\n2 \\cdot v\n\\]\nThat’s the beautiful relationship yes? To prove this is not the only one vector, which can do this try this vector \\(\\vec{v} = [0\\quad 1\\quad 2]\\) instead of old \\(v\\). You should get \\(11\\cdot \\vec{v}\\)\nThis beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case \\(2\\) and \\(11\\) are eigenvalues of the matrix \\(A\\).\nLet formalize the notion of eigenvalue and eigenvector:\n\nLet \\(A\\) be an \\(n\\times n\\) square matrix. If \\(\\lambda\\) is a scalar and \\(v\\) is non-zero vector in \\(\\mathbb{R^n}\\) such that \\(Av = \\lambda v\\) then we say that \\(\\lambda\\) is an eigenvalue and \\(v\\) is eigenvector\n\nI believe you are interested in how to find eigenvalues. Consider again our matrix \\(A\\) and follow steps to find eigenvalues. Given that our matrix \\(A\\) is a square matrix, the condition that characterizes an eigenvalue \\(\\lambda\\) is the existence of a nonzero vector \\(v\\) such that \\(Av = \\lambda v\\). We can rewrite this equation in the following way:\n\\[\nAv = \\lambda v\n\\\\\nAv - \\lambda v = 0\n\\\\\nAv - \\lambda I v = 0\n\\\\\n(A - \\lambda I)v = 0\n\\]\nThe final form of this equation makes it clear that \\(v\\) is the solution of a square, homogeneous system. To have the nonzero solution(we required it in above definition), then the determinant of the coefficient matrix - \\((A - \\lambda I)\\) must be zero. This is achieved when the columns of the coefficient matrix are linearly dependent. In other words, to find eigenvalues we have to choose \\(\\lambda\\) such that to solve the following equation:\n\\[\ndet(A - \\lambda I) = 0\n\\]\nThis equation is called characteristic equation\nFor more clarity, let solve it with a particular example. We have square matrix \\(A\\) and follow the above equation gives us:\n\\[\ndet(A - \\lambda I) = det\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\lambda \\cdot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\Bigg) =\ndet\\Bigg(\n\\begin{bmatrix}\n2 - \\lambda & 0 & 0 \\\\\n0 & 3 - \\lambda & 4 \\\\\n0 & 4 & 9 - \\lambda\n\\end{bmatrix}\n\\Bigg)\n\\Rightarrow\n\\] \\[\n\\Rightarrow\n(2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] =\n-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\n\\]\nThe equation \\(-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\\) is called characteristic polynomial of the matrix \\(A\\) and will be of degree \\(n\\) if \\(A\\) is \\(n\\times n\\)\nThe zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix \\(A\\). In this case the roots are \\(2\\), \\(1\\), and \\(11\\). Surprise! Our matrix \\(A\\) have three eigenvalues and two of them are already known for us from above example.\nEigenvalues of a square matrix \\(A\\) have some nice features:\n\nThe determinant of \\(A\\) equals to the product of the eigenvalues\nThe trace of \\(A\\) (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues\nIf \\(A\\) is symmetric matrix, then all of its eigenvalues are real\nIf \\(A\\) is invertible (The determinant of \\(A\\) is not zero) and \\(\\lambda_{1}, \\cdots, \\lambda_{n}\\) are its eigenvalues, then the eigenvalues of \\(A^{-1}\\) are \\(1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}\\)\n\nFrom first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero.\n\n\n\nIt’s time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue.\n\nAny nonzero vector \\(v\\) which satisfies characteristic equation is said to be an eigenvector of \\(A\\) corresponding to \\(\\lambda\\)\n\nContinue above example and see what are eigenvectors corresponding to eigenvalues \\(\\lambda = 2\\), \\(\\lambda = 1\\), and \\(\\lambda = 11\\), respectively.\nEigenvector for \\(\\lambda = 1\\)\n\\[\n(A - 1I)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\\Bigg(\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix} -\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\Bigg)\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 4 \\\\\n0 & 4 & 8\n\\end{bmatrix}\\cdot\n\\begin{bmatrix}\nv_{1} \\\\\nv_{2} \\\\\nv_{3}\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\]\nRewrite this as a system of equations, we’ll get\n\\[\n\\begin{cases}\nv_{1} = 0\\\\\n2v_{2} + 4v_{3} = 0\\\\\n4v_{2} + 8v{3} = 0\n\\end{cases}\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2v_{3}\\\\\nv_{3} = 1\n\\end{cases}\n\\rightarrow\n\\begin{cases}\nv_{1} = 0 \\\\\nv_{2} = -2\\\\\nv_{3} = 1\n\\end{cases}\n\\]\nSo, our eigenvector corresponding to eigenvalue \\(\\lambda = 1\\) is\n\\[\nv_{\\lambda = 1} =\n\\begin{bmatrix}\n0 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\nFinding eigenvectors for \\(\\lambda = 2\\) and \\(\\lambda = 11\\) is up to you.\n\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\n\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Note that this eigenvectors seems different from my calculation. However they are not different.\n# They are normalized to have unit length\nprint(\"Eigenvalues are: \", eigenvalues)\nprint(\"Eigenvectors are: \", eigenvectors, sep=\"\\n\")\n\nEigenvalues are:  [11.  1.  2.]\nEigenvectors are: \n[[ 0.          0.          1.        ]\n [ 0.4472136   0.89442719  0.        ]\n [ 0.89442719 -0.4472136   0.        ]]\n\n\n\n\n\nThe Spectral Radius of a square matrix \\(A\\) is the largest absolute values of its eigenvalues and is denoted by \\(\\rho(A)\\). More formally,\n\nSpectral radius of a \\(n \\times n\\) matrix \\(A\\) is:\n\n\\[\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda\n\\mid \\ :\n\\lambda \\ is \\ an \\ eigenvalue \\ of \\ A\n\\Big\\}\n\\]\nStated otherwise, we have\n\\[\n\\rho(A) = max\n\\Big\\{\n\\mid \\lambda_{1}\n\\mid,\n\\cdots,\n\\mid \\lambda_{n}\n\\mid\n\\Big\\}\n\\]\nIt’s noteworthy that the set of all eigenvalues\n\\[\n\\Big\\{ \\lambda : \\lambda \\in \\lambda(A)\n\\Big\\}\n\\]\nis called the Spectrum\nFrom above example we had three eigenvalues, \\(\\lambda = 2\\), \\(\\lambda = 1\\) and \\(\\lambda = 11\\) which are spectrum of \\(A\\) and spectral radius for our matrix \\(A\\) is \\(\\lambda = 11\\)"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_1.html",
    "href": "posts/mathematics/linear_algebra_1.html",
    "title": "Introduction to Linear Algebra with Python",
    "section": "",
    "text": "Introduction\nThe series will follow the sequence:\n\nIntroduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn these series I will attempt to demystify linear algebra concepts for beginners and combine it with Python for practical use.\nLinear algebra is one of the building block of data science among others. Its importance is huge, as all supervised, unsupervised and semi-supervised algorithms use it with some degree. One great example is Google’s famous Page Rank algorithm, which heavily relies on it. And that is not all. We can find as many usage cases of linear algebra as many individuals exist in the data science field.\nThe purpose of this blog series is to introduce you the ways how to use linear algebra in data science. More precisely, by this series, I intend to help aspiring data scientist to refresh their linear algebra knowledge with Python and gain some hands-on experience. Moreover, this may serve you as a starting point to dig deeper into an amazing world of linear algebra with Python.\nLet start by defining what is linear algebra\n\n\n    Linear algebra is a branch of mathematics that is concerned with mathematical structures, closed under addition and scalar multiplication operations and that includes the theory of systems of linear equations, matrices, determinants, vector spaces, and linear transformations.\n\n\nLoosely speaking, mathematical structure is a set, together with a family of operations and relations defined on that set. Now divide “Closed under addition and scalar multiplication” into two parts. First is “closed under addition” which means that a set is “closed under addition” if the sum of any two members of this set belongs to this set again. For example, imagine the set of even integers. Then, take any to integer and add them up. The result is an even integer belonging to the initial set. Here is a mathematical definition.\n\\[\nA=\\{x \\in \\mathbb{Z}~\\vert~mod~2 =0\\}\n\\]\nSecond is “closed under scalar multiplication”. This means that the product of any member of the set and any scalar \\(\\alpha\\) such that \\(\\alpha~\\in \\mathbb{R}\\) is also in the set. The above-mentioned set is also closed under scalar multiplication. Generally, the sets \\(\\mathbb{N}, ~ \\mathbb{Z}, ~ \\mathbb{Q}\\) and \\(\\mathbb{R}\\) are closed under both addition and multiplication.\n\\[\nA = (0,1)\n\\]\nis closed under multiplication, but not addition. \\((0.6 + 0.7 = 1.3 &gt; 1)\\)\nThe set of all half integers \\[\n\\frac{\\mathbb{Z}}{2} =\n\\{x : \\exists~{y} \\in \\mathbb{Z}~(x = \\frac{y}{2})\\}\n\\] is closed under addition, but not under multiplication. \\((0.5 * 0.5 = 0.25~\\notin~\\frac{\\mathbb{Z}}{2})\\)\nThe system of linear equations is a collection of two or more linear equations involving the same set of variables. The example is the following: \\[\n\\begin{cases}\n3x + 2y - z = 1 \\\\\n2x - 2y + 4z = -2 \\\\\n-x + \\frac{1}{2}y - z = 0\n\\end{cases}\n\\]\nMatrix is just a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. There is no plain English definition of the determinant, but forthcoming blogs in this series will cover it in detail. Vector space is a set of objects (vectors) closed under finite vector addition and scalar multiplication.\nLinear transformation or linear map is a mapping between two vector spaces that preserves the addition and scalar multiplication rule. More mathematically, a linear transformation between two vector space \\(V\\) and \\(W\\) is a map \\(T : V~\\rightarrow~W\\) such that the following hold:\n\n\\(T(v_1 + v_2) = T(v_1) + T(v_2)\\) for any vectors \\(v_1\\) and \\(v_2\\) in \\(V\\)\n\\(T(\\alpha v_1) = \\alpha T(v_1)\\) for any scalar \\(\\alpha\\)\n\n\n\nConsclusion\nTo sum up, this post is an introduction towards series, where I will introduce you linear algebra concepts intuitively and programmatically in Python. The main idea of this series is to feel comfortable in the field and to give you the direction where to dig deeper."
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html",
    "href": "posts/mathematics/linear_algebra_2.html",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "Introduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn this post I will introduce you to the basics of linear algebra, starting from introduciong one and two variable equations, then moving on to vectors and operations on vectors. After that, I will move on to matrices and operations defined on matrices. I will also show you how to use Python to solve linear algebra problems.\n\n\nGenerally, equations state that two things are equal. They contain one or more variables and solving them means to find the value of those variable to make equality true. This value is known as a solution. Consider the following equation:\n\\[\n2x + 5 = 15\n\\]\nIn this case, our variable is \\(x\\) and the solution is \\(5\\).\n\nx = 5\n\n2 * x + 5 == 15\n\nTrue\n\n\n\n\n\nEquations with two variables are known as linear equations. Consider the following equation:\n\\[\n2y + 3 = 2x - 1\n\\]\nThis equation includes two different variables, \\(x\\), and \\(y\\). These variables depend on one another. The value of \\(x\\) is determined in part by the value of \\(y\\) and vice-versa. So we can’t solve this equation as in the case of one variable equation. However, we can express \\(y\\) in terms of \\(x\\) and obtain a result that describes a relative relationship between the variables.\nFor example, let’s solve this equation for \\(y\\). First, rearrange equation in a way to have the following:\n\\[\n2y = 2x - 4\n\\Rightarrow\ny = x - 2\n\\]\nNote that this is not linear function, this is an affine function\nBelow we will see the solution of the above equation for various values of \\(y\\). It’s also good practice to plot the solutions.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Create a dataframe with a column x, containing values from -10 to 10\ndf = pd.DataFrame({\"x\": range(-10, 11)})\n\n# Add column y, by applying the solved equation to x\ndf[\"y\"] = df[\"x\"] - 2\n\n# Display the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-10\n-12\n\n\n1\n-9\n-11\n\n\n2\n-8\n-10\n\n\n3\n-7\n-9\n\n\n4\n-6\n-8\n\n\n\n\n\n\n\nAbove table shows valid solutions for values of \\(x\\) in range \\((-10, 10)\\). Besides numerical solution, let see the graphical solution.\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df[\"x\"], df[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.show()\n\n\n\n\n\n\n\n\nThe solution of the above equation lies on the blue line, for any value pairs \\((x,y)~\\in~\\mathbb{R}\\)\nWhen we use a linear equation to plot a line, we can easily see where the line intersects the X and Y axes of the plot. These points are known as intercepts. The x-intercept is where the line intersects the X (horizontal) axis, and the y-intercept is where the line intersects the Y (horizontal) axis.\nThe x-intercept is the point where the line crosses the \\(X\\) axis, and at this point, the value for \\(y\\) is always 0. Similarly, the y-intercept is where the line crosses the \\(Y\\) axis, at which \\(x\\) value is 0. So to find the intercepts, we need to solve the equation for \\(x\\) when \\(y\\) is 0 and for \\(y\\) when \\(x\\) is 0.\nFor the x-intercept, we have:\n\\[\ny = x - 2 = 0\n\\Rightarrow\nx = 2\n\\]\nFor y-intercept, we have:\n\\[\ny = x - 2\n\\Rightarrow\ny = 0 - 2\n\\Rightarrow\ny = -2\n\\]\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n\nplt.plot(df[\"x\"], df[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"x-intercept\", (2, 0), color=\"red\", fontsize=12)\nplt.annotate(\"y-intercept\", (0, -2), color=\"red\", fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nIt is natural to ask, what if we move one unit along the \\(x\\) axis, how the value for the \\(y\\) change? The answer to this question is the notion of slope. Slope is defined as\n\\[\nm = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}}\n\\]\nThis means that for any given two ordered pairs of \\(x\\) and \\(y\\), how a change in \\(x\\) affect \\(y\\). For example:\n\n(5, 3)\n(6, 4)\n\nSo, according to our formula, slope equal to:\n\\[\nm = \\frac{4 - 3}{6 - 5} = 1\n\\]\nSo what does that actually mean? Well, if we start from any point on the blue line and move one unit to the right (along with the \\(X\\) axis), we’ll need to move 1 unit up (along with the \\(Y\\) axis) to get back to the blue line.\n\n\n\nTo have the system of equations means that we have two or more linear equations together and we have to solve them simultaneously to make the equality true. There are three possible solutions of the linear system. One solution, No solution or system is inconsistent and infinitely many solutions. Generally, the linear system can have two or more variables and two or more equations. There, I will consider two variable and two-equation system with three solutions, in order to depict the intuition. It’s up to you to delve deeper.\n\nThe system with one solution, meaning two lines intersect\n\n\\[\n\\begin{cases}\n2x-y = 2 \\\\\nx+y = -2\n\\end{cases}\n\\]\nIf we divide these equations we’ll get \\(x=-2\\) and \\(y=0\\). This is the solution. Now let see it graphically.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = 2 * df_1[\"x\"] - 2\n\ndf_2 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_2[\"y\"] = -1 * df_2[\"x\"] - 2\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"2x - y = 2\", (7.5, 18), weight=\"bold\")\nplt.annotate(\"x + y = -2\", (-10, 10), weight=\"bold\")\n\n# I put coordinates(0,-3) intentionally to make annotation look clear\nplt.annotate(\"Solution (x = -2 ; y = 0)\", (0, -3))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe system with no solution or inconsistent system, meaning two lines are parallel\n\n\\[\n\\begin{cases}\n3x+2y = 3 \\\\\n3x+2y = -4\n\\end{cases}\n\\]\nThe system is inconsistent. There is no solution.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = (-3 / 2) * df_1[\"x\"] + 3 / 2\n\ndf_2 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_2[\"y\"] = (-3 / 2) * df_2[\"x\"] - 2\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"3x + 2y = 3\", (-5, 10), weight=\"bold\")\nplt.annotate(\"3x + 2y = -4\", (-10, 7.5), weight=\"bold\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe system with infinitely many solutions, meaning two lines coincide\n\n\\[\n\\begin{cases}\nx-y = -3 \\\\\n2x-2y = -6\n\\end{cases}\n\\]\nThe system has infinitely many solutions, as one of them is a linear combination of another. In this case, the second equation is scaled by 2 version of the first equation.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = df_1[\"x\"] - 3\n\ndf_2 = pd.DataFrame({\"x\": range(-5, 6)})\ndf_2[\"y\"] = df_2[\"x\"] - 3\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"2x - 2y = -6\", (5, 5), weight=\"bold\")\nplt.annotate(\"x - y = -3\", (-5, -5), weight=\"bold\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn plain English, the vector is a directed arrow. Mathematically, the vector is an object that has magnitude and direction. Magnitude is the length of the vector and direction is from its tail to its end. In other words, imagine vector as the line which connects two points in the Cartesian Coordinate System. A vector of length \\(n\\) is a sequence or array of \\(n\\) numbers, which we can be written as:\n\\[\n\\vec{X} = (x_1, x_2, x_3...x_n)\n\\]\nor\n\\[\n\\vec{X} = [x_1, x_2, x_3, ... x_n]\n\\]\nHorizontally represented vector is a row vector, while vertically represented vector is a column vector. Let see how they look graphically.\n\nfig, ax = plt.subplots(figsize=(10, 8))\n# Set the axes through the origin\nfor spine in [\"left\", \"bottom\"]:\n    ax.spines[spine].set_position(\"zero\")\n\nfor spine in [\"right\", \"top\"]:\n    ax.spines[spine].set_color(\"none\")\n\nax.set(xlim=(-6, 6), ylim=(-6, 6))\nax.grid()\n\nvecs = ((2, 4), (-3, 3))  # These are vectors\n\nfor v in vecs:\n    ax.annotate(\"\", xy=v, xytext=(0, 0), arrowprops=dict(facecolor=\"blue\", shrink=0, alpha=0.7, width=0.5))\n    ax.text(1.1 * v[0], 1.1 * v[1], str(v))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe dimension of vector is the number of elements in it. For example, the above vector is a row vector with dimension \\(1\\times n\\), but if we take it as a column vector its dimension will be \\(n\\times 1\\).\n\\[\n\\vec{X} = [x_1,x_2,...x_n]_{1\\times~n}\n\\]\nand\n\\[\n\\vec{X} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}_{n\\times1}\n\\]\n\nX = np.array([[1, 2, 3]])\nY = np.array([[1], [2], [3]])\n\nprint(\"X is a row vector with dimension\", np.shape(X))\nprint(\"Y is a column vector with dimension\", np.shape(Y))\n\nX is a row vector with dimension (1, 3)\nY is a column vector with dimension (3, 1)\n\n\n\n\n\nThe most common operations on vectors are vector addition/subtraction and scalar multiplication.\nIf we have two vectors, \\(\\vec{X}\\) and \\(\\vec{Y}\\), we can add them up in the following way:\n\\[\n\\vec{X} + \\vec{Y} =\n\\begin{bmatrix}\n    x_1 \\\\\n    x_2 \\\\\n    \\vdots \\\\\n    x_n\n\\end{bmatrix}_{n\\times1} +\n\\begin{bmatrix}\n     y_1 \\\\\n     y_2 \\\\\n    \\vdots \\\\\n     y_n\n\\end{bmatrix}_{n\\times1} =\n\\begin{bmatrix}\n    x_1 + y_1 \\\\\n    x_2 + y_2 \\\\\n    \\vdots \\\\\n    x_n + y_n\n\\end{bmatrix}_{n\\times1}\n\\]\n\nX = np.array([[1], [2], [3]])\nY = np.array([[3], [4], [5]])\n\nprint(\"X + Y = \", X + Y, sep=\"\\n\")\n\nX + Y = \n[[4]\n [6]\n [8]]\n\n\nMultiplying vector by a scalar \\(\\alpha\\), gives\n\\[\n\\alpha \\vec{X} =\n\\begin{bmatrix}\n\\alpha x_1 \\\\\n\\alpha x_2 \\\\\n\\vdots \\\\\n\\alpha x_n\n\\end{bmatrix}_{n\\times1}\n\\]\nFor scalar multiplication, if we have vector \\(X\\) and scalar \\(\\alpha = 5\\) then alpha times \\(X\\) is:\n\nX = np.array([[1, 2, 3]])\n\nprint(5 * X)\n\n[[ 5 10 15]]\n\n\n\n\n\nThe vector length or the magnitude is calculated by the following formula:\n\\[\n\\|\\vec{X}\\| = \\sqrt{x_1^2 + x_2^2 + x_3^2 + ... + x_n^2} = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nWe can link notion of vector length to the Euclidean distance. If our vector \\(\\vec{X}\\) has tail at origin, \\(\\vec{0} = [0_1, 0_2, 0_3, ... , 0_n]\\) and point at \\(\\vec{X} = [x_1,x_2,...x_n]\\), then Euclidean distance between tail and point is the length of \\(\\vec{X}\\) by the formula:\n\\[\nd(\\vec{0},\\vec{X}) = \\sqrt{(0_1 - x_1)^2 + (0_2 - x_2)^2 + ... + (0_n - x_n)^2} = \\sqrt{\\sum_{i=1}^n (0_i - x_i)^2}\n\\]\nFor example, we have vector \\(X\\)\n\\[\n\\vec{X} =\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\nthen its length is\n\\[\n\\|\\vec{X}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\n\\]\n\nX = np.array([1, 2, 3])\n\n# In numpy, there are two ways to compute vector length\n\nprint(np.sqrt(np.sum(X**2)))  # More verbose method\nprint(np.linalg.norm(X))  # More concise method\n\n3.7416573867739413\n3.7416573867739413\n\n\n\n\n\nWhat if the length of a vector equal to 1? This kind of vector is known as the unit vector and it plays a very important role in different calculations and formulae. We’ll see it in later posts, but here unit vector is defined as\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|}\n\\]\nwhere \\(\\hat{X}\\) is a unit vector, the numerator is vector \\(\\vec{X}\\) and denominator is the norm of vector \\(\\vec{X}\\).\nWe can use vector \\(X\\) from above example. We already calculated it length which is \\(\\|\\vec{X}\\| = \\sqrt{14}\\). So, we can construct unit vector \\(\\hat{X}\\) in the following way:\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|} =\n\\frac{1}{\\sqrt{14}}; \\frac{2}{\\sqrt{14}}; \\frac{3}{\\sqrt{14}}\n\\Rightarrow\n[0.26726124; \\ 0.53452248; \\ 0.80178373]\n\\]\n\nX = np.array([1, 2, 3])\n\nn = X / np.linalg.norm(X)\n\nprint(\"Vector n =\", n)\n\nVector n = [0.26726124 0.53452248 0.80178373]\n\n\n\n# If we take length of vector n, we'll get 1\n\nprint(\"The length of vector n is:\", np.linalg.norm(n))\n\nThe length of vector n is: 1.0\n\n\n\n\n\nMultiplication of two vectors is known as dot product, scalar product, or inner product and is defined by:\n\\[\n\\langle\\, \\vec{X},\\vec{Y}\\rangle~=~\\vec{X}\\cdot\\vec{Y}~=~x_1\\times y_1 + x_2\\times y_2 + ... + x_n\\times y_n~=~\\sum_{i=1}^n x_i\\cdot y_i\n\\]\nThe inner product is defined only when the dimensions of two vectors coincide.\nAnother formula of inner product is:\n\\[\n\\vec{X}\\cdot\\vec{Y}~=~\\|\\vec{X}\\|\\cdot\\|\\vec{Y}\\|\\cdot\\cos{\\theta}\n\\]\nwhere \\(\\cos{\\theta}\\) is an angle between the vectors \\(\\vec{X}\\) and \\(\\vec{Y}\\).\n\n# It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple.\nX = np.array([1, 2, 3])\nY = np.array((2, 4, 6))\n\n# Numpy have two possible ways to compute vector inner product\n\nscalar_prod = np.sum(X * Y)\n\ndot_prod = np.dot(X, Y)\n\nprint(\"Scalar Product is:\", scalar_prod)\nprint(\"Dot Product is:\", dot_prod)\n\nScalar Product is: 28\nDot Product is: 28\n\n\n\n# Compute angle between vector X and vector Y\ncosine_theta = np.dot(X, Y) / (np.linalg.norm(X) * np.linalg.norm(Y))\n\n# Angle theta\ntheta = np.arccos(cosine_theta)\n\nprint(\"Cosine theta is:\", cosine_theta)\nprint(\"Angle theta is:\", theta)\n\nCosine theta is: 1.0\nAngle theta is: 0.0\n\n\n\n\n\n\n\n\nMatrix is a rectangular array of numbers and/or expressions that are arranged into rows and columns. These rows and columns can be considered as row and column vectors. So, the matrix is the rectangular array which contains either row or column vectors. Generally, capital letters are used to denote matrix and lower case letters to denote each element of that matrix and I will follow this convention. A matrix arranges numbers into rows and columns, like this:\n\\[\nA =\n\\begin{bmatrix}\na_{1,1} & a_{1,2}\\\\\na_{2,1} & a_{2,2}\n\\end{bmatrix}\n\\]\nHere, matrix \\(A\\) has four elements, denoted by lower letter \\(a\\), where subscripts denote row and column number. For example, \\(a_{2,1}\\) denotes element at the cross of the second row and first column.\n\n\n\nIf a matrix \\(A\\) has \\(n\\) rows and \\(m\\) columns, we call \\(A\\) \\(n\\times m\\) matrix and it is read as “n by m matrix”. A typical \\(n\\times m\\) matrix \\(A\\) can be written as:\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}_{n \\times m}\n\\]\nWhen \\(n=m\\) we have square matrix. To link this matrix to the vector we can rewrite it by the following way:\n\\[\nA =\n\\begin{bmatrix}\n[a_{11} & a_{12} & \\cdots & a_{1m}] \\\\\n[a_{21} & a_{22} & \\cdots & a_{2m}] \\\\\n[\\vdots & \\vdots &  & \\vdots] \\\\\n[a_{n1} & a_{n2} & \\cdots & a_{nm}]\n\\end{bmatrix}_{n \\times m}\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"Matrix A is\", A, sep=\"\\n\")\n\nprint(\"Dimensions of A is:\", np.shape(A))\n\nMatrix A is\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nDimensions of A is: (3, 3)\n\n\n\n\n\nWe add two matrices element-wise. In order this addition to exist, we require that the dimensions of the two matrices coincide. If we have two matrices, \\(A\\) and \\(B\\), addition is defined by:\n\\[\nA + B =\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm} \\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{11} & \\cdots & b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nb_{n1} & \\cdots & b_{nm} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\na_{11} + b_{11} &  \\cdots & a_{1m} + b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} + b_{n1} &  \\cdots & a_{nm} + b_{nm} \\\\\n\\end{bmatrix}\n\\]\nMatrix subtraction is defined in the same fashion as the addition.\n\nA = np.array([[1, 5, 3], [4, 2, 8], [3, 6, 9]])\n\nB = np.array([[1, 1, 3], [1, 2, 8], [0, 5, 3]])\n\nprint(\"Matrix A is:\", A, sep=\"\\n\")\n\nprint(\"Matrix B is:\", B, sep=\"\\n\")\n\nprint(\"The sum of them is:\", A + B, sep=\"\\n\")\n\nprint(\"The difference of them is:\", A - B, sep=\"\\n\")\n\nMatrix A is:\n[[1 5 3]\n [4 2 8]\n [3 6 9]]\nMatrix B is:\n[[1 1 3]\n [1 2 8]\n [0 5 3]]\nThe sum of them is:\n[[ 2  6  6]\n [ 5  4 16]\n [ 3 11 12]]\nThe difference of them is:\n[[0 4 0]\n [3 0 0]\n [3 1 6]]\n\n\nThe negative of a matrix, is just a matrix with the sign of each element reversed:\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\]\n\\[\n-A =\n\\begin{bmatrix}\n-a_{11} & -a_{12} & \\cdots & -a_{1m} \\\\\n-a_{21} & -a_{22} & \\cdots & -a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\n-a_{n1} & -a_{n2} & \\cdots & -a_{nm}\n\\end{bmatrix}\n\\]\n\nC = np.array([[-5, -3, -1], [1, 3, 5]])\n\nprint(\"Matrix C is:\", C, sep=\"\\n\")\n\nprint(\"The negative of C is:\", -C, sep=\"\\n\")\n\nMatrix C is:\n[[-5 -3 -1]\n [ 1  3  5]]\nThe negative of C is:\n[[ 5  3  1]\n [-1 -3 -5]]\n\n\nMultiplying matrices is a little more complex than the operations we’ve seen so far. There are two cases to consider. One is scalar multiplication (multiplying a matrix by a single number), and second is matrix multiplication (multiplying a matrix by another matrix).\nIf we have some scalar or number \\(\\gamma\\) and matrix \\(A\\), scalar multiplication is:\n\\[\n\\gamma A =\n\\gamma\n\\begin{bmatrix}\na_{11} &  \\cdots & a_{1m} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\na_{n1} &  \\cdots & a_{nm} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\gamma a_{11} & \\cdots & \\gamma a_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\gamma a_{n1} & \\cdots & \\gamma a_{nm} \\\\\n\\end{bmatrix}\n\\]\n\nscalar = 2\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n\nprint(\"Scalar is:\", scalar)\n\nprint(\"Matrix A is:\", A, sep=\"\\n\")\n\nprint(\"Matrix multiplied by the scalar is:\", scalar * A, sep=\"\\n\")\n\nScalar is: 2\nMatrix A is:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nMatrix multiplied by the scalar is:\n[[ 2  4  6]\n [ 8 10 12]\n [14 16 18]]\n\n\nTo multiply two matrices, we take inner product of \\(i\\)-th row of the matrix \\(A\\) and \\(j\\)-th columns of matrix \\(B\\). If we have two matrices \\(A\\) is \\(n \\times k\\) and \\(B\\) is \\(j \\times m\\), then to multiply \\(A\\) and \\(B\\), we require \\(k=j\\), and resulting matrix \\(AB\\) is \\(n \\times m\\).\n\\[\nA \\cdot B =\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1k} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nk} \\\\\n\\end{bmatrix}_{n\\times k}\n\\cdot\n\\begin{bmatrix}\nb_{11} & \\cdots & b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nb_{j1} & \\cdots & b_{jm} \\\\\n\\end{bmatrix}_{j \\times m}\n= \\quad\n\\]\n\\[\n\\begin{bmatrix}\n(a_{11} \\times b_{11} & +~\\cdots~+ & a_{1k} \\times b_{j1}),~\\cdots~,(a_{11} \\times b_{1m} & +~\\cdots~+ & a_{1k} \\times b_{jm}) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n(a_{n1} \\times b_{11} & +~\\cdots~+ & a_{nk} \\times b_{j1}),~\\cdots~,(a_{n1} \\times b_{1m} & +~\\cdots~+ & a_{nk} \\times b_{jm})\\\\\n\\end{bmatrix}_{n \\times m}\n\\]\nIf you did not catch the idea of matrix multiplication don’t worry. It takes time to get used to it. Below, I will provide a numerical example to make it more clear.\n\nNote that, in matrix multiplication, \\(A \\cdot B\\) is not same as \\(B \\cdot A\\).\n\n\n# In Numpy matrix multiplication can be done with no effort\n\nA = np.array([[1, 2, 3], [4, 5, 6]])\n\nB = np.array([[9, 8], [7, 6], [5, 4]])\n\nprint(\"A * B is:\", np.dot(A, B), sep=\"\\n\")\n\nprint(\"B * A is:\", np.dot(B, A), sep=\"\\n\")\n\nA * B is:\n[[ 38  32]\n [101  86]]\nB * A is:\n[[41 58 75]\n [31 44 57]\n [21 30 39]]\n\n\nIt’s really easy to multiply a matrix by a matrix in Numpy. Let us see how it can be done by hand.\nLet’s look at an example:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\ 4 & 5 & 6\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n9 & 8 \\\\ 7 & 6 \\\\ 5 & 4\n\\end{bmatrix}\n\\]\nNote that the first matrix is \\(2\\times 3\\), and the second matrix is \\(3\\times 2\\). The important thing here is that the first matrix has two rows, and the second matrix has two columns. To perform the multiplication, we first take the dot product of the first row of the first matrix (1,2,3) and the first column of the second matrix (9,7,5):\n\\[\n(1,2,3)\n\\cdot\n(9,7,5) =\n(1 \\times 9) + (2 \\times 7) + (3 \\times 5) = 38\n\\]\nIn our resulting matrix (which will always have the same number of rows as the first matrix, and the same number of columns as the second matrix), we can enter this into the first row and first column element:\n\\[\n\\begin{bmatrix}\n38 & ?\\\\? & ?\n\\end{bmatrix}\n\\]\nNow we can take the dot product of the first row of the first matrix and the second column of the second matrix:\n\\[\n(1,2,3) \\cdot (8,6,4) = (1 \\times 8) + (2 \\times 6) + (3 \\times 4) = 32\n\\]\nLet’s add that to our resulting matrix in the first row and second column element:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\? & ?\n\\end{bmatrix}\n\\]\nNow we can repeat this process for the second row of the first matrix and the first column of the second matrix:\n\\[\n(4,5,6) \\cdot (9,7,5) = (4 \\times 9) + (5 \\times 7) + (6 \\times 5) = 101\n\\]\nWhich fills in the next element in the result:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\101 & ?\n\\end{bmatrix}\n\\]\nFinally, we get the dot product for the second row of the first matrix and the second column of the second matrix:\n\\[\n(4,5,6) \\cdot (8,6,4) = (4 \\times 8) + (5 \\times 6) + (6 \\times 4) = 86\n\\]\nGiving us:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\101 & 86\n\\end{bmatrix}\n\\]\nIf this is not enough to catch the idea, take a look this explanation\n\n\n\nIn the above example, we saw that the matrix is the collection of vectors. We also know that vectors can be horizontal as well as vertical, or row and column vectors. Now, what if we change in any matrix row vectors into column vectors? This operation is known as transposition. The idea of this operation is to change matrix rows into matrix columns or vice versa, and is denoted by the superscript \\(T\\).\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}_{n \\times m}\n\\]\nthen\n\\[\nA^{T} =\n\\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{n1} \\\\\na_{12} & a_{22} & \\cdots & a_{n2} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{1m} & a_{2m} & \\cdots & a_{nm}\n\\end{bmatrix}_{m \\times n}\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"Matix A is:\", A, sep=\"\\n\")\n\nprint(\"Transpose of A is:\", A.T, sep=\"\\n\")\n\nMatix A is:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nTranspose of A is:\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n\n\n\n\n\nThere are several different types of matrices. In this post, we will introduce only identity matix. Future post will introduce other types of matrices. An identity matrix (usually indicated by a capital \\(I\\)) is the equivalent in matrix terms of the number 1. It is always square matrix, and it has the value 1 in the diagonal element positions I1,1, I2,2, etc; and 0 in all other element positions. Here’s an example of a \\(3 \\times 3\\) identity matrix:\n\\[\nI =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}_{3 \\times 3}\n\\]\nMultiplying any matrix by an identity matrix is the same as multiplying a number by 1; the result is the same as the original value.\n\n# We have two ways to define identity matrix in Numpy.\n# First is to define by hand, like above examples, and second is to use Numpy's buildin function\n\nI_1 = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n\nI_2 = np.identity(3)\n\nprint(I_1)\n\nprint(I_2)\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n# Matrix identity multiplication\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nI = np.identity(3)\n\nprint(\"A = \", A, sep=\"\\n\")\n\nprint(\"A * I = \", np.dot(A, I), sep=\"\\n\")\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nA * I = \n[[1. 2. 3.]\n [4. 5. 6.]\n [7. 8. 9.]]"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html#one-variable-equation",
    "href": "posts/mathematics/linear_algebra_2.html#one-variable-equation",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "Generally, equations state that two things are equal. They contain one or more variables and solving them means to find the value of those variable to make equality true. This value is known as a solution. Consider the following equation:\n\\[\n2x + 5 = 15\n\\]\nIn this case, our variable is \\(x\\) and the solution is \\(5\\).\n\nx = 5\n\n2 * x + 5 == 15\n\nTrue"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html#two-variable-equation",
    "href": "posts/mathematics/linear_algebra_2.html#two-variable-equation",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "Equations with two variables are known as linear equations. Consider the following equation:\n\\[\n2y + 3 = 2x - 1\n\\]\nThis equation includes two different variables, \\(x\\), and \\(y\\). These variables depend on one another. The value of \\(x\\) is determined in part by the value of \\(y\\) and vice-versa. So we can’t solve this equation as in the case of one variable equation. However, we can express \\(y\\) in terms of \\(x\\) and obtain a result that describes a relative relationship between the variables.\nFor example, let’s solve this equation for \\(y\\). First, rearrange equation in a way to have the following:\n\\[\n2y = 2x - 4\n\\Rightarrow\ny = x - 2\n\\]\nNote that this is not linear function, this is an affine function\nBelow we will see the solution of the above equation for various values of \\(y\\). It’s also good practice to plot the solutions.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Create a dataframe with a column x, containing values from -10 to 10\ndf = pd.DataFrame({\"x\": range(-10, 11)})\n\n# Add column y, by applying the solved equation to x\ndf[\"y\"] = df[\"x\"] - 2\n\n# Display the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-10\n-12\n\n\n1\n-9\n-11\n\n\n2\n-8\n-10\n\n\n3\n-7\n-9\n\n\n4\n-6\n-8\n\n\n\n\n\n\n\nAbove table shows valid solutions for values of \\(x\\) in range \\((-10, 10)\\). Besides numerical solution, let see the graphical solution.\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df[\"x\"], df[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.show()\n\n\n\n\n\n\n\n\nThe solution of the above equation lies on the blue line, for any value pairs \\((x,y)~\\in~\\mathbb{R}\\)\nWhen we use a linear equation to plot a line, we can easily see where the line intersects the X and Y axes of the plot. These points are known as intercepts. The x-intercept is where the line intersects the X (horizontal) axis, and the y-intercept is where the line intersects the Y (horizontal) axis.\nThe x-intercept is the point where the line crosses the \\(X\\) axis, and at this point, the value for \\(y\\) is always 0. Similarly, the y-intercept is where the line crosses the \\(Y\\) axis, at which \\(x\\) value is 0. So to find the intercepts, we need to solve the equation for \\(x\\) when \\(y\\) is 0 and for \\(y\\) when \\(x\\) is 0.\nFor the x-intercept, we have:\n\\[\ny = x - 2 = 0\n\\Rightarrow\nx = 2\n\\]\nFor y-intercept, we have:\n\\[\ny = x - 2\n\\Rightarrow\ny = 0 - 2\n\\Rightarrow\ny = -2\n\\]\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n\nplt.plot(df[\"x\"], df[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"x-intercept\", (2, 0), color=\"red\", fontsize=12)\nplt.annotate(\"y-intercept\", (0, -2), color=\"red\", fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nIt is natural to ask, what if we move one unit along the \\(x\\) axis, how the value for the \\(y\\) change? The answer to this question is the notion of slope. Slope is defined as\n\\[\nm = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}}\n\\]\nThis means that for any given two ordered pairs of \\(x\\) and \\(y\\), how a change in \\(x\\) affect \\(y\\). For example:\n\n(5, 3)\n(6, 4)\n\nSo, according to our formula, slope equal to:\n\\[\nm = \\frac{4 - 3}{6 - 5} = 1\n\\]\nSo what does that actually mean? Well, if we start from any point on the blue line and move one unit to the right (along with the \\(X\\) axis), we’ll need to move 1 unit up (along with the \\(Y\\) axis) to get back to the blue line."
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html#the-systems-of-equations",
    "href": "posts/mathematics/linear_algebra_2.html#the-systems-of-equations",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "To have the system of equations means that we have two or more linear equations together and we have to solve them simultaneously to make the equality true. There are three possible solutions of the linear system. One solution, No solution or system is inconsistent and infinitely many solutions. Generally, the linear system can have two or more variables and two or more equations. There, I will consider two variable and two-equation system with three solutions, in order to depict the intuition. It’s up to you to delve deeper.\n\nThe system with one solution, meaning two lines intersect\n\n\\[\n\\begin{cases}\n2x-y = 2 \\\\\nx+y = -2\n\\end{cases}\n\\]\nIf we divide these equations we’ll get \\(x=-2\\) and \\(y=0\\). This is the solution. Now let see it graphically.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = 2 * df_1[\"x\"] - 2\n\ndf_2 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_2[\"y\"] = -1 * df_2[\"x\"] - 2\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"2x - y = 2\", (7.5, 18), weight=\"bold\")\nplt.annotate(\"x + y = -2\", (-10, 10), weight=\"bold\")\n\n# I put coordinates(0,-3) intentionally to make annotation look clear\nplt.annotate(\"Solution (x = -2 ; y = 0)\", (0, -3))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe system with no solution or inconsistent system, meaning two lines are parallel\n\n\\[\n\\begin{cases}\n3x+2y = 3 \\\\\n3x+2y = -4\n\\end{cases}\n\\]\nThe system is inconsistent. There is no solution.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = (-3 / 2) * df_1[\"x\"] + 3 / 2\n\ndf_2 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_2[\"y\"] = (-3 / 2) * df_2[\"x\"] - 2\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"3x + 2y = 3\", (-5, 10), weight=\"bold\")\nplt.annotate(\"3x + 2y = -4\", (-10, 7.5), weight=\"bold\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe system with infinitely many solutions, meaning two lines coincide\n\n\\[\n\\begin{cases}\nx-y = -3 \\\\\n2x-2y = -6\n\\end{cases}\n\\]\nThe system has infinitely many solutions, as one of them is a linear combination of another. In this case, the second equation is scaled by 2 version of the first equation.\n\ndf_1 = pd.DataFrame({\"x\": range(-10, 11)})\ndf_1[\"y\"] = df_1[\"x\"] - 3\n\ndf_2 = pd.DataFrame({\"x\": range(-5, 6)})\ndf_2[\"y\"] = df_2[\"x\"] - 3\n\nplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor=\"w\", edgecolor=\"k\")\nplt.plot(df_1[\"x\"], df_1[\"y\"])\nplt.plot(df_2[\"x\"], df_2[\"y\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid()\nplt.axhline(color=\"black\")\nplt.axvline(color=\"black\")\nplt.annotate(\"2x - 2y = -6\", (5, 5), weight=\"bold\")\nplt.annotate(\"x - y = -3\", (-5, -5), weight=\"bold\")\nplt.show()"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html#vectors",
    "href": "posts/mathematics/linear_algebra_2.html#vectors",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "In plain English, the vector is a directed arrow. Mathematically, the vector is an object that has magnitude and direction. Magnitude is the length of the vector and direction is from its tail to its end. In other words, imagine vector as the line which connects two points in the Cartesian Coordinate System. A vector of length \\(n\\) is a sequence or array of \\(n\\) numbers, which we can be written as:\n\\[\n\\vec{X} = (x_1, x_2, x_3...x_n)\n\\]\nor\n\\[\n\\vec{X} = [x_1, x_2, x_3, ... x_n]\n\\]\nHorizontally represented vector is a row vector, while vertically represented vector is a column vector. Let see how they look graphically.\n\nfig, ax = plt.subplots(figsize=(10, 8))\n# Set the axes through the origin\nfor spine in [\"left\", \"bottom\"]:\n    ax.spines[spine].set_position(\"zero\")\n\nfor spine in [\"right\", \"top\"]:\n    ax.spines[spine].set_color(\"none\")\n\nax.set(xlim=(-6, 6), ylim=(-6, 6))\nax.grid()\n\nvecs = ((2, 4), (-3, 3))  # These are vectors\n\nfor v in vecs:\n    ax.annotate(\"\", xy=v, xytext=(0, 0), arrowprops=dict(facecolor=\"blue\", shrink=0, alpha=0.7, width=0.5))\n    ax.text(1.1 * v[0], 1.1 * v[1], str(v))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe dimension of vector is the number of elements in it. For example, the above vector is a row vector with dimension \\(1\\times n\\), but if we take it as a column vector its dimension will be \\(n\\times 1\\).\n\\[\n\\vec{X} = [x_1,x_2,...x_n]_{1\\times~n}\n\\]\nand\n\\[\n\\vec{X} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}_{n\\times1}\n\\]\n\nX = np.array([[1, 2, 3]])\nY = np.array([[1], [2], [3]])\n\nprint(\"X is a row vector with dimension\", np.shape(X))\nprint(\"Y is a column vector with dimension\", np.shape(Y))\n\nX is a row vector with dimension (1, 3)\nY is a column vector with dimension (3, 1)\n\n\n\n\n\nThe most common operations on vectors are vector addition/subtraction and scalar multiplication.\nIf we have two vectors, \\(\\vec{X}\\) and \\(\\vec{Y}\\), we can add them up in the following way:\n\\[\n\\vec{X} + \\vec{Y} =\n\\begin{bmatrix}\n    x_1 \\\\\n    x_2 \\\\\n    \\vdots \\\\\n    x_n\n\\end{bmatrix}_{n\\times1} +\n\\begin{bmatrix}\n     y_1 \\\\\n     y_2 \\\\\n    \\vdots \\\\\n     y_n\n\\end{bmatrix}_{n\\times1} =\n\\begin{bmatrix}\n    x_1 + y_1 \\\\\n    x_2 + y_2 \\\\\n    \\vdots \\\\\n    x_n + y_n\n\\end{bmatrix}_{n\\times1}\n\\]\n\nX = np.array([[1], [2], [3]])\nY = np.array([[3], [4], [5]])\n\nprint(\"X + Y = \", X + Y, sep=\"\\n\")\n\nX + Y = \n[[4]\n [6]\n [8]]\n\n\nMultiplying vector by a scalar \\(\\alpha\\), gives\n\\[\n\\alpha \\vec{X} =\n\\begin{bmatrix}\n\\alpha x_1 \\\\\n\\alpha x_2 \\\\\n\\vdots \\\\\n\\alpha x_n\n\\end{bmatrix}_{n\\times1}\n\\]\nFor scalar multiplication, if we have vector \\(X\\) and scalar \\(\\alpha = 5\\) then alpha times \\(X\\) is:\n\nX = np.array([[1, 2, 3]])\n\nprint(5 * X)\n\n[[ 5 10 15]]\n\n\n\n\n\nThe vector length or the magnitude is calculated by the following formula:\n\\[\n\\|\\vec{X}\\| = \\sqrt{x_1^2 + x_2^2 + x_3^2 + ... + x_n^2} = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nWe can link notion of vector length to the Euclidean distance. If our vector \\(\\vec{X}\\) has tail at origin, \\(\\vec{0} = [0_1, 0_2, 0_3, ... , 0_n]\\) and point at \\(\\vec{X} = [x_1,x_2,...x_n]\\), then Euclidean distance between tail and point is the length of \\(\\vec{X}\\) by the formula:\n\\[\nd(\\vec{0},\\vec{X}) = \\sqrt{(0_1 - x_1)^2 + (0_2 - x_2)^2 + ... + (0_n - x_n)^2} = \\sqrt{\\sum_{i=1}^n (0_i - x_i)^2}\n\\]\nFor example, we have vector \\(X\\)\n\\[\n\\vec{X} =\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\nthen its length is\n\\[\n\\|\\vec{X}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\n\\]\n\nX = np.array([1, 2, 3])\n\n# In numpy, there are two ways to compute vector length\n\nprint(np.sqrt(np.sum(X**2)))  # More verbose method\nprint(np.linalg.norm(X))  # More concise method\n\n3.7416573867739413\n3.7416573867739413\n\n\n\n\n\nWhat if the length of a vector equal to 1? This kind of vector is known as the unit vector and it plays a very important role in different calculations and formulae. We’ll see it in later posts, but here unit vector is defined as\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|}\n\\]\nwhere \\(\\hat{X}\\) is a unit vector, the numerator is vector \\(\\vec{X}\\) and denominator is the norm of vector \\(\\vec{X}\\).\nWe can use vector \\(X\\) from above example. We already calculated it length which is \\(\\|\\vec{X}\\| = \\sqrt{14}\\). So, we can construct unit vector \\(\\hat{X}\\) in the following way:\n\\[\n\\hat{X} = \\frac{X}{\\|X\\|} =\n\\frac{1}{\\sqrt{14}}; \\frac{2}{\\sqrt{14}}; \\frac{3}{\\sqrt{14}}\n\\Rightarrow\n[0.26726124; \\ 0.53452248; \\ 0.80178373]\n\\]\n\nX = np.array([1, 2, 3])\n\nn = X / np.linalg.norm(X)\n\nprint(\"Vector n =\", n)\n\nVector n = [0.26726124 0.53452248 0.80178373]\n\n\n\n# If we take length of vector n, we'll get 1\n\nprint(\"The length of vector n is:\", np.linalg.norm(n))\n\nThe length of vector n is: 1.0\n\n\n\n\n\nMultiplication of two vectors is known as dot product, scalar product, or inner product and is defined by:\n\\[\n\\langle\\, \\vec{X},\\vec{Y}\\rangle~=~\\vec{X}\\cdot\\vec{Y}~=~x_1\\times y_1 + x_2\\times y_2 + ... + x_n\\times y_n~=~\\sum_{i=1}^n x_i\\cdot y_i\n\\]\nThe inner product is defined only when the dimensions of two vectors coincide.\nAnother formula of inner product is:\n\\[\n\\vec{X}\\cdot\\vec{Y}~=~\\|\\vec{X}\\|\\cdot\\|\\vec{Y}\\|\\cdot\\cos{\\theta}\n\\]\nwhere \\(\\cos{\\theta}\\) is an angle between the vectors \\(\\vec{X}\\) and \\(\\vec{Y}\\).\n\n# It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple.\nX = np.array([1, 2, 3])\nY = np.array((2, 4, 6))\n\n# Numpy have two possible ways to compute vector inner product\n\nscalar_prod = np.sum(X * Y)\n\ndot_prod = np.dot(X, Y)\n\nprint(\"Scalar Product is:\", scalar_prod)\nprint(\"Dot Product is:\", dot_prod)\n\nScalar Product is: 28\nDot Product is: 28\n\n\n\n# Compute angle between vector X and vector Y\ncosine_theta = np.dot(X, Y) / (np.linalg.norm(X) * np.linalg.norm(Y))\n\n# Angle theta\ntheta = np.arccos(cosine_theta)\n\nprint(\"Cosine theta is:\", cosine_theta)\nprint(\"Angle theta is:\", theta)\n\nCosine theta is: 1.0\nAngle theta is: 0.0"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_2.html#matrices",
    "href": "posts/mathematics/linear_algebra_2.html#matrices",
    "title": "Basic Linear Algebra with Python",
    "section": "",
    "text": "Matrix is a rectangular array of numbers and/or expressions that are arranged into rows and columns. These rows and columns can be considered as row and column vectors. So, the matrix is the rectangular array which contains either row or column vectors. Generally, capital letters are used to denote matrix and lower case letters to denote each element of that matrix and I will follow this convention. A matrix arranges numbers into rows and columns, like this:\n\\[\nA =\n\\begin{bmatrix}\na_{1,1} & a_{1,2}\\\\\na_{2,1} & a_{2,2}\n\\end{bmatrix}\n\\]\nHere, matrix \\(A\\) has four elements, denoted by lower letter \\(a\\), where subscripts denote row and column number. For example, \\(a_{2,1}\\) denotes element at the cross of the second row and first column.\n\n\n\nIf a matrix \\(A\\) has \\(n\\) rows and \\(m\\) columns, we call \\(A\\) \\(n\\times m\\) matrix and it is read as “n by m matrix”. A typical \\(n\\times m\\) matrix \\(A\\) can be written as:\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}_{n \\times m}\n\\]\nWhen \\(n=m\\) we have square matrix. To link this matrix to the vector we can rewrite it by the following way:\n\\[\nA =\n\\begin{bmatrix}\n[a_{11} & a_{12} & \\cdots & a_{1m}] \\\\\n[a_{21} & a_{22} & \\cdots & a_{2m}] \\\\\n[\\vdots & \\vdots &  & \\vdots] \\\\\n[a_{n1} & a_{n2} & \\cdots & a_{nm}]\n\\end{bmatrix}_{n \\times m}\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"Matrix A is\", A, sep=\"\\n\")\n\nprint(\"Dimensions of A is:\", np.shape(A))\n\nMatrix A is\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nDimensions of A is: (3, 3)\n\n\n\n\n\nWe add two matrices element-wise. In order this addition to exist, we require that the dimensions of the two matrices coincide. If we have two matrices, \\(A\\) and \\(B\\), addition is defined by:\n\\[\nA + B =\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm} \\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{11} & \\cdots & b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nb_{n1} & \\cdots & b_{nm} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\na_{11} + b_{11} &  \\cdots & a_{1m} + b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} + b_{n1} &  \\cdots & a_{nm} + b_{nm} \\\\\n\\end{bmatrix}\n\\]\nMatrix subtraction is defined in the same fashion as the addition.\n\nA = np.array([[1, 5, 3], [4, 2, 8], [3, 6, 9]])\n\nB = np.array([[1, 1, 3], [1, 2, 8], [0, 5, 3]])\n\nprint(\"Matrix A is:\", A, sep=\"\\n\")\n\nprint(\"Matrix B is:\", B, sep=\"\\n\")\n\nprint(\"The sum of them is:\", A + B, sep=\"\\n\")\n\nprint(\"The difference of them is:\", A - B, sep=\"\\n\")\n\nMatrix A is:\n[[1 5 3]\n [4 2 8]\n [3 6 9]]\nMatrix B is:\n[[1 1 3]\n [1 2 8]\n [0 5 3]]\nThe sum of them is:\n[[ 2  6  6]\n [ 5  4 16]\n [ 3 11 12]]\nThe difference of them is:\n[[0 4 0]\n [3 0 0]\n [3 1 6]]\n\n\nThe negative of a matrix, is just a matrix with the sign of each element reversed:\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\]\n\\[\n-A =\n\\begin{bmatrix}\n-a_{11} & -a_{12} & \\cdots & -a_{1m} \\\\\n-a_{21} & -a_{22} & \\cdots & -a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\n-a_{n1} & -a_{n2} & \\cdots & -a_{nm}\n\\end{bmatrix}\n\\]\n\nC = np.array([[-5, -3, -1], [1, 3, 5]])\n\nprint(\"Matrix C is:\", C, sep=\"\\n\")\n\nprint(\"The negative of C is:\", -C, sep=\"\\n\")\n\nMatrix C is:\n[[-5 -3 -1]\n [ 1  3  5]]\nThe negative of C is:\n[[ 5  3  1]\n [-1 -3 -5]]\n\n\nMultiplying matrices is a little more complex than the operations we’ve seen so far. There are two cases to consider. One is scalar multiplication (multiplying a matrix by a single number), and second is matrix multiplication (multiplying a matrix by another matrix).\nIf we have some scalar or number \\(\\gamma\\) and matrix \\(A\\), scalar multiplication is:\n\\[\n\\gamma A =\n\\gamma\n\\begin{bmatrix}\na_{11} &  \\cdots & a_{1m} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\na_{n1} &  \\cdots & a_{nm} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\gamma a_{11} & \\cdots & \\gamma a_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\gamma a_{n1} & \\cdots & \\gamma a_{nm} \\\\\n\\end{bmatrix}\n\\]\n\nscalar = 2\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n\nprint(\"Scalar is:\", scalar)\n\nprint(\"Matrix A is:\", A, sep=\"\\n\")\n\nprint(\"Matrix multiplied by the scalar is:\", scalar * A, sep=\"\\n\")\n\nScalar is: 2\nMatrix A is:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nMatrix multiplied by the scalar is:\n[[ 2  4  6]\n [ 8 10 12]\n [14 16 18]]\n\n\nTo multiply two matrices, we take inner product of \\(i\\)-th row of the matrix \\(A\\) and \\(j\\)-th columns of matrix \\(B\\). If we have two matrices \\(A\\) is \\(n \\times k\\) and \\(B\\) is \\(j \\times m\\), then to multiply \\(A\\) and \\(B\\), we require \\(k=j\\), and resulting matrix \\(AB\\) is \\(n \\times m\\).\n\\[\nA \\cdot B =\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1k} \\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nk} \\\\\n\\end{bmatrix}_{n\\times k}\n\\cdot\n\\begin{bmatrix}\nb_{11} & \\cdots & b_{1m} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nb_{j1} & \\cdots & b_{jm} \\\\\n\\end{bmatrix}_{j \\times m}\n= \\quad\n\\]\n\\[\n\\begin{bmatrix}\n(a_{11} \\times b_{11} & +~\\cdots~+ & a_{1k} \\times b_{j1}),~\\cdots~,(a_{11} \\times b_{1m} & +~\\cdots~+ & a_{1k} \\times b_{jm}) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n(a_{n1} \\times b_{11} & +~\\cdots~+ & a_{nk} \\times b_{j1}),~\\cdots~,(a_{n1} \\times b_{1m} & +~\\cdots~+ & a_{nk} \\times b_{jm})\\\\\n\\end{bmatrix}_{n \\times m}\n\\]\nIf you did not catch the idea of matrix multiplication don’t worry. It takes time to get used to it. Below, I will provide a numerical example to make it more clear.\n\nNote that, in matrix multiplication, \\(A \\cdot B\\) is not same as \\(B \\cdot A\\).\n\n\n# In Numpy matrix multiplication can be done with no effort\n\nA = np.array([[1, 2, 3], [4, 5, 6]])\n\nB = np.array([[9, 8], [7, 6], [5, 4]])\n\nprint(\"A * B is:\", np.dot(A, B), sep=\"\\n\")\n\nprint(\"B * A is:\", np.dot(B, A), sep=\"\\n\")\n\nA * B is:\n[[ 38  32]\n [101  86]]\nB * A is:\n[[41 58 75]\n [31 44 57]\n [21 30 39]]\n\n\nIt’s really easy to multiply a matrix by a matrix in Numpy. Let us see how it can be done by hand.\nLet’s look at an example:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\ 4 & 5 & 6\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n9 & 8 \\\\ 7 & 6 \\\\ 5 & 4\n\\end{bmatrix}\n\\]\nNote that the first matrix is \\(2\\times 3\\), and the second matrix is \\(3\\times 2\\). The important thing here is that the first matrix has two rows, and the second matrix has two columns. To perform the multiplication, we first take the dot product of the first row of the first matrix (1,2,3) and the first column of the second matrix (9,7,5):\n\\[\n(1,2,3)\n\\cdot\n(9,7,5) =\n(1 \\times 9) + (2 \\times 7) + (3 \\times 5) = 38\n\\]\nIn our resulting matrix (which will always have the same number of rows as the first matrix, and the same number of columns as the second matrix), we can enter this into the first row and first column element:\n\\[\n\\begin{bmatrix}\n38 & ?\\\\? & ?\n\\end{bmatrix}\n\\]\nNow we can take the dot product of the first row of the first matrix and the second column of the second matrix:\n\\[\n(1,2,3) \\cdot (8,6,4) = (1 \\times 8) + (2 \\times 6) + (3 \\times 4) = 32\n\\]\nLet’s add that to our resulting matrix in the first row and second column element:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\? & ?\n\\end{bmatrix}\n\\]\nNow we can repeat this process for the second row of the first matrix and the first column of the second matrix:\n\\[\n(4,5,6) \\cdot (9,7,5) = (4 \\times 9) + (5 \\times 7) + (6 \\times 5) = 101\n\\]\nWhich fills in the next element in the result:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\101 & ?\n\\end{bmatrix}\n\\]\nFinally, we get the dot product for the second row of the first matrix and the second column of the second matrix:\n\\[\n(4,5,6) \\cdot (8,6,4) = (4 \\times 8) + (5 \\times 6) + (6 \\times 4) = 86\n\\]\nGiving us:\n\\[\n\\begin{bmatrix}\n38 & 32\\\\101 & 86\n\\end{bmatrix}\n\\]\nIf this is not enough to catch the idea, take a look this explanation\n\n\n\nIn the above example, we saw that the matrix is the collection of vectors. We also know that vectors can be horizontal as well as vertical, or row and column vectors. Now, what if we change in any matrix row vectors into column vectors? This operation is known as transposition. The idea of this operation is to change matrix rows into matrix columns or vice versa, and is denoted by the superscript \\(T\\).\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}_{n \\times m}\n\\]\nthen\n\\[\nA^{T} =\n\\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{n1} \\\\\na_{12} & a_{22} & \\cdots & a_{n2} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{1m} & a_{2m} & \\cdots & a_{nm}\n\\end{bmatrix}_{m \\times n}\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"Matix A is:\", A, sep=\"\\n\")\n\nprint(\"Transpose of A is:\", A.T, sep=\"\\n\")\n\nMatix A is:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nTranspose of A is:\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n\n\n\n\n\nThere are several different types of matrices. In this post, we will introduce only identity matix. Future post will introduce other types of matrices. An identity matrix (usually indicated by a capital \\(I\\)) is the equivalent in matrix terms of the number 1. It is always square matrix, and it has the value 1 in the diagonal element positions I1,1, I2,2, etc; and 0 in all other element positions. Here’s an example of a \\(3 \\times 3\\) identity matrix:\n\\[\nI =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}_{3 \\times 3}\n\\]\nMultiplying any matrix by an identity matrix is the same as multiplying a number by 1; the result is the same as the original value.\n\n# We have two ways to define identity matrix in Numpy.\n# First is to define by hand, like above examples, and second is to use Numpy's buildin function\n\nI_1 = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n\nI_2 = np.identity(3)\n\nprint(I_1)\n\nprint(I_2)\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n# Matrix identity multiplication\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nI = np.identity(3)\n\nprint(\"A = \", A, sep=\"\\n\")\n\nprint(\"A * I = \", np.dot(A, I), sep=\"\\n\")\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nA * I = \n[[1. 2. 3.]\n [4. 5. 6.]\n [7. 8. 9.]]"
  },
  {
    "objectID": "posts/general/information_theory.html",
    "href": "posts/general/information_theory.html",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "Information flows around us. It’s everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams. For computers, all of them are represented by only two digits 0 and 1, and they carry some information. “Information theory studies the transmission, processing, extraction, and utilization of information.”wikipedia In simple words, with information theory, given different kinds of signals, we try to measure how much information is presented in each of those signals. The theory itself originates from the original work of Claude Shannon named A Mathematical Theory of Communication\nIt will be helpful to see how machine learning and information theory are related. According to “Dive Into Deep Learning” hence d2l considers this relationship to be\n\nMachine learning aims to extract interesting signals from data and make critical predictions. On the other hand, information theory studies encoding, decoding, transmitting, and manipulating information. As a result, information theory provides a fundamental language for discussing the information processing in machine learned systems.source\n\nInformation theory is tightly connected to mathematics and statistics. We will see later on how, but before that, it’s worth to say where is used the concepts of information theory in statistics and mathematics. We all know or have heard about random variables that are drawn from some probability distribution. From linear algebra, we also know how to measure the distance between two points, or between two planes. But, how can we measure the distance between two probability distribution? In other words, how similar or dissimilar are these two probability distribution? Information theory gives us the ability to answer this question and quantify the similarity measure between two distributions. Before we continue, let me outline the measurement unit of information theory. Shannon introduced the bit as the unit of information. The series of 0 and 1 encode any data. Accordingly, the sequence of binary digits of length \\(n\\) contains \\(n\\) bits of information. That has been said, we can review concepts of information theory.\nThere are a few main concepts in information theory, and I will go through each of them in a detailed manner. First in line is:\n\n\nTo understand this concept well, I will review two examples—one from statistics and probability and the second from the information theory. Let start with statistics and probability. Imagine we conduct an experiment giving several outcomes with a different probability. For example, rolling the fair dice with uniform probability \\(\\frac{1}{6}\\) of returning numbers from 1 to 6. Now, consider three outcomes, defined as \\(A=\\{outcome \\leq 6\\}\\) \\(B=\\{outcome is odd\\}\\), and \\(C=\\{outcome=1\\}\\) over probability space \\(\\Omega\\), which in turn contains all the outcomes. Self-information, sometimes stated as information content or surprisal indicates how much unlikely the event \\(A\\), or \\(B\\), or \\(C\\) is, how much surprised we are by observing either event. Here is the question: How can we convert probability \\(p\\) of an event into a number of bits? Claude Shannon gave us the formula for that:\n\\[\nI(X) = - \\log_2(p)\n\\]\nFor our three events, \\(A\\), \\(B\\), and \\(C\\) the self-information or surprisal is the following:\n\\[\nI(A) = - \\log_2(1) = 0\n\\]\n\\[\nI(B) = - \\log_2(\\frac{3}{6}) = 1\n\\]\n\\[\nI(C) = - \\log_2(\\frac{1}{6}) = 2.58\n\\]\nFrom an information theory perspective, if we have a series of binary digits of the length \\(n\\), then the probability of getting 0 or 1 is \\(\\frac{1}{2^{n}}\\). According to Shannon, self-information is the bits of information we receive from observing the event \\(X\\). Let \\(X\\) be the following code: 0101, then its information content is 4 bits according to our formula:\n\\[\nI(X) = I(0101) = - \\log_2(\\frac{1}{2^{4}}) = 4\n\\]\n\nimport numpy as np\n\n\ndef self_information(p):\n    return -np.log2(p)\n\n\nself_information(1 / 2 ** 4)\n\nnp.float64(4.0)\n\n\nThe main takeaway here is that if a particular event has 100% probability, its self-information is \\(-\\log_2(1) = 0\\), meaning that it does not carry any information, and we have no surprise at all. Whereas, if the probability would be close to zero, or we can effectively say it’s zero, then self-information is \\(-\\log_2(0) = \\infty\\). This implies that the rare events have high surprisal or high information content.\nWe see that information content only measures the information of a single event. To generalize this notion for any discrete and/or continues event, we will get the idea of Entropy.\n\n\n\nIf we have any random variable \\(X\\), whether it will be a discrete or continuous and \\(X\\) follows a probability distribution \\(P\\) with p.d.f if it’s continuous or p.m.f if it’s discrete. Can we calculate the average value of \\(X\\)? Yes, we can. From statistics, the formula of the average or a.k.a expectation is\n\\[\n\\mathbb E(X) = \\sum_{i=1}^{k} x_{i} \\cdot p_{i}\n\\]\nWhere \\(x_{i}\\) is one particular event with its probability \\(p_{i}\\). The same is in information theory. The Entropy of a random variable \\(X\\) is the expectation of its self-information, given by:\n\\[\nH(X) = - \\sum_{i} p_{i} \\log_{2} p_{i}\n\\]\nIn Python it looks the following:\n\n# np.nansum return the sum of NaNs. Treats them as zeros.\n\ndef entropy(p):\n    out = np.nansum(-p * np.log2(p))\n    return out\n\n\nentropy(np.array([0.1, 0.5, 0.1, 0.3]))\n\nnp.float64(1.6854752972273346)\n\n\nHere, we only consider one random variable, \\(X\\), and its expected surprisal. What if we have two random variables \\(X\\) and \\(Y\\)? How can we measure their joint information content? In other words, we are interested what information is included in \\(X\\) and \\(Y\\) compared to each separately. Here comes the Joint Entropy\n\n\n\nTo review this concept let me introduce two random variables \\(X\\) and \\(Y\\) and they follow the probability distribution denoted by \\(p_{X}(x)\\) and \\(p_Y(y)\\), respectively. \\((X, Y)\\) has joint probability \\(p_{X, Y}(x, y)\\). The Joint Entropy hence is defined as:\n\\[\nH(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log_{2} p_{X, Y}(x, y)\n\\]\nHere are two important facts. If \\(X = Y\\) this implies that \\(H(X,Y) = H(X) = H(Y)\\) and if \\(X\\) and \\(Y\\) are independent, then \\(H(X, Y) = H(X) + H(Y)\\).\n\ndef joint_entropy(p_xy):\n    out = np.nansum(-p_xy * np.log2(p_xy))\n    return out\n\n\njoint_entropy(np.array([[0.1, 0.5, 0.8], [0.1, 0.3, 0.02]]))\n\nnp.float64(2.0558948969327187)\n\n\nAs we see, joint entropy indicates the amount of information in the pair of two random variables. What if we are interested to know how much information is contained, say in \\(Y\\) but not in \\(X\\)?\n\n\n\nThe conditional entropy is used to measure the relationship between variables. The following formula gives this measurement:\n\\[\nH(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log_{2} p(y \\mid x)\n\\]\nLet investigate how conditional entropy is related to entropy and joint entropy. Using the above formula, we can conclude that:\n\\[\nH(Y \\mid X) = H(X, Y) - H(X)\n\\]\nmeaning that the information contained in \\(Y\\) given \\(X\\) equals information jointly contained in \\(X\\) and \\(Y\\) minus the amount of information only contained in \\(X\\).\n\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy / p_x\n    out = np.nansum(-p_xy * np.log2(p_y_given_x))\n    return out\n\n\nconditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n\nnp.float64(0.8635472023399721)\n\n\nKnowing conditional entropy means knowing the amount of information contained in \\(Y\\) but not in \\(X\\). Now let see how much information is shared between \\(X\\) and \\(Y\\).\n\n\n\nTo find the mutual information between two random variables \\(X\\) and \\(Y\\), let start the process by finding all the information in both \\(X\\) and \\(Y\\) together and then subtract the part which is not shared. The information both in \\(X\\) and \\(Y\\) is \\(H(X, Y)\\). Subtracting two conditional entropies gives:\n\\[\nI(X, Y) = H(X, Y) - H(Y \\mid X) − H(X \\mid Y)\n\\]\nThis means that we have to subtract the information only contained in \\(X\\) and \\(Y\\) to all the information at hand. This relationship is perfectly described by this picture.\n\nThe concept of mutual information likewise correlation coefficient, allow us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them.\n\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    out = np.nansum(p_xy * np.log2(p))\n    return out\n\n\nmutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n                np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n\nnp.float64(0.7194602975157967)\n\n\nAs in the case of the correlation coefficient, mutual information has some notable properties:\n\nMutual information is symmetric\nMutual information is non-negative\n\\(I(X, Y) = 0\\) iff \\(X\\) and \\(Y\\) are independent\n\nWe can interpret the mutual information \\(I(X, Y)\\) as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent.\n\n\n\nI asked the question about measuring the distance between two probability distributions. The time has come to answer this question precisely. If we have random variable \\(X\\) which follows probability distributin \\(P\\) and has p.d.f or p.m.f \\(p(x)\\). Imagine we estimated \\(P\\) with other probability distribution \\(Q\\), which in turn has p.d.f or p.m.f \\(q(x)\\). The distance between thse two probability distribution is measured by Kullback–Leibler (KL) Divergence:\n\\[\nD_{\\mathrm{KL}}(P\\|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n\\]\nThe lower value of the \\(KL\\) divergence, the closer our estimate is to the actual distribution.\n\nThe KL divergence is non-symmetric or equivalently, \\(D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P), \\text{ if } P \\neq Q\\)\nThe KL divergence is non-negative or equivalently, \\(D_{\\mathrm{KL}}(P\\|Q) \\geq 0\\)\n\n\ndef kl_divergence(p, q):\n    # Replace non-positive values with a small positive number (e.g., 1e-7)\n    # to avoid `invalid value encountered in log` warning.\n    p = np.where(p &gt; 0, p, 1e-7)\n    q = np.where(q &gt; 0, q, 1e-7)\n    kl = p * np.log2(p / q)\n    out = np.nansum(kl)\n    return np.abs(out)\n\n\np = np.random.normal(1, 2, size=1000)\nq = np.random.normal(1, 2, size=1000)\n\nkl_divergence(p, q)\n\nnp.float64(11527.660129655596)\n\n\n\n\n\nTo understand Cross-Entropy, let me use the example from the KL divergence part. Now, imagine we perform classification tasks, where \\(y\\) is the true label, and \\(\\hat{y}\\) is estimated label by our model. Cross-Entropy denoted by \\(\\mathrm{CE}(y, \\hat{y})\\) is used as a objective function in many classification tasks in deep learning. The formula is the following:\n\\[\n\\mathrm{CE} (P, Q) = H(P) + D_{\\mathrm{KL}}(P\\|Q)\n\\]\nThe two terms on the right-hand side are self-information and KL divergence. \\(P\\) is the distribution of the true labels, and \\(Q\\) is the distribution of the estimated labels. As we are only interested in knowing how far we are from the actual label and \\(H(P)\\) is also given, the above formula is reduced to minimize only the second term (KL divergence) at the right-hand side. Hence, we have\n\\[\n\\mathrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log_{2}{p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)}\n\\]\n\ndef cross_entropy(y_hat, y):\n    ce = -np.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n\n\nlabels = np.array([0, 2])\npreds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n\nnp.float64(0.9485599924429406)"
  },
  {
    "objectID": "posts/general/information_theory.html#self-information",
    "href": "posts/general/information_theory.html#self-information",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "To understand this concept well, I will review two examples—one from statistics and probability and the second from the information theory. Let start with statistics and probability. Imagine we conduct an experiment giving several outcomes with a different probability. For example, rolling the fair dice with uniform probability \\(\\frac{1}{6}\\) of returning numbers from 1 to 6. Now, consider three outcomes, defined as \\(A=\\{outcome \\leq 6\\}\\) \\(B=\\{outcome is odd\\}\\), and \\(C=\\{outcome=1\\}\\) over probability space \\(\\Omega\\), which in turn contains all the outcomes. Self-information, sometimes stated as information content or surprisal indicates how much unlikely the event \\(A\\), or \\(B\\), or \\(C\\) is, how much surprised we are by observing either event. Here is the question: How can we convert probability \\(p\\) of an event into a number of bits? Claude Shannon gave us the formula for that:\n\\[\nI(X) = - \\log_2(p)\n\\]\nFor our three events, \\(A\\), \\(B\\), and \\(C\\) the self-information or surprisal is the following:\n\\[\nI(A) = - \\log_2(1) = 0\n\\]\n\\[\nI(B) = - \\log_2(\\frac{3}{6}) = 1\n\\]\n\\[\nI(C) = - \\log_2(\\frac{1}{6}) = 2.58\n\\]\nFrom an information theory perspective, if we have a series of binary digits of the length \\(n\\), then the probability of getting 0 or 1 is \\(\\frac{1}{2^{n}}\\). According to Shannon, self-information is the bits of information we receive from observing the event \\(X\\). Let \\(X\\) be the following code: 0101, then its information content is 4 bits according to our formula:\n\\[\nI(X) = I(0101) = - \\log_2(\\frac{1}{2^{4}}) = 4\n\\]\n\nimport numpy as np\n\n\ndef self_information(p):\n    return -np.log2(p)\n\n\nself_information(1 / 2 ** 4)\n\nnp.float64(4.0)\n\n\nThe main takeaway here is that if a particular event has 100% probability, its self-information is \\(-\\log_2(1) = 0\\), meaning that it does not carry any information, and we have no surprise at all. Whereas, if the probability would be close to zero, or we can effectively say it’s zero, then self-information is \\(-\\log_2(0) = \\infty\\). This implies that the rare events have high surprisal or high information content.\nWe see that information content only measures the information of a single event. To generalize this notion for any discrete and/or continues event, we will get the idea of Entropy."
  },
  {
    "objectID": "posts/general/information_theory.html#entropy",
    "href": "posts/general/information_theory.html#entropy",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "If we have any random variable \\(X\\), whether it will be a discrete or continuous and \\(X\\) follows a probability distribution \\(P\\) with p.d.f if it’s continuous or p.m.f if it’s discrete. Can we calculate the average value of \\(X\\)? Yes, we can. From statistics, the formula of the average or a.k.a expectation is\n\\[\n\\mathbb E(X) = \\sum_{i=1}^{k} x_{i} \\cdot p_{i}\n\\]\nWhere \\(x_{i}\\) is one particular event with its probability \\(p_{i}\\). The same is in information theory. The Entropy of a random variable \\(X\\) is the expectation of its self-information, given by:\n\\[\nH(X) = - \\sum_{i} p_{i} \\log_{2} p_{i}\n\\]\nIn Python it looks the following:\n\n# np.nansum return the sum of NaNs. Treats them as zeros.\n\ndef entropy(p):\n    out = np.nansum(-p * np.log2(p))\n    return out\n\n\nentropy(np.array([0.1, 0.5, 0.1, 0.3]))\n\nnp.float64(1.6854752972273346)\n\n\nHere, we only consider one random variable, \\(X\\), and its expected surprisal. What if we have two random variables \\(X\\) and \\(Y\\)? How can we measure their joint information content? In other words, we are interested what information is included in \\(X\\) and \\(Y\\) compared to each separately. Here comes the Joint Entropy"
  },
  {
    "objectID": "posts/general/information_theory.html#joint-entropy",
    "href": "posts/general/information_theory.html#joint-entropy",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "To review this concept let me introduce two random variables \\(X\\) and \\(Y\\) and they follow the probability distribution denoted by \\(p_{X}(x)\\) and \\(p_Y(y)\\), respectively. \\((X, Y)\\) has joint probability \\(p_{X, Y}(x, y)\\). The Joint Entropy hence is defined as:\n\\[\nH(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log_{2} p_{X, Y}(x, y)\n\\]\nHere are two important facts. If \\(X = Y\\) this implies that \\(H(X,Y) = H(X) = H(Y)\\) and if \\(X\\) and \\(Y\\) are independent, then \\(H(X, Y) = H(X) + H(Y)\\).\n\ndef joint_entropy(p_xy):\n    out = np.nansum(-p_xy * np.log2(p_xy))\n    return out\n\n\njoint_entropy(np.array([[0.1, 0.5, 0.8], [0.1, 0.3, 0.02]]))\n\nnp.float64(2.0558948969327187)\n\n\nAs we see, joint entropy indicates the amount of information in the pair of two random variables. What if we are interested to know how much information is contained, say in \\(Y\\) but not in \\(X\\)?"
  },
  {
    "objectID": "posts/general/information_theory.html#conditional-entropy",
    "href": "posts/general/information_theory.html#conditional-entropy",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "The conditional entropy is used to measure the relationship between variables. The following formula gives this measurement:\n\\[\nH(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log_{2} p(y \\mid x)\n\\]\nLet investigate how conditional entropy is related to entropy and joint entropy. Using the above formula, we can conclude that:\n\\[\nH(Y \\mid X) = H(X, Y) - H(X)\n\\]\nmeaning that the information contained in \\(Y\\) given \\(X\\) equals information jointly contained in \\(X\\) and \\(Y\\) minus the amount of information only contained in \\(X\\).\n\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy / p_x\n    out = np.nansum(-p_xy * np.log2(p_y_given_x))\n    return out\n\n\nconditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n\nnp.float64(0.8635472023399721)\n\n\nKnowing conditional entropy means knowing the amount of information contained in \\(Y\\) but not in \\(X\\). Now let see how much information is shared between \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "posts/general/information_theory.html#mutual-information",
    "href": "posts/general/information_theory.html#mutual-information",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "To find the mutual information between two random variables \\(X\\) and \\(Y\\), let start the process by finding all the information in both \\(X\\) and \\(Y\\) together and then subtract the part which is not shared. The information both in \\(X\\) and \\(Y\\) is \\(H(X, Y)\\). Subtracting two conditional entropies gives:\n\\[\nI(X, Y) = H(X, Y) - H(Y \\mid X) − H(X \\mid Y)\n\\]\nThis means that we have to subtract the information only contained in \\(X\\) and \\(Y\\) to all the information at hand. This relationship is perfectly described by this picture.\n\nThe concept of mutual information likewise correlation coefficient, allow us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them.\n\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    out = np.nansum(p_xy * np.log2(p))\n    return out\n\n\nmutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n                np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n\nnp.float64(0.7194602975157967)\n\n\nAs in the case of the correlation coefficient, mutual information has some notable properties:\n\nMutual information is symmetric\nMutual information is non-negative\n\\(I(X, Y) = 0\\) iff \\(X\\) and \\(Y\\) are independent\n\nWe can interpret the mutual information \\(I(X, Y)\\) as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent."
  },
  {
    "objectID": "posts/general/information_theory.html#kullbackleibler-divergence---relative-entropy",
    "href": "posts/general/information_theory.html#kullbackleibler-divergence---relative-entropy",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "I asked the question about measuring the distance between two probability distributions. The time has come to answer this question precisely. If we have random variable \\(X\\) which follows probability distributin \\(P\\) and has p.d.f or p.m.f \\(p(x)\\). Imagine we estimated \\(P\\) with other probability distribution \\(Q\\), which in turn has p.d.f or p.m.f \\(q(x)\\). The distance between thse two probability distribution is measured by Kullback–Leibler (KL) Divergence:\n\\[\nD_{\\mathrm{KL}}(P\\|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n\\]\nThe lower value of the \\(KL\\) divergence, the closer our estimate is to the actual distribution.\n\nThe KL divergence is non-symmetric or equivalently, \\(D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P), \\text{ if } P \\neq Q\\)\nThe KL divergence is non-negative or equivalently, \\(D_{\\mathrm{KL}}(P\\|Q) \\geq 0\\)\n\n\ndef kl_divergence(p, q):\n    # Replace non-positive values with a small positive number (e.g., 1e-7)\n    # to avoid `invalid value encountered in log` warning.\n    p = np.where(p &gt; 0, p, 1e-7)\n    q = np.where(q &gt; 0, q, 1e-7)\n    kl = p * np.log2(p / q)\n    out = np.nansum(kl)\n    return np.abs(out)\n\n\np = np.random.normal(1, 2, size=1000)\nq = np.random.normal(1, 2, size=1000)\n\nkl_divergence(p, q)\n\nnp.float64(11527.660129655596)"
  },
  {
    "objectID": "posts/general/information_theory.html#cross-entropy",
    "href": "posts/general/information_theory.html#cross-entropy",
    "title": "Basics of Information Theory with Python",
    "section": "",
    "text": "To understand Cross-Entropy, let me use the example from the KL divergence part. Now, imagine we perform classification tasks, where \\(y\\) is the true label, and \\(\\hat{y}\\) is estimated label by our model. Cross-Entropy denoted by \\(\\mathrm{CE}(y, \\hat{y})\\) is used as a objective function in many classification tasks in deep learning. The formula is the following:\n\\[\n\\mathrm{CE} (P, Q) = H(P) + D_{\\mathrm{KL}}(P\\|Q)\n\\]\nThe two terms on the right-hand side are self-information and KL divergence. \\(P\\) is the distribution of the true labels, and \\(Q\\) is the distribution of the estimated labels. As we are only interested in knowing how far we are from the actual label and \\(H(P)\\) is also given, the above formula is reduced to minimize only the second term (KL divergence) at the right-hand side. Hence, we have\n\\[\n\\mathrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log_{2}{p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)}\n\\]\n\ndef cross_entropy(y_hat, y):\n    ce = -np.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n\n\nlabels = np.array([0, 2])\npreds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n\nnp.float64(0.9485599924429406)"
  },
  {
    "objectID": "posts/mathematics/taylor_series_expansion.html",
    "href": "posts/mathematics/taylor_series_expansion.html",
    "title": "Taylor Series Expansion with Python",
    "section": "",
    "text": "In this blog, I want to review famous Taylor Series Expansion and its special case Maclaurin Series Expansion. According to wikipedia, the aim of Taylor Series Expansion (TSE) is to represent a function as an infinite sum of terms that are derived from the values of that function’s derivatives, which in turn are evaluated at some predefined single point. In other words, by using TSE, we try to represent some given function as an infinite sum of its derivatives and these derivatives are evaluated at some single point which we can choose. Before diving into mechanics of TSE and its special case Maclaurin Series Expansion (MSE), it’s worth to know some history behind these guys. Back in the 17th century the concept of expansion first was introduced by mathematician James Gregory, but in 1715 the notion of function expansion was formally introduced by Brook Taylor.\nA one-dimensional Taylor series is an expansion of a real function \\(\\mathbb F(x)\\) about a point \\(x=a\\) is given by:\n\\[\n\\mathbb F(x) \\approx\n\\mathbb F(a) + \\mathbb F^{'}(a)\\cdot(x - a) + \\frac{1}{2!}\\cdot\\mathbb F^{''}(a)\\cdot(x - a)^{2} +\n\\frac{1}{3!}\\cdot\\mathbb F^{3}(a)\\cdot(x - a)^{3} + \\cdots + \\frac{1}{n!}\\cdot\\mathbb F^{n}(a)\\cdot(x - a)^{n}\n\\]\nwhere, \\(n!\\) denotes the factorial of \\(n\\) and \\(\\mathbb F^{n}(a)\\) denotes nth derivative of \\(\\mathbb F\\) evaluated at point \\(a\\). Every term on the right hand side denotes the order of Taylor expansion. For instance, \\(\\mathbb F(a)\\) is zeroth-order expansion and \\(\\mathbb F^{'}(a)\\cdot(x - a)\\) is the first-order expansion. The above representation is called open-form representation of an expansion.\nWe can write this expansion in more compact notation in the following way:\n\\[\n\\sum_{n = 0}^{\\infty} =\n\\frac{\\mathbb F^{n}(a)}{n!}\\cdot(x - a)^n\n\\]\nThis is the closed-form representation of an expansion.\nTo see the intuition, let review some example. I’m interested what is Taylor expansion of order 3 of \\(cos(x)\\) at \\(x = a\\). To follow the above definition we have:\n\\[\n\\mathbb F(x) =\ncos(x)\n\\]\n\\[\ncos(x) \\approx\ncos(a) - sin(a)\\cdot(x - a) - \\frac{1}{2}\\cdot cos(a)\\cdot (x - a)^2 + \\frac{1}{6}\\cdot sin(a)\\cdot(x - a)^3\n\\]\nYou ask, what is \\(a\\) and how can we choose it? \\(a\\) is the point where we want to have \\(cosine\\) approximation and it can be any number from \\(-\\infty\\) to \\(+\\infty\\). Note that, this is not the case for other functions. We are restricted to choose \\(a\\) from domain of a given function.\nNow, let do Taylor approximation for \\(sin(x)\\) at \\(x = a\\)\n\\[\n\\mathbb F(x) =\nsin(x)\n\\]\n\\[\nsin(x) \\approx\nsin(a) + cos(a)\\cdot(x - a) + \\frac{1}{2}\\cdot sin(a)\\cdot (x - a)^2 - \\frac{1}{6}\\cdot cos(a)\\cdot(x - a)^3\n\\]\nWe can go further and do Taylor series expansion for exponent \\(e^{x}\\) at \\(x = a\\) is\n\\[\n\\mathbb F(x) =\ne^{x}\n\\]\n\\[\ne^{x} \\approx\ne^{a} + e^{a}\\cdot(x - a) + \\frac{1}{2}\\cdot e^{a}\\cdot (x - a)^2 + \\frac{1}{6}\\cdot e^{a}\\cdot(x - a)^3\n\\]\nAs we have three functions approximations, let choose the value for \\(a\\) and set it to zero and see what we will have.\nFor \\(cos(x)\\) where \\(x = a = 0\\) we have:\n\\[\ncos(x) =\n1 - \\frac{1}{2}x^2\n\\]\nfor \\(sin(x)\\) at \\(x = a = 0\\) we have:\n\\[\nsin(x) =\nx - \\frac{1}{6}x^3\n\\]\nFor \\(e^{x}\\) where \\(x = a = 0\\) we have:\n\\[\ne^{x} =\n1 + x + \\frac{1}{2}x^2 + \\frac{1}{6}x^3\n\\]\nThis kind of expansion is known as Maclaurin series expansion, in other words when approximation point is zero we call it Maclaurin expansion.\nCalculating third order approximation for these functions by hand does not seem too hard, but for higher order it’s tedious. To solve this problem we can use Python, namely Sympy if we want to have a symbolic approximation, or Numpy/Scipy to have a numeric approximation. Not to be confused with numeric approximation and approximation point. At \\(a = 0\\) for function \\(\\mathbb F(x) = e^{x}\\) we had Taylor approximation \\(e^{x} = 1 + x + \\frac{1}{2}x^2 + \\frac{1}{6}x^3\\). If we evaluate this expression at, say \\(x = 1\\) we have function output. In this setup,\n\\[\n\\mathbb F(x) = e^{x} \\approx 1 + x + \\frac{1}{2}x^2 + \\frac{1}{6}x^3\\mid_{x=1}\n\\]\nand evaluated at \\(x = 1\\) we have \\(\\mathbb F(1) = e^{1} = 2.71828182 \\approx 2.66666666\\), which is close to the real output.\nNow let visualize these functions and their Taylor approximations at different points with a different order of expansion. Before visualizing results it’s good to have a function which will do symbolic Taylor expansion for higher orders for one variable functions. For multi-variable functions, it’s up to you.\n\n\n\nfrom sympy import series, Symbol\nfrom sympy.functions import sin, cos, exp\nfrom sympy.plotting import plot\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"ggplot\")\n\n\n# Define symbol\nx = Symbol(\"x\")\n\n\ndef taylor(function, x0, n):\n    \"\"\"\n    Do Taylor Series Expansion for a given real valued function.\n\n    Args:\n        function: function to approximate\n        x0: point where to approximate\n        n: order of approximation\n\n    Returns:\n        A list of the Taylor series expansion of the function\n    \"\"\"\n    return function.series(x, x0, n).removeO()\n\n\ndef plot_expansion(function, expansions, points, title):\n    \"\"\"\n    Plots a function and its Taylor Series Expansion\n\n    Args:\n        function: Original function\n        expansions: List of Taylor Series Expansion for that function\n        points: List of points to plot\n        title: Title of the plot\n    \"\"\"\n    p = plot(function, *expansions, points, legend=True, show=False)\n    p[0].line_color = \"blue\"\n    p[1].line_color = \"green\"\n    p[2].line_color = \"firebrick\"\n    p[3].line_color = \"black\"\n    p.title = title\n    p.show()\n\nWhile defining “taylor” function, in return statement I used “.removeO()” method. This method is used in series expansion to remove \\(\\mathit O(x^{n})\\) term, which is Landau order term at \\(x = 0\\) and not to be confused with big \\(\\mathit O\\) notation used in computer science, which generally represents the Landau order term at \\(x = \\infty\\).\nWe can do \\(sin(x)\\), \\(cos(x)\\), and \\(e(x)\\) expansion by using Sympy.\n\nprint(\"sin(x) =\", taylor(sin(x), 0, 4))\n\nprint(\"cos(x) =\", taylor(cos(x), 0, 4))\n\nprint(\"e(x) =\", taylor(exp(x), 0, 4))\n\nsin(x) = -x**3/6 + x\ncos(x) = 1 - x**2/2\ne(x) = x**3/6 + x**2/2 + x + 1\n\n\nThat’s not all. We can evaluate these functions at any point. For instance as we did above for \\(x = 1\\)\n\nprint(\"sin(1) =\", taylor(sin(x), 0, 4).subs(x, 1))\n\nprint(\"cos(1) =\", taylor(cos(x), 0, 4).subs(x, 1))\n\nprint(\"e(1) =\", taylor(exp(x), 0, 4).subs(x, 1))\n\nsin(1) = 5/6\ncos(1) = 1/2\ne(1) = 8/3\n\n\nAs we have all the necessary tools to visualize the results, let do it.\n\n\n\n# This will plot sine and its Taylor approximations\n\nplot_expansion(\n    sin(x),\n    [taylor(sin(x), 0, 1), taylor(sin(x), 0, 3), taylor(sin(x), 0, 5)],\n    (x, -3.5, 3.5),\n    \"Taylor Series Expansion for Sine\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# This will plot cosine and its Taylor approximations\n\nplot_expansion(\n    cos(x),\n    [taylor(cos(x), 0, 2), taylor(cos(x), 0, 4), taylor(cos(x), 0, 6)],\n    (x, -4.5, 4.5),\n    \"Taylor Series Expansion for Cosine\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# This will plot exponent and its Taylor approximations\n\nplot_expansion(\n    exp(x),\n    [taylor(exp(x), 0, 1), taylor(exp(x), 0, 2), taylor(exp(x), 0, 3)],\n    (x, -2, 2),\n    \"Taylor Series Expansion for Exponent\",\n)"
  },
  {
    "objectID": "posts/mathematics/taylor_series_expansion.html#practical-examples",
    "href": "posts/mathematics/taylor_series_expansion.html#practical-examples",
    "title": "Taylor Series Expansion with Python",
    "section": "",
    "text": "from sympy import series, Symbol\nfrom sympy.functions import sin, cos, exp\nfrom sympy.plotting import plot\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"ggplot\")\n\n\n# Define symbol\nx = Symbol(\"x\")\n\n\ndef taylor(function, x0, n):\n    \"\"\"\n    Do Taylor Series Expansion for a given real valued function.\n\n    Args:\n        function: function to approximate\n        x0: point where to approximate\n        n: order of approximation\n\n    Returns:\n        A list of the Taylor series expansion of the function\n    \"\"\"\n    return function.series(x, x0, n).removeO()\n\n\ndef plot_expansion(function, expansions, points, title):\n    \"\"\"\n    Plots a function and its Taylor Series Expansion\n\n    Args:\n        function: Original function\n        expansions: List of Taylor Series Expansion for that function\n        points: List of points to plot\n        title: Title of the plot\n    \"\"\"\n    p = plot(function, *expansions, points, legend=True, show=False)\n    p[0].line_color = \"blue\"\n    p[1].line_color = \"green\"\n    p[2].line_color = \"firebrick\"\n    p[3].line_color = \"black\"\n    p.title = title\n    p.show()\n\nWhile defining “taylor” function, in return statement I used “.removeO()” method. This method is used in series expansion to remove \\(\\mathit O(x^{n})\\) term, which is Landau order term at \\(x = 0\\) and not to be confused with big \\(\\mathit O\\) notation used in computer science, which generally represents the Landau order term at \\(x = \\infty\\).\nWe can do \\(sin(x)\\), \\(cos(x)\\), and \\(e(x)\\) expansion by using Sympy.\n\nprint(\"sin(x) =\", taylor(sin(x), 0, 4))\n\nprint(\"cos(x) =\", taylor(cos(x), 0, 4))\n\nprint(\"e(x) =\", taylor(exp(x), 0, 4))\n\nsin(x) = -x**3/6 + x\ncos(x) = 1 - x**2/2\ne(x) = x**3/6 + x**2/2 + x + 1\n\n\nThat’s not all. We can evaluate these functions at any point. For instance as we did above for \\(x = 1\\)\n\nprint(\"sin(1) =\", taylor(sin(x), 0, 4).subs(x, 1))\n\nprint(\"cos(1) =\", taylor(cos(x), 0, 4).subs(x, 1))\n\nprint(\"e(1) =\", taylor(exp(x), 0, 4).subs(x, 1))\n\nsin(1) = 5/6\ncos(1) = 1/2\ne(1) = 8/3\n\n\nAs we have all the necessary tools to visualize the results, let do it.\n\n\n\n# This will plot sine and its Taylor approximations\n\nplot_expansion(\n    sin(x),\n    [taylor(sin(x), 0, 1), taylor(sin(x), 0, 3), taylor(sin(x), 0, 5)],\n    (x, -3.5, 3.5),\n    \"Taylor Series Expansion for Sine\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# This will plot cosine and its Taylor approximations\n\nplot_expansion(\n    cos(x),\n    [taylor(cos(x), 0, 2), taylor(cos(x), 0, 4), taylor(cos(x), 0, 6)],\n    (x, -4.5, 4.5),\n    \"Taylor Series Expansion for Cosine\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# This will plot exponent and its Taylor approximations\n\nplot_expansion(\n    exp(x),\n    [taylor(exp(x), 0, 1), taylor(exp(x), 0, 2), taylor(exp(x), 0, 3)],\n    (x, -2, 2),\n    \"Taylor Series Expansion for Exponent\",\n)"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_3.html",
    "href": "posts/mathematics/linear_algebra_3.html",
    "title": "Intermediate Linear Algebra with Python - Part I",
    "section": "",
    "text": "Introduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn this post I will introduce you to the notion of vector, its characteristics, and operations defined on vectors. I will also show you how to use Python to do these operations.\n\n\n\n\nIn the case of the dot product between two vectors, we saw that the result is a scalar. In the case of a cross product the result is a vector, so the cross product is also called the vector product. The resulted vector is a vector that is at right angles to both the other vectors in 3D Euclidean space. This means that the cross-product only really makes sense when working with vectors that contain three components.\nThere are two formulas to calculate the cross product. One is algebraic and second is geometric. More precisely, the first formula catches the algebraic intuition of cross product and second catches the geometric intuition of the cross product.\nIf we have two vectors \\(A\\) and \\(B\\) in such a way:\n\\[\nA =\n\\begin{bmatrix}\n    a_{1} \\\\\n    a_{2} \\\\\n    a_{3}\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    b_{1} \\\\\n    b_{2} \\\\\n    b_{3}\n\\end{bmatrix}\n\\]\nThe algebraic formula is:\n\\[\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(a_{2} \\cdot b_{3}) - (a_{3} \\cdot b_{2}) \\\\\n(a_{3} \\cdot b_{1}) - (a_{1} \\cdot b_{3}) \\\\\n(a_{1} \\cdot b_{2}) - (a_{2} \\cdot b_{1})\n\\end{bmatrix}\n\\]\nAt the first glance, the formula is not easy to remember. Let try another formula which uses some advanced components.\n\\[\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n\\vec{i} & \\vec{j} & \\vec{k} \\\\\na_{1} & a_{2} & a_{3} \\\\\nb_{1} & b_{2} & b_{3}\n\\end{bmatrix}\n\\]\nwhere \\(\\vec{i}\\), \\(\\vec{j}\\) and \\(\\vec{k}\\) are bases vectors and I will explain in advance part of these series.\nTo find the cross product we have to find the determinant of above matrix in the following way:\n\\[\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\na_{2} & a_{3} \\\\\nb_{2} & b_{3}\n\\end{bmatrix} \\vec{i} \\quad - \\quad\n\\begin{bmatrix}\na_{1} & a_{3} \\\\\nb_{1} & b_{3}\n\\end{bmatrix} \\vec{j} \\quad + \\quad\n\\begin{bmatrix}\na_{1} & a_{2} \\\\\nb_{1} & b_{2}\n\\end{bmatrix} \\vec{k}\n\\]\nSuppose we have two vectors:\n\\[\nA =\n\\begin{bmatrix}\n    2 \\\\\n    3 \\\\\n    1\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    1 \\\\\n    2 \\\\\n    -2\n\\end{bmatrix}\n\\]\nThen cross product is:\n\\[\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(3 \\cdot (-2)) - (1 \\cdot 2) \\\\\n(1 \\cdot 1) - (2 \\cdot (-2)) \\\\\n(2 \\cdot 2) - (3 \\cdot 1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n(-6) - 2 \\\\\n1 - (-4) \\\\\n4 - 3\n\\end{bmatrix} =\n\\begin{bmatrix}\n-8 \\\\\n5 \\\\\n1\n\\end{bmatrix}\n\\]\nThe geometric formula is:\n\\[\n\\vec{C} =\n\\vec{A} \\times \\vec{B} =\n\\|\\vec{A}\\|\\cdot\\|\\vec{B}\\|\\cdot\\sin{\\theta} \\cdot \\hat{n}\n\\]\nwhere, \\(\\theta\\) is the angle between \\(\\vec{A}\\) and \\(\\vec{B}\\). Also, \\(\\hat{n}\\) is a unit vector perpendicular to both \\(\\vec{A}\\) and \\(\\vec{B}\\), such that \\(\\vec{A}\\), \\(\\vec{B}\\) and \\(\\hat{n}\\) form a right-handed system\nThere is also geometric application of the cross product. If we three vectors \\(\\vec{a}\\), \\(\\vec{b}\\) and \\(\\vec{c}\\) which forms the three dimensional figure such as:\n\n\n\nright-handed system\n\n\nThen the area of the parallelogram ( two dimensional front of this object ) is given by:\n\\[\n{\\rm{Area}} = \\left\\| {\\vec{a} \\times \\vec{b}} \\right\\|\n\\]\nThis is the dot product of the result of cross product between vectors \\(\\vec{a}\\) and \\(\\vec{b}\\)\nThe volume of the parallelepiped ( the whole three dimensional object ) is given by:\n\\[\n{\\rm{Volume}} = \\left| {\\vec{a} \\centerdot \\left( {\\vec{b} \\times \\vec{c}} \\right)} \\right|\n\\]\nhere, absolute value bars are necessary since the result could be negative and volume must be positive.\n\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n\nA = np.array([2, 3, 1])\nB = np.array([1, 2, -2])\n\nprint(\"A =\", A)\nprint(\"B = \", B)\n\nprint(\"Cross Product is:\", np.cross(A, B), sep=\"\\n\")\n\nA = [2 3 1]\nB =  [ 1  2 -2]\nCross Product is:\n[-8  5  1]\n\n\n\n\n\nsuppose we have set of vectors:\n\\[\n{\\alpha _1},...,{\\alpha _n} \\in A\n\\]\nthen we can define the space \\(S\\) spanned by:\n\\[\n{\\alpha _1},...,{\\alpha _n}\n\\]\nas\n\\[\nS\\left( {{\\alpha_{1}},...,{\\alpha_{n}}} \\right) = \\left\\{ {\\sum_{i = 1}^{n} {{c_{i}}{\\alpha_{i}}\\;|\\;{c_{i}} \\in \\mathbb{R}} } \\right\\}\n\\]\nwhich is the set of all linear combinations of the vectors in this subspace. The set is a subspace of \\(A\\):\n\\[\nS\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) \\subset A\n\\]\nIn other words, if we have set of vectors\n\\[\nA := \\{a_1, \\ldots, a_k\\}\n\\in\n\\mathbb R^n\n\\]\nit’s natural to think about the new vectors we can create from these vectors by performing linear operations. New vectors created in this way are called linear combinations of \\(A\\). Particularly, \\(X \\in \\mathbb{R}^n\\) is a linear combination of \\(A\\) if\n\\[\nX =\n\\beta_1 a_1 + \\cdots + \\beta_k a_k\n\\text{ for some scalars } \\beta_1, \\ldots, \\beta_k\n\\]\nhere, \\(\\beta_1, \\ldots, \\beta_k\\) are called coefficients of the linear combination.\nThe set of linear combinations of \\(A\\) is the span of \\(A\\) and is written as \\(Span(A)\\)\nFor example, if\n\\[\n\\vec{A} = \\{e_1, e_2, e_3\\}\n\\quad\n\\text{such that}\n\\quad\ne_1 :=\n\\begin{bmatrix}\n     1 \\\\\n     0 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_2 :=\n\\begin{bmatrix}\n     0 \\\\\n     1 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_3 :=\n\\quad\n\\begin{bmatrix}\n     0 \\\\\n     0 \\\\\n     1\n\\end{bmatrix}\n\\]\nthen the span of \\(A\\) is all of \\(\\mathbb{R}^3\\), because, for any \\(X=(x_1, x_2, x_3)\\in \\mathbb{R}^3\\), we can write\n\\[\nX =\nx_1 e_1 + x_2 e_2 + x_3 e_3\n\\]\nThis means that by using \\(A\\) or vectors \\(e_1\\), \\(e_2\\) and \\(e_3\\) we can generate any vector in \\(\\mathbb{R}^3\\) by performing linear operations.\nThis figure below shows the span of \\(A = \\{a_1, a_2\\}\\) in \\(\\mathbb{R}^3\\). The span is a 2 dimensional plane passing through these two points and the origin.\n\n# Linear function to generate a plane\ndef f(x, y):\n    return (0.2 * x) + (0.1 * y)\n\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set(xlim=(-5, 5), ylim=(-5, 5), zlim=(-5, 5), xticks=(0,), yticks=(0,), zticks=(0,))\n\nz = np.linspace(-5, 5, 3)\ny = np.zeros(3)\nx = np.zeros(3)\n\nax.plot(x, y, z, \"k-\", lw=2, alpha=0.5)\nax.plot(z, x, y, \"k-\", lw=2, alpha=0.5)\nax.plot(y, z, x, \"k-\", lw=2, alpha=0.5)\n\n# Set vector coordinates\nx_coords = np.array((3, 3))\ny_coords = np.array((4, -4))\nz = f(x_coords, y_coords)\n\nfor i in (0, 1):\n    ax.text(x_coords[i], y_coords[i], z[i], f\"$a_{i+1}$\", fontsize=14)\n\n# We need to draw lines from origin to the vectors\nfor i in (0, 1):\n    x = (0, x_coords[i])\n    y = (0, y_coords[i])\n    z = (0, f(x_coords[i], y_coords[i]))\n    ax.plot(x, y, z, \"b-\", lw=1.5, alpha=0.6)\n\n\n# As we already draw axes and vectors, it's time to plot the plane\nxr2 = np.linspace(-5, 5, 50)\nyr2 = np.linspace(-5, 5, 50)\n\nx2, y2 = np.meshgrid(xr2, yr2)\nz2 = f(x2, y2)\n\nax.plot_surface(x2, y2, z2, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=True, alpha=0.2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove, I mentioned a linear combination. In order to define linear dependence and independence let farther clarify what is a linear combination. If we have a set of vectors\n\\[\n\\vec{A} = \\{a_1, \\ldots, a_k\\}\n\\]\nwhich all have the same dimension, then\n\nA linear combination of the vectors in \\(A\\) is any vector of the form \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k}\\), where \\(c_{1}, c_{2}, \\ldots, c_{k}\\) are arbitrary scalars\n\nFor example, if \\(A = \\{[1, 2], [2, 1]\\}\\), then\n\\[\n2 a_{1} - a_{2} = 2 ([1, 2]) - [2, 1] = [0, 3]\n\\]\nis linear combination of the vectors in \\(A\\).\n\nA set \\(A\\) of m-dimensional vectors is linearly independent if the only linear combination of vectors in \\(A\\) that equals \\(0\\) is the trivial linear combination\n\nThis formal definition seems a little bit confusing. Let consider the example to catch the idea.\nThe set of vectors\n\\[\nA =\n\\{[1, 0], [0, 1]\\}\n\\]\nis linearly independent. Let prove this claim. We need to find constants \\(c_{1}\\) and \\(c_{2}\\) satisfying\n\\[\nc_{1} ([1, 0]) + c_{2} ([0, 1]) = [0, 0]\n\\]\nsolving this system of equations gives that \\([c_{1}, c_{2}] = [0, 0] \\rightarrow c_{1} = c_{2} = 0\\), in turn this implies that \\(A\\) is linearly independent.\nThe following statements are equivalent for a linear independence of \\(A\\):\n\nNo vector in \\(A\\) can be formed as a linear combination of the other vectors. This means that if we have three vectors in \\(A\\), any of them cannot be expressed as the linear combination of the other two.\nIf \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k} = 0\\), then \\(c_{1} = c_{2} = \\cdots = c_{k} = 0\\)\n\n\nA set \\(A\\) of m-dimensional vectors is linearly dependent if there is a nontrivial linear combination of the vectors in \\(A\\) that adds up to \\(0\\)\n\nThe set of vectors\n\\[\nA =\n\\{[1, 2], [2, 4]\\}\n\\]\nis linearly dependent set of vectors. Let see how.\n\\[\nc_{1} ([1, 2]) + c_{2} ([2, 4]) = [0, 0]\n\\]\nThere is a nontrivial linear combination with \\(c_{1} = 2\\) and \\(c_{2} = 1\\) that yields \\(0\\). This implies \\(A\\) is the linearly dependent set of vectors. It’s easy in this case to spot linear dependence by first glance, as the second vector is 2 times the first vector, which indicates linear dependence.\n\nmatrix = np.array([[1, 2], [2, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n\n(0, 1)\n\n\nThis says that the vectors at index 0 and 1 are linearly independent. Let consider a linearly dependent set of vectors to see the result of the above code clearly.\n\nmatrix = np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 1, 0], [1, 0, 0, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n\n(0, 1, 3)\n\n\nThis says that vectors at index 0, 1, and 3 are linearly independent, while vector at index 2 is linearly dependent."
  },
  {
    "objectID": "posts/mathematics/linear_algebra_3.html#vector",
    "href": "posts/mathematics/linear_algebra_3.html#vector",
    "title": "Intermediate Linear Algebra with Python - Part I",
    "section": "",
    "text": "In the case of the dot product between two vectors, we saw that the result is a scalar. In the case of a cross product the result is a vector, so the cross product is also called the vector product. The resulted vector is a vector that is at right angles to both the other vectors in 3D Euclidean space. This means that the cross-product only really makes sense when working with vectors that contain three components.\nThere are two formulas to calculate the cross product. One is algebraic and second is geometric. More precisely, the first formula catches the algebraic intuition of cross product and second catches the geometric intuition of the cross product.\nIf we have two vectors \\(A\\) and \\(B\\) in such a way:\n\\[\nA =\n\\begin{bmatrix}\n    a_{1} \\\\\n    a_{2} \\\\\n    a_{3}\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    b_{1} \\\\\n    b_{2} \\\\\n    b_{3}\n\\end{bmatrix}\n\\]\nThe algebraic formula is:\n\\[\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(a_{2} \\cdot b_{3}) - (a_{3} \\cdot b_{2}) \\\\\n(a_{3} \\cdot b_{1}) - (a_{1} \\cdot b_{3}) \\\\\n(a_{1} \\cdot b_{2}) - (a_{2} \\cdot b_{1})\n\\end{bmatrix}\n\\]\nAt the first glance, the formula is not easy to remember. Let try another formula which uses some advanced components.\n\\[\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n\\vec{i} & \\vec{j} & \\vec{k} \\\\\na_{1} & a_{2} & a_{3} \\\\\nb_{1} & b_{2} & b_{3}\n\\end{bmatrix}\n\\]\nwhere \\(\\vec{i}\\), \\(\\vec{j}\\) and \\(\\vec{k}\\) are bases vectors and I will explain in advance part of these series.\nTo find the cross product we have to find the determinant of above matrix in the following way:\n\\[\n\\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\na_{2} & a_{3} \\\\\nb_{2} & b_{3}\n\\end{bmatrix} \\vec{i} \\quad - \\quad\n\\begin{bmatrix}\na_{1} & a_{3} \\\\\nb_{1} & b_{3}\n\\end{bmatrix} \\vec{j} \\quad + \\quad\n\\begin{bmatrix}\na_{1} & a_{2} \\\\\nb_{1} & b_{2}\n\\end{bmatrix} \\vec{k}\n\\]\nSuppose we have two vectors:\n\\[\nA =\n\\begin{bmatrix}\n    2 \\\\\n    3 \\\\\n    1\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    1 \\\\\n    2 \\\\\n    -2\n\\end{bmatrix}\n\\]\nThen cross product is:\n\\[\n\\vec{C} = \\vec{A} \\times \\vec{B} =\n\\begin{bmatrix}\n(3 \\cdot (-2)) - (1 \\cdot 2) \\\\\n(1 \\cdot 1) - (2 \\cdot (-2)) \\\\\n(2 \\cdot 2) - (3 \\cdot 1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n(-6) - 2 \\\\\n1 - (-4) \\\\\n4 - 3\n\\end{bmatrix} =\n\\begin{bmatrix}\n-8 \\\\\n5 \\\\\n1\n\\end{bmatrix}\n\\]\nThe geometric formula is:\n\\[\n\\vec{C} =\n\\vec{A} \\times \\vec{B} =\n\\|\\vec{A}\\|\\cdot\\|\\vec{B}\\|\\cdot\\sin{\\theta} \\cdot \\hat{n}\n\\]\nwhere, \\(\\theta\\) is the angle between \\(\\vec{A}\\) and \\(\\vec{B}\\). Also, \\(\\hat{n}\\) is a unit vector perpendicular to both \\(\\vec{A}\\) and \\(\\vec{B}\\), such that \\(\\vec{A}\\), \\(\\vec{B}\\) and \\(\\hat{n}\\) form a right-handed system\nThere is also geometric application of the cross product. If we three vectors \\(\\vec{a}\\), \\(\\vec{b}\\) and \\(\\vec{c}\\) which forms the three dimensional figure such as:\n\n\n\nright-handed system\n\n\nThen the area of the parallelogram ( two dimensional front of this object ) is given by:\n\\[\n{\\rm{Area}} = \\left\\| {\\vec{a} \\times \\vec{b}} \\right\\|\n\\]\nThis is the dot product of the result of cross product between vectors \\(\\vec{a}\\) and \\(\\vec{b}\\)\nThe volume of the parallelepiped ( the whole three dimensional object ) is given by:\n\\[\n{\\rm{Volume}} = \\left| {\\vec{a} \\centerdot \\left( {\\vec{b} \\times \\vec{c}} \\right)} \\right|\n\\]\nhere, absolute value bars are necessary since the result could be negative and volume must be positive.\n\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n\nA = np.array([2, 3, 1])\nB = np.array([1, 2, -2])\n\nprint(\"A =\", A)\nprint(\"B = \", B)\n\nprint(\"Cross Product is:\", np.cross(A, B), sep=\"\\n\")\n\nA = [2 3 1]\nB =  [ 1  2 -2]\nCross Product is:\n[-8  5  1]\n\n\n\n\n\nsuppose we have set of vectors:\n\\[\n{\\alpha _1},...,{\\alpha _n} \\in A\n\\]\nthen we can define the space \\(S\\) spanned by:\n\\[\n{\\alpha _1},...,{\\alpha _n}\n\\]\nas\n\\[\nS\\left( {{\\alpha_{1}},...,{\\alpha_{n}}} \\right) = \\left\\{ {\\sum_{i = 1}^{n} {{c_{i}}{\\alpha_{i}}\\;|\\;{c_{i}} \\in \\mathbb{R}} } \\right\\}\n\\]\nwhich is the set of all linear combinations of the vectors in this subspace. The set is a subspace of \\(A\\):\n\\[\nS\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) \\subset A\n\\]\nIn other words, if we have set of vectors\n\\[\nA := \\{a_1, \\ldots, a_k\\}\n\\in\n\\mathbb R^n\n\\]\nit’s natural to think about the new vectors we can create from these vectors by performing linear operations. New vectors created in this way are called linear combinations of \\(A\\). Particularly, \\(X \\in \\mathbb{R}^n\\) is a linear combination of \\(A\\) if\n\\[\nX =\n\\beta_1 a_1 + \\cdots + \\beta_k a_k\n\\text{ for some scalars } \\beta_1, \\ldots, \\beta_k\n\\]\nhere, \\(\\beta_1, \\ldots, \\beta_k\\) are called coefficients of the linear combination.\nThe set of linear combinations of \\(A\\) is the span of \\(A\\) and is written as \\(Span(A)\\)\nFor example, if\n\\[\n\\vec{A} = \\{e_1, e_2, e_3\\}\n\\quad\n\\text{such that}\n\\quad\ne_1 :=\n\\begin{bmatrix}\n     1 \\\\\n     0 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_2 :=\n\\begin{bmatrix}\n     0 \\\\\n     1 \\\\\n     0\n\\end{bmatrix}\n, \\quad\ne_3 :=\n\\quad\n\\begin{bmatrix}\n     0 \\\\\n     0 \\\\\n     1\n\\end{bmatrix}\n\\]\nthen the span of \\(A\\) is all of \\(\\mathbb{R}^3\\), because, for any \\(X=(x_1, x_2, x_3)\\in \\mathbb{R}^3\\), we can write\n\\[\nX =\nx_1 e_1 + x_2 e_2 + x_3 e_3\n\\]\nThis means that by using \\(A\\) or vectors \\(e_1\\), \\(e_2\\) and \\(e_3\\) we can generate any vector in \\(\\mathbb{R}^3\\) by performing linear operations.\nThis figure below shows the span of \\(A = \\{a_1, a_2\\}\\) in \\(\\mathbb{R}^3\\). The span is a 2 dimensional plane passing through these two points and the origin.\n\n# Linear function to generate a plane\ndef f(x, y):\n    return (0.2 * x) + (0.1 * y)\n\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(projection=\"3d\")\nax.set(xlim=(-5, 5), ylim=(-5, 5), zlim=(-5, 5), xticks=(0,), yticks=(0,), zticks=(0,))\n\nz = np.linspace(-5, 5, 3)\ny = np.zeros(3)\nx = np.zeros(3)\n\nax.plot(x, y, z, \"k-\", lw=2, alpha=0.5)\nax.plot(z, x, y, \"k-\", lw=2, alpha=0.5)\nax.plot(y, z, x, \"k-\", lw=2, alpha=0.5)\n\n# Set vector coordinates\nx_coords = np.array((3, 3))\ny_coords = np.array((4, -4))\nz = f(x_coords, y_coords)\n\nfor i in (0, 1):\n    ax.text(x_coords[i], y_coords[i], z[i], f\"$a_{i+1}$\", fontsize=14)\n\n# We need to draw lines from origin to the vectors\nfor i in (0, 1):\n    x = (0, x_coords[i])\n    y = (0, y_coords[i])\n    z = (0, f(x_coords[i], y_coords[i]))\n    ax.plot(x, y, z, \"b-\", lw=1.5, alpha=0.6)\n\n\n# As we already draw axes and vectors, it's time to plot the plane\nxr2 = np.linspace(-5, 5, 50)\nyr2 = np.linspace(-5, 5, 50)\n\nx2, y2 = np.meshgrid(xr2, yr2)\nz2 = f(x2, y2)\n\nax.plot_surface(x2, y2, z2, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=True, alpha=0.2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove, I mentioned a linear combination. In order to define linear dependence and independence let farther clarify what is a linear combination. If we have a set of vectors\n\\[\n\\vec{A} = \\{a_1, \\ldots, a_k\\}\n\\]\nwhich all have the same dimension, then\n\nA linear combination of the vectors in \\(A\\) is any vector of the form \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k}\\), where \\(c_{1}, c_{2}, \\ldots, c_{k}\\) are arbitrary scalars\n\nFor example, if \\(A = \\{[1, 2], [2, 1]\\}\\), then\n\\[\n2 a_{1} - a_{2} = 2 ([1, 2]) - [2, 1] = [0, 3]\n\\]\nis linear combination of the vectors in \\(A\\).\n\nA set \\(A\\) of m-dimensional vectors is linearly independent if the only linear combination of vectors in \\(A\\) that equals \\(0\\) is the trivial linear combination\n\nThis formal definition seems a little bit confusing. Let consider the example to catch the idea.\nThe set of vectors\n\\[\nA =\n\\{[1, 0], [0, 1]\\}\n\\]\nis linearly independent. Let prove this claim. We need to find constants \\(c_{1}\\) and \\(c_{2}\\) satisfying\n\\[\nc_{1} ([1, 0]) + c_{2} ([0, 1]) = [0, 0]\n\\]\nsolving this system of equations gives that \\([c_{1}, c_{2}] = [0, 0] \\rightarrow c_{1} = c_{2} = 0\\), in turn this implies that \\(A\\) is linearly independent.\nThe following statements are equivalent for a linear independence of \\(A\\):\n\nNo vector in \\(A\\) can be formed as a linear combination of the other vectors. This means that if we have three vectors in \\(A\\), any of them cannot be expressed as the linear combination of the other two.\nIf \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k} = 0\\), then \\(c_{1} = c_{2} = \\cdots = c_{k} = 0\\)\n\n\nA set \\(A\\) of m-dimensional vectors is linearly dependent if there is a nontrivial linear combination of the vectors in \\(A\\) that adds up to \\(0\\)\n\nThe set of vectors\n\\[\nA =\n\\{[1, 2], [2, 4]\\}\n\\]\nis linearly dependent set of vectors. Let see how.\n\\[\nc_{1} ([1, 2]) + c_{2} ([2, 4]) = [0, 0]\n\\]\nThere is a nontrivial linear combination with \\(c_{1} = 2\\) and \\(c_{2} = 1\\) that yields \\(0\\). This implies \\(A\\) is the linearly dependent set of vectors. It’s easy in this case to spot linear dependence by first glance, as the second vector is 2 times the first vector, which indicates linear dependence.\n\nmatrix = np.array([[1, 2], [2, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n\n(0, 1)\n\n\nThis says that the vectors at index 0 and 1 are linearly independent. Let consider a linearly dependent set of vectors to see the result of the above code clearly.\n\nmatrix = np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 1, 0], [1, 0, 0, 1]])  # this is our set of vectors\n\n_, inds = sympy.Matrix(matrix).T.rref()\n\nprint(inds)\n\n(0, 1, 3)\n\n\nThis says that vectors at index 0, 1, and 3 are linearly independent, while vector at index 2 is linearly dependent."
  },
  {
    "objectID": "posts/mathematics/linear_algebra_4.html",
    "href": "posts/mathematics/linear_algebra_4.html",
    "title": "Intermediate Linear Algebra with Python - Part II",
    "section": "",
    "text": "Introduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn this post I will introduce you to the notion of matrix, different types of matrices, and how to operate on them. I will also show you how to use Python to manipulate matrices.\n\n\n\n\nDuring years of linear algebra evolution, there appeared different types of matrices. Some of them were fundamentals, some of them appeared lately. In this part, I will introduce some basic types of matrices and give you reference to find some other useful ones. Previously, I talked about the identity matrix, which operates as number 1 in matrix multiplication and is denoted by capital letter \\(I\\).\nA square matrix is a matrix with the same number of rows and columns.\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nA diagonal matrix is a matrix in which the entries on principal diagonal are non-zero and all the others are zeros.\n\\[\nA =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 2 & 0 \\\\\n    0 & 0 & 3\n\\end{bmatrix}\n\\]\nScalar multiple of the identity matrix is called scalar matrix that is also diagonal. This means on the main diagonal all elements are equal.\n\\[\nA =\n\\begin{bmatrix}\n    2 & 0 & 0 \\\\\n    0 & 2 & 0 \\\\\n    0 & 0 & 2\n\\end{bmatrix}\n\\]\nA square matrix is called triangular matrix if all of its elements above the main diagonal are zero (lower triangular matrix) or all of its elements below the main diagonal are zero (upper triangular matrix).\n\\[\nA =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    4 & 5 & 0 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\quad\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    0 & 5 & 6 \\\\\n    0 & 0 & 9\n\\end{bmatrix}\n\\]\nThese matrices are lower and upper triangular matrices, respectively.\nA null or zero matrix is a matrix with all elements equal to zero.\n\\[\nA =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0\n\\end{bmatrix}\n\\]\nA matrix of ones is where all elements equal to 1.\n\\[\nA =\n\\begin{bmatrix}\n    1 & 1 & 1 \\\\\n    1 & 1 & 1 \\\\\n    1 & 1 & 1\n\\end{bmatrix}\n\\]\nSymmetric matrix is a square matrix that is equal to its own transpose or \\(A = A^T\\). For example,\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    2 & 4 & 5 \\\\\n    3 & 5 & 6\n\\end{bmatrix}\n\\]\nis a symmetric matrix. Furthermore, matrix elements are symmetric with respect to main diagonal or are equal.\nA skew-symmetric matrix is a square matrix whose transpose equals its negative or \\(A^T = -A\\). For example,\n\\[\nA =\n\\begin{bmatrix}\n    0 & 3 & 4 \\\\\n    -3 & 0 & 7 \\\\\n    -4 & -7 & 0\n\\end{bmatrix}\n\\quad\nA^T =\n\\begin{bmatrix}\n    0 & -3 & -4 \\\\\n    3 & 0 & -7 \\\\\n    4 & 7 & 0\n\\end{bmatrix}\n\\quad\n-A =\n\\begin{bmatrix}\n    0 & -3 & -4 \\\\\n    3 & 0 & -7 \\\\\n    4 & 7 & 0\n\\end{bmatrix}\n\\]\nInvolutory matrix is a square matrix that is equal to its own inverse. More precisely, it is the matrix whose square is the identity matrix.\n\\[\nA =\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\]\nthen\n\\[\nA^2 =\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1\n\\end{bmatrix}\n\\ =\nI\n\\]\nA square matrix is called idempotent matrix, if multiplied by itself yields itself. Equivalently, \\(A \\cdot A = A\\)\n\\[\nA =\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\]\nA nildepotent matrix is such that \\(A^k = 0\\) for some positive integer \\(k\\). This means, for some positive \\(k\\), multiplying matrix \\(A\\) by \\(k\\) times gives zero matrix. For matrix \\(A\\) and for \\(k=2\\) we have:\n\\[\nA =\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\]\n\\[\n\\quad\n\\]\n\\[\nA^2 =\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0\n\\end{bmatrix}\n\\]\n\nSo, as I said there are much much more matrices, but I restricted here due to limited space. If you think you need more, definitely check this out wikipedia page.\n\nimport numpy as np\n\n\n# Diagonal Matrix\ndiagonal = np.diag([1, 2, 3])\nprint(\"Diagonal Matrix\", diagonal, sep=\"\\n\")\n\n\n# Lower Triangular Matrix\nlow_triang = np.tril([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Lower Triangular Matrix\", low_triang, sep=\"\\n\")\n\n\n# Upper Triangular Matrix\nupper_triang = np.triu([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Upper Triangular Matrix\", upper_triang, sep=\"\\n\")\n\n\n# Matrix of Zeros\nzeros = np.zeros((3, 3), dtype=int)\nprint(\"Matrix of Zeros\", zeros, sep=\"\\n\")\n\n\n# Matrix of Ones\nones = np.ones((3, 3), dtype=int)\nprint(\"Matrix of Ones\", ones, sep=\"\\n\")\n\n\n# Identity Matrix\nidentity = np.eye(3, dtype=int)\nprint(\"Identity Matrix\", identity, sep=\"\\n\")\n\nDiagonal Matrix\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nLower Triangular Matrix\n[[1 0 0]\n [4 5 0]\n [7 8 9]]\nUpper Triangular Matrix\n[[1 2 3]\n [0 5 6]\n [0 0 9]]\nMatrix of Zeros\n[[0 0 0]\n [0 0 0]\n [0 0 0]]\nMatrix of Ones\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\nIdentity Matrix\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n\n\n\n\n\nThe trace of \\(n\\times n\\) square matrix \\(A\\) is the sum of all elements on the main diagonal. It is defined only for square matrices and the formula is:\n\\[\ntr(A)=\\sum_{i=1}^{n}a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}\n\\]\nWhere \\(a_{ii}\\) denotes the entry on \\(i\\)-th row and \\(j\\)-th column of matrix A.\nFor example, Let \\(A\\) be a matrix,\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nThen the trace is:\n\\[\ntr(A)=\\sum_{i=1}^{3}a_{ii} = a_{11} + a_{22} + a_{33} = 1 + 5 + 9 = 15\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\ntrace = np.trace(A)\nprint(\"Trace: \", trace)\n\nTrace:  15\n\n\n\n\n\nThere is no plain English definition of determinant but I’ll try to explain it by examples to catch the main idea behind of that special number. However, we can consider determinant as a function which as an input accepts \\(n \\times n\\) matrix and output real or a complex number, that is called determinant of input matrix and is denoted by \\(det(A)\\) or \\(|A|\\).\nFor any \\(2 \\times 2\\) square matrix \\(A\\) determinant is calculated by:\n\\[\nA =\n\\begin{bmatrix}\n    a & b \\\\\n    c & d \\\\\n\\end{bmatrix}\n\\]\n\\[\ndet(A) = ad - bc\n\\]\nIt seems easy to calculate the determinant of any \\(2 \\times 2\\) matrix right? Now think about how to calculate determinant for higher dimensional matrices…did you find a way? If no, let me explain it step by step. If we have, say \\(3 \\times 3\\) matrix \\(A\\) and want to calculate determinant we need some other notions such as minors and co-factors of that matrix.\n\nA = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\ndeterminant = np.linalg.det(A)\nprint(\"Determinant: \", determinant)\n\nDeterminant:  -32.000000000000014\n\n\n\n\n\nA minor of matrix \\(A\\) is the determinant of some smaller square matrix. Precisely, the minor \\(M_{i,j}\\) is the determinant of matrix \\(A\\) with row \\(i\\) and column \\(j\\) omitted. Minor of matrix \\(A\\) is denoted by \\(M_{ij}\\), where \\(i\\) and \\(j\\) denotes element of \\(i\\)-th row and \\(j\\)-th column. Let have general matrix \\(A\\):\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nWe can take rows or columns to find all minors. It’s up to you which one you take, rows or columns. Let take the columns. We take the first element of our matrix \\(a_{11}\\) and delete row and column along it. As the first element is \\(a_{11}\\), we have to delete first row and first column. After that, we take the second element of the first column which is \\(a_{21}\\) and do same or delete second row and first column. After that, we take the third element of the first column \\(a_{31}\\) and delete third row and first column. We have to do these for three columns. After all of that, we have:\n\\[\nM_{11} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    \\square & a_{22} & a_{23} \\\\\n    \\square & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{22} & a_{23} \\\\\n    a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\na_{22}a_{33} - a_{23}a_{32}\n\\] \\[\n\\quad\n\\] \\[\nM_{21} =\n\\begin{bmatrix}\n    \\square & a_{12} & a_{13} \\\\\n    \\square & \\square & \\square \\\\\n    \\square & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{12} & a_{13} \\\\\n    a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\na_{12}a_{33} - a_{13}a_{32}\n\\] \\[\n\\quad\n\\] \\[\nM_{31} =\n\\begin{bmatrix}\n    \\square & a_{12} & a_{13} \\\\\n    \\square & a_{22} & a_{23} \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{12} & a_{13} \\\\\n    a_{22} & a_{23}\n\\end{bmatrix}\n\\ =\na_{12}a_{23} - a_{13}a_{22}\n\\] \\[\n\\quad\n\\] \\[\nM_{12} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    a_{21} & \\square & a_{23} \\\\\n    a_{31} & \\square & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{21} & a_{23} \\\\\n    a_{31} & a_{33}\n\\end{bmatrix}\n\\ =\na_{21}a_{33} - a_{23}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{22} =\n\\begin{bmatrix}\n    a_{11} & \\square & a_{13} \\\\\n    \\square & \\square & \\square \\\\\n    a_{31} & \\square & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{13} \\\\\n    a_{31} & a_{33}\n\\end{bmatrix}\n\\ =\na_{11}a_{33} - a_{13}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{32} =\n\\begin{bmatrix}\n    a_{11} & \\square & a_{13} \\\\\n    a_{21} & \\square & a_{23} \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{13} \\\\\n    a_{21} & a_{13}\n\\end{bmatrix}\n\\ =\na_{11}a_{13} - a_{13}a_{21}\n\\] \\[\n\\quad\n\\] \\[\nM_{13} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    a_{21} & a_{22} & \\square \\\\\n    a_{31} & a_{32} & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{21} & a_{22} \\\\\n    a_{31} & a_{32}\n\\end{bmatrix}\n\\ =\na_{21}a_{32} - a_{22}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{23} =\n\\begin{bmatrix}\n    a_{11} & a_{12} & \\square \\\\\n    \\square & \\square & \\square \\\\\n    a_{31} & a_{32} & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{12} \\\\\n    a_{31} & a_{32}\n\\end{bmatrix}\n\\ =\na_{11}a_{32} - a_{12}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{33} =\n\\begin{bmatrix}\n    a_{11} & a_{12} & \\square \\\\\n    a_{21} & a_{22} & \\square \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{12} \\\\\n    a_{21} & a_{22}\n\\end{bmatrix}\n\\ =\na_{11}a_{22} - a_{12}a_{21}\n\\]\nThese nine scalars are minors of matrix \\(A\\). Once again, minor is not smaller matrix, it is determinant of a smaller matrix.\n\n\n\nWe left one more step to compute determinant of \\(3 \\times 3\\) matrix \\(A\\). This step is cofactor of matrix \\(A\\). The cofactor of matrix \\(A\\) is the minor, multiplied by \\((-1)^{i+j}\\) and is denoted by \\(C_{ij}\\)\n\\[\nC_{ij} =\n(-1)^{i+j} \\cdot M_{ij}\n\\]\nwhere \\(i\\) is the number of row and \\(j\\) is the number of column of matrix \\(A\\).\nIn the above case our co-factors are:\n\\[\nC_{11} =\n(-1)^{1+1} \\cdot M_{11}\n\\]\n\\[\nC_{21} =\n(-1)^{2+1} \\cdot M_{21}\n\\]\n\\[\nC_{31} =\n(-1)^{3+1} \\cdot M_{31}\n\\]\n\\[\nC_{12} =\n(-1)^{1+2} \\cdot M_{12}\n\\]\n\\[\nC_{22} =\n(-1)^{2+2} \\cdot M_{22}\n\\]\n\\[\nC_{32} =\n(-1)^{3+2} \\cdot M_{32}\n\\]\n\\[\nC_{13} =\n(-1)^{1+3} \\cdot M_{13}\n\\]\n\\[\nC_{23} =\n(-1)^{2+3} \\cdot M_{23}\n\\]\n\\[\nC_{33} =\n(-1)^{3+3} \\cdot M_{33}\n\\]\nSo, the sum of \\(i\\) and \\(j\\) in the power of \\((-1)\\) switch the sign of every minor.\n\n\n\nWe are ready to compute the determinant of our \\(3 \\times 3\\) matrix \\(A\\). We need to expand this matrix along one of the row or one of the column to compute the determinant. It’s up to you which one you take, row or column. Let take the first column. Now, what does expansion means? We have to fix either \\(i\\) if we choose a row, or \\(j\\) if we choose column. At first glance it seems confusing but an example will make sense. This expansion is called Laplace Expansion and is used to compute the determinant of any \\(n \\times n\\) matrix.\n\\[\ndet(A) = \\sum_{j\\prime=1}^{n}a_{ij\\prime}C_{ij\\prime}\n\\ =\n\\sum_{i\\prime=1}^{n}a_{i\\prime j}C_{i\\prime j}\n\\]\nwhere \\(i\\prime\\) means we fixed index \\(i\\) or row and we change only column index. In case of \\(j\\prime\\) we fixed index \\(j\\) or columns and change the only row. So, when \\(i\\) is fixed it is called row expansion and when \\(j\\) is fixed it’s called column expansion. \\(C_{ij}\\) is our co-factor.\nTo continue the above example, let expand our initial matrix \\(A\\) by the first column, meaning I fix \\(j\\) to be 1 and only change row index \\(i\\) from 1 to 3. In this particular case above formula is:\n\\[\ndet(A) = \\sum_{j\\prime=1}^{3}a_{ij\\prime}C_{ij\\prime}\n\\ =\na_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31}\n\\]\nInstead, if I choose first row, I have to fix row index \\(i\\) and change column index \\(j\\) from 1 to 3 and determinant formula is:\n\\[\ndet(A) = \\sum_{i\\prime=1}^{3}a_{i\\prime j}C_{i\\prime j}\n\\ =\na_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\n\\]\nBelow, you will see numerical example.\n\n\n\nWe can’t actually divide matrix by a matrix; but when we want to divide matrices, we can take advantage of the fact that division by a given number is the same as multiplication by the reciprocal of that number.\nFor matrix division, we use a related idea; we multiply matrix by the inverse of a matrix. If we have two matrices \\(A\\) and \\(B\\), we can do:\n\\[\nA \\div B = A \\cdot B^{-1}\n\\]\nHere, \\(B^{-1}\\) is the inverse of matrix \\(B\\). As taking inverse of a matrix requires computations and is not easy, let explain it below and then return here.\n\n\n\nThe matrix is said to be invertible if:\n\\[\nA \\cdot A^{-1} = I\n\\]\nwhere \\(I\\) is the identity matrix and \\(A^{-1}\\) is the inverse of \\(A\\). Generally, matrix inverse is only defined for square matrices, but there still exist ways to take the inverse of non-square matrices but this is out of the scope of this blog series and I will not consider.\nFor \\(2 \\times 2\\) matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n    a & b \\\\\n    c & d \\\\\n\\end{bmatrix}\n\\]\nInverse of \\(A\\) is:\n\\[\nA^{-1} =\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\n    d & -b \\\\\n    -c & a \\\\\n\\end{bmatrix}\n\\]\nWhat happened there?\n\nI swapped the positions of a and d\nI changed the signs of b and c\nI multiplied the resulting matrix by 1 over the determinant of the matrix \\(A\\)\n\nFor example,\n\\[\nA =\n\\begin{bmatrix}\n    6 & 2 \\\\\n    1 & 2 \\\\\n\\end{bmatrix}\n\\quad\nA^{-1} =\n\\frac{1}{(6 \\times 2) - (2 \\times 1)}\n\\begin{bmatrix}\n    2 & -2 \\\\\n    -1 & 6 \\\\\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    0.2 & -0.2 \\\\\n    -0.1 & 0.6 \\\\\n\\end{bmatrix}\n\\]\nto check if this is really the inverse of \\(A\\), multiply \\(A\\) by its inverse in order to get an identity matrix.\nNow let take the inverse of \\(3 \\times 3\\) matrix. This process is long and involves taking minors, co-factors and determinant. After that, above-mentioned operations should be understandable. It has to be mentioned that there are several ways to take matrix inverse but as I started here explaining minors, co-factors and determinant I use this technique to find inverse.\nWe can calculate the inverse by:\n\nstep 1: Calculate the matrix of minors\nstep 2: Turn the matrix of minors into the matrix of cofactors\nstep 3: Transpose the matrix of cofactors\nstep 4: Multiply transpose of cofactor by 1/determinant\n\nLet have matrix:\n\\[\nA =\n\\begin{bmatrix}\n    4 & 2 & 2 \\\\\n    6 & 2 & 4 \\\\\n    2 & 2 & 8\n\\end{bmatrix}\n\\]\nStep 1: Calculate the matrix of minors\n\\[\nM_{11} =\n\\begin{bmatrix}\\color{blue}4 & \\color{lightgray}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (4\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\n\\end{bmatrix}\n\\]\n\\[\nM_{12} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{blue}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(6\\times8) - (4\\times2) = 40\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{13} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{lightgray}2 & \\color{blue}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(6\\times2) - (2\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{21} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{blue}6 & \\color{lightgray}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (2\\times2) = 12\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{22} =\n\\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{blue}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(4\\times8) - (2\\times2) = 28\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{23} =\n\\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{lightgray}2 & \\color{blue}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{31} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{blue}2 & \\color{lightgray}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(2\\times4) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{32} =\n\\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{blue}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times4) - (2\\times6) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{33} =\n\\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{lightgray}2 & \\color{blue}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times6) = -4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & -4\\end{bmatrix}\n\\]\nOur matrix of minors is:\n\\[\nM =\n\\begin{bmatrix}\n    8 & 40 & 8 \\\\\n    12 & 28 & 4 \\\\\n    4 & 4 & -4\n\\end{bmatrix}\n\\]\nNote that I used rows to find minors, in contrast to columns in the previous example.\nStep 2: Turn the matrix of minors into the matrix of cofactors\nTo turn minors matrix into cofactor matrix, we just need to change the sign of elements in minors matrix according to the rule proposed above section.\nCofactor matrix is:\n\\[\nC =\n\\begin{bmatrix}\n    8 & -40 & 8 \\\\\n    -12 & 28 & -4 \\\\\n    4 & -4 & -4\n\\end{bmatrix}\n\\]\nStep 3: Transpose the matrix of cofactors\nWe need to take the transpose of the cofactor matrix. In other words, swap their positions over the main diagonal (the main diagonal stays the same).\n\\[\nC^{T}=\n\\begin{bmatrix}8 & \\color{green}-\\color{green}1\\color{green}2 & \\color{orange}4\\\\\\color{green}-\\color{green}4\\color{green}0 & 28 & \\color{purple}-\\color{purple}4\\\\\\color{orange}8 & \\color{purple}-\\color{purple}4 & -4\\end{bmatrix}\n\\]\nThis matrix is called Adjugate or Adjoint, which is simple the transpose of the cofactor matrix.\nStep 4: Multiply transpose of cofactor by \\(\\frac{1}{determinant}\\)\nAs we did all the necessary operations to have determinant, let compute it firstly and then multiply the adjoint matrix by \\(\\frac{1}{determinant}\\).\nUsing formula:\n\\[\ndet(A) = \\sum_{i\\prime=1}^{3}a_{i\\prime j}C_{i\\prime j}\n\\ =\na_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\n\\]\nWe have:\n\\[\ndet(A) = (4 \\times 8) + (2 \\times (-40)) + (2 \\times 8) = -32\n\\]\nNow the inverse is:\n\\[\nA^{-1} =\n\\frac{1}{-32}\n\\cdot\n\\begin{bmatrix}\n    8 & -40 & 8 \\\\\n    -12 & 28 & -4 \\\\\n    4 & -4 & -4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    -0.25 & 0.375 & -0.125 \\\\\n    1.25 & 0.875 & 0.125 \\\\\n    -0.25 & 0.125 & 0.125\n\\end{bmatrix}\n\\]\nLet’s verify that the original matrix multiplied by the inverse results in an identity matrix:\n\\[\nA \\cdot A^{-1} =\n\\begin{bmatrix}4 & 2 & 2\\\\6 & 2 & 4\\\\2 & 2 & 8\\end{bmatrix} \\cdot \\begin{bmatrix}-0.25 & 0.375 & -0.125\\\\1.25 & -0.875 & 0.125\\\\-0.25 & 0.125 & 0.125\\end{bmatrix} =\n\\begin{bmatrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{bmatrix} = I\n\\]\nDo you see how challenging can be finding the inverse of \\(4 \\times 4\\) matrix? That’s why we use calculators or computer program to compute it.\n\nA = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\ninverse = np.linalg.inv(A)\nprint(\"Inverse\", inverse, sep=\"\\n\")\n\nInverse\n[[-0.25   0.375 -0.125]\n [ 1.25  -0.875  0.125]\n [-0.25   0.125  0.125]]\n\n\n\n\n\nAs we already know how to compute the inverse of a matrix, the division is easy now. If we have two matrices:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nand\n\\[\nB =\n\\begin{bmatrix}\n    4 & 2 & 2 \\\\\n    6 & 2 & 4 \\\\\n    2 & 2 & 8\n\\end{bmatrix}\n\\]\nthen \\(A\\) divided by \\(B\\) is\n\\[\nA \\cdot B^{-1}\n\\ =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    -0.25 & 0.375 & -0.125 \\\\\n    1.25 & 0.875 & 0.125 \\\\\n    -0.25 & 0.125 & 0.125\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1.5 & -1 & 0.5 \\\\\n    3.75 & -2.125 & 0.875 \\\\\n    6 & -3.25 & 1.25\n\\end{bmatrix}\n\\ \\equiv\nA \\div B\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nB = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\n# A divided by B is dot product between A and inverse of B\ninv_B = np.linalg.inv(B)\nX = np.dot(A, inv_B)\n\nprint(X)\n\n[[ 1.5   -1.     0.5  ]\n [ 3.75  -2.125  0.875]\n [ 6.    -3.25   1.25 ]]\n\n\n\n\n\nIn the previous blog, I talked about the system of linear equations and we solved this system graphically and algebraically. One of the great things about matrices, is that they can help us solve systems of equations. For example, consider the following system of equations:\n\\[\n\\begin{cases}\n2x + 4y = 18 \\\\\n6x + 2y = 34\n\\end{cases}\n\\]\nWe can write this in matrix form, like this:\n\\[\n\\begin{bmatrix}\n    2 & 4 \\\\\n    6 & 2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    x \\\\\n    y\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\]\nIf we calculate the dot product between the matrix and vector on the left side, we can see clearly that this represents the original equations.\nNow let rename our matrices:\n\\[\nA =\n\\begin{bmatrix}\n    2 & 4 \\\\\n    6 & 2\n\\end{bmatrix}\n\\quad\nX =\n\\begin{bmatrix}\n    x \\\\\n    y\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\]\nThis can be represented as \\(AX = B\\) and we know that to find \\(X\\) we have to solve this: \\(B \\div A\\). Since we cannot divide matrices in this way, we have to use the previous technique. Find the inverse of \\(A\\) and multiply by \\(B\\).\nThe inverse of \\(A\\):\n\\[\nA^{-1} =\n\\begin{bmatrix}\n    -0.1 & 0.2 \\\\\n    0.3 & -0.1\n\\end{bmatrix}\n\\]\n\\[\nX =\n\\begin{bmatrix}\n    -0.1 & 0.2 \\\\\n    0.3 & -0.1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    5 \\\\\n    2\n\\end{bmatrix}\n\\]\nNow, instead of \\(x\\) and \\(y\\) in the original equation put \\(5\\) and \\(2\\) and this will make equality true.\n\\[\n10 + 8 = 18\n\\] \\[\n30 + 4 = 34\n\\]\n\nA = np.array([[2, 4], [6, 2]])\n\nB = np.array([[18], [34]])\n\nA_inverse = np.linalg.inv(A)\n\nprint(\"The inverse of A is\", A_inverse, sep=\"\\n\")\n\nX = np.dot(A_inverse, B)\n\nprint(\"X =\", X, sep=\"\\n\")\n\nThe inverse of A is\n[[-0.1  0.2]\n [ 0.3 -0.1]]\nX =\n[[5.]\n [2.]]\n\n\n\n\n\nElementary Row Operations (ERO) play an important role in many matrix algebra applications, such as finding the inverse of a matrix and solving simultaneous linear equations. These topics are covered in advance part of the series. An ERO transforms a given matrix \\(A\\) into a new matrix \\(A^{'}\\) via one of the following operations:\n\nInterchange two rows (or columns)\nMultiply each element in a row (or column) by a non-zero number\nMultiply a row (or column) by a non-zero number and add the result to another row (or column)\n\nTo catch the idea behind this operations let do the example. We have a matrix \\(A\\) such that\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    1 & 3 & 5 & 6 \\\\\n    0 & 1 & 2 & 3\n\\end{bmatrix}\n\\]\nType 1 ERO that interchange rows 1 and 3 of \\(A\\) would yield\n\\[\nA^{'} =\n\\begin{bmatrix}\n    0 & 1 & 2 & 3 \\\\\n    1 & 3 & 5 & 6 \\\\\n    1 & 2 & 3 & 4\n\\end{bmatrix}\n\\]\nType 2 ERO that multiplies row 2 of \\(A\\) by 3 would yield\n\\[\nA^{'} =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    3 & 9 & 15 & 18 \\\\\n    0 & 1 & 2 & 3\n\\end{bmatrix}\n\\]\nType 3 ERO that multiplies row 2 of \\(A\\) by 4 and replace row 3 of \\(A\\) by \\(4 \\times (\\text{row 2 of A}) + \\text{row 3 of A}\\),\nwould yield row 3 of \\(A^{'}\\) to be\n\\(4 \\times [1 \\ 3 \\ 5 \\ 6] + [0 \\ 1 \\ 2 \\ 3] = [4 \\ 13 \\ 22 \\ 27]\\)\nand\n\\[\nA^{'} =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    1 & 3 & 5 & 6 \\\\\n    4 & 13 & 22 & 27\n\\end{bmatrix}\n\\]\nExcept this, to perform an elementary row operation on the matrix \\(A\\), first we can perform the operation on the corresponding identity matrix to obtain an elementary matrix, then multiply \\(A\\) on the left by this elementary matrix. More precisely this means that we take one ERO, whichever we want and perform this operation on corresponding identity matrix of \\(A\\). If \\(A\\) has \\(m \\times n\\) dimension we have identity matrix \\(I_{m \\times n}\\). After that, we multiply \\(A\\) by this identity matrix. If we denote the elementary matrix by \\(E\\) then, we multiply \\(A\\) by \\(E\\) in the following way:\n\\[\n\\begin{equation}E_{1} \\cdot A \\end{equation}\n\\]\nwhere \\(E_{1}\\) is ERO one performed on identity matrix.\n\n\n\nThe maximum number of linearly independent rows in a matrix \\(A\\) is called the row rank of \\(A\\) and the maximum number of linearly independent columns in \\(A\\) is called the column rank of \\(A\\). If \\(A\\) is \\(n \\times m\\) matrix, that is if matrix \\(A\\) has \\(m\\) rows and \\(n\\) columns then the following inequality holds:\n\\[\n\\text{row rank of} \\ A \\leq m \\\\\n\\text{column rank of} \\ A \\leq n\n\\]\nFurthermore, for any matrix \\(A\\)\n\\[\n\\text{row rank of} \\ A = \\text{column rank of} \\ A\n\\]\nFrom the above inequality it follows that\n\\[\nRank(A) \\leq min(m, n)\n\\]\nThis means that if a matrix has, for example, 3 rows and 5 columns, its rank cannot be more than 3. The rank of a matrix would be zero if and only if the matrix had no elements. If a matrix had even one element, its minimum rank would be one. When all of the vectors in a matrix are linearly independent, the matrix is said to be full rank.\nTo calculate the rank of a matrix, we have to compute the determinant. It turns out that the rank of a matrix \\(A\\), denoted by \\(Rank(A)\\) is the size of the largest non-zero \\(m \\times m\\) sub-matrix with non-zero determinant. To simplify further, if the determinant of \\(4 \\times 4\\) matrix \\(A\\) is zero and any \\(3 \\times3\\) sub-matrix of original matrix \\(A\\) has non-zero determinant then the rank of the original matrix \\(A\\) is \\(3\\). So we can say that rank shows the “non-degenerateness” of the matrix \\(A\\).\nActually, there is no only one way to compute the rank. I will provide one more way in the advanced tutorial.\n\nA = np.array([[1, 2, 3], [2, 4, 6], [1, -3, 5]])\n\nrank_A = np.linalg.matrix_rank(A)\n\nprint(\"Rank(A) = \", rank_A)\n\n\n# matrix B has full rank\nB = np.array([[2, 2, -1], [4, 0, 2], [0, 6, -3]])\n\nrank_B = np.linalg.matrix_rank(B)\n\nprint(\"Rank(B) = \", rank_B)\n\nRank(A) =  2\nRank(B) =  3\n\n\n\n\n\nWe can rise a square matrix \\(A\\) in any nonnegative power just like any number. This is defined as the product of \\(A\\) by itself \\(n\\) times. If matrix \\(A\\) has inverse, then \\(A^{-n} = (A^{-1})^{n}\\) or take inverse of \\(A\\) and multiply by itself \\(n\\) times.\nFor example, if\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\]\nthen\n\\[\nA^{2} =\nA A\n\\ =\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    7 & 10 \\\\\n    15 & 22\n\\end{bmatrix}\n\\]\n\nA = np.array([[1, 2], [3, 4]])\n\nA_square = np.linalg.matrix_power(A, 2)\nprint(A_square)\n\n[[ 7 10]\n [15 22]]\n\n\n\n\n\nIn the previous post I talked about vector norms but did not mentioned matrix norm. There are three types of matrix norms:\n\nMatrix norms induced by vector norms\nEntrywise matrix norms\nSchatten norms\n\nHere, I will introduce only the first two types of matrix norm and depict one example of each to give the general idea of matrix norms.\nInduced norms usually are denoted by: \\[\\|A\\|_p\\]\nIn the special cases of \\(p = 1, 2, \\infty\\), the induced matrix norms can be computed by:\n\\[\n\\|A\\|_1 = max_{1\\leq j \\leq m} \\sum_{i = 1}^{m}|a_{ij}|\n\\]\nWhich is the maximum absolute column sum of the matrix \\(A\\)\n\\[\n\\|A\\|_2 = \\|A\\|_F = \\sigma_{max}(A)\n\\]\nWhere \\(\\|A\\|_F\\) is Frobenius Norm, which will be discussed below and \\(\\sigma_{max}(A)\\) is the spectral norm. The later will be discussed in the next post.\n\\[\n\\|A\\|_{\\infty} = max_{1\\leq i \\leq m} \\sum_{j = 1}^{n}|a_{ij}|\n\\]\nwhich is the maximum absolute row sum of the matrix \\(A\\).\nTo clarify this farther, let consider the following example:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\n\\[\n\\|A\\|_1 = max(|1| + |4| + |7|; |2| + |5| + |8|; |3| + |6| + |9|) = max(12; 15; 18) = 18\n\\] \\[\n\\quad\n\\] \\[\n\\|A\\|_{\\infty} = max(|1| + |2| + |3|; |4| + |5| + |6|; |7| + |8| + |9|) = max(6; 15; 24) = 24\n\\]\nImagine, we have a vector whose elements are matrices instead of scalars. Then norm defined here is entrywise matrix norm. The general formula for entrywise matrix norm is:\n\\[\n\\|A\\|_{p,q} = \\left(\\sum_{j=1}^{n}\\left(\\sum_{i=1}^{m}|a_{ij}^{p}\\right)^\\frac{q}{p}\\right)^\\frac{1}{q}\n\\]\nwhere \\(p,q \\geq 1\\)\nWhen \\(p=q=2\\) we have Frobenius Norm or Frobenius Inner Product:\n\\[\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^{n}|a_{ij}|^2}\n\\]\nand when \\(p=q=\\infty\\), we have Max Norm:\n\\[\n\\|A\\|_{max} = max_{ij}|a_{ij}|\n\\]\nIf we have matrix \\(A\\) such that:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nThen Frobenius Norm is:\n\\[\n\\|A\\|_{F} = \\sqrt{(|1|^2 + |2|^2 + |3|^2 + |4|^2 + |5|^2 + |6|^2 + |7|^2 + |8|^2 + |9|^2)} =\n\\]\n\\[\n\\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81} = \\sqrt{285} \\approx 16.89\n\\]\nFor two matrices \\(A\\) and \\(B\\) we have Frobenius Inner Product:\n\\[\n\\langle A,B \\rangle_{F} = \\sum_{i,j}\\overline{A_{i,j}}B_{i,j} = tr \\left(\\overline{A^T}B \\right)\n\\]\nWhere overline denotes the complex conjugate of a matrix.\nIf\n\\[\nA =\n\\begin{bmatrix}\n    2 & 0 \\\\\n    1 & 1 \\\\\n\\end{bmatrix}\n\\]\nand\n\\[\nB =\n\\begin{bmatrix}\n    8 & -3 \\\\\n    4 & 1 \\\\\n\\end{bmatrix}\n\\]\nthen\n\\[\n\\langle A,B \\rangle_{F} = 2 \\cdot 8 + 0 \\cdot (-3) + 1 \\cdot 4 + 1 \\cdot 1 = 21\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nfrobenius_norm = np.linalg.norm(A, ord=\"fro\")\ncolumn_max_norm = np.linalg.norm(A, ord=1)\nrow_max_norm = np.linalg.norm(A, ord=np.inf)  # same as infinity norm\n\n# Frobenius Inner Product\nA = np.array([[2, 0], [1, 1]])\nB = np.array([[8, -3], [4, 1]])\n\n# numpy.vdot function flattens high dimensional arrays and takes dot product\nfrobenius_inner_product = np.vdot(A, B)\n\nprint(\"Frobenius Norm is: \", frobenius_norm)\nprint(\"Column Max Norm is: \", column_max_norm)\nprint(\"Row Max Norm is: \", row_max_norm)\nprint(\"Frobenius Inner Product is: \", frobenius_inner_product)\n\nFrobenius Norm is:  16.881943016134134\nColumn Max Norm is:  18.0\nRow Max Norm is:  24.0\nFrobenius Inner Product is:  21"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_4.html#matrix",
    "href": "posts/mathematics/linear_algebra_4.html#matrix",
    "title": "Intermediate Linear Algebra with Python - Part II",
    "section": "",
    "text": "During years of linear algebra evolution, there appeared different types of matrices. Some of them were fundamentals, some of them appeared lately. In this part, I will introduce some basic types of matrices and give you reference to find some other useful ones. Previously, I talked about the identity matrix, which operates as number 1 in matrix multiplication and is denoted by capital letter \\(I\\).\nA square matrix is a matrix with the same number of rows and columns.\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nA diagonal matrix is a matrix in which the entries on principal diagonal are non-zero and all the others are zeros.\n\\[\nA =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 2 & 0 \\\\\n    0 & 0 & 3\n\\end{bmatrix}\n\\]\nScalar multiple of the identity matrix is called scalar matrix that is also diagonal. This means on the main diagonal all elements are equal.\n\\[\nA =\n\\begin{bmatrix}\n    2 & 0 & 0 \\\\\n    0 & 2 & 0 \\\\\n    0 & 0 & 2\n\\end{bmatrix}\n\\]\nA square matrix is called triangular matrix if all of its elements above the main diagonal are zero (lower triangular matrix) or all of its elements below the main diagonal are zero (upper triangular matrix).\n\\[\nA =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    4 & 5 & 0 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\quad\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    0 & 5 & 6 \\\\\n    0 & 0 & 9\n\\end{bmatrix}\n\\]\nThese matrices are lower and upper triangular matrices, respectively.\nA null or zero matrix is a matrix with all elements equal to zero.\n\\[\nA =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0\n\\end{bmatrix}\n\\]\nA matrix of ones is where all elements equal to 1.\n\\[\nA =\n\\begin{bmatrix}\n    1 & 1 & 1 \\\\\n    1 & 1 & 1 \\\\\n    1 & 1 & 1\n\\end{bmatrix}\n\\]\nSymmetric matrix is a square matrix that is equal to its own transpose or \\(A = A^T\\). For example,\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    2 & 4 & 5 \\\\\n    3 & 5 & 6\n\\end{bmatrix}\n\\]\nis a symmetric matrix. Furthermore, matrix elements are symmetric with respect to main diagonal or are equal.\nA skew-symmetric matrix is a square matrix whose transpose equals its negative or \\(A^T = -A\\). For example,\n\\[\nA =\n\\begin{bmatrix}\n    0 & 3 & 4 \\\\\n    -3 & 0 & 7 \\\\\n    -4 & -7 & 0\n\\end{bmatrix}\n\\quad\nA^T =\n\\begin{bmatrix}\n    0 & -3 & -4 \\\\\n    3 & 0 & -7 \\\\\n    4 & 7 & 0\n\\end{bmatrix}\n\\quad\n-A =\n\\begin{bmatrix}\n    0 & -3 & -4 \\\\\n    3 & 0 & -7 \\\\\n    4 & 7 & 0\n\\end{bmatrix}\n\\]\nInvolutory matrix is a square matrix that is equal to its own inverse. More precisely, it is the matrix whose square is the identity matrix.\n\\[\nA =\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\]\nthen\n\\[\nA^2 =\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    -5 & -8 & 0 \\\\\n    3 & 5 & 0 \\\\\n    1 & 2 & -1\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1\n\\end{bmatrix}\n\\ =\nI\n\\]\nA square matrix is called idempotent matrix, if multiplied by itself yields itself. Equivalently, \\(A \\cdot A = A\\)\n\\[\nA =\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    2 & -2 & -4 \\\\\n    -1 & 3 & 4 \\\\\n    1 & -2 & -3\n\\end{bmatrix}\n\\]\nA nildepotent matrix is such that \\(A^k = 0\\) for some positive integer \\(k\\). This means, for some positive \\(k\\), multiplying matrix \\(A\\) by \\(k\\) times gives zero matrix. For matrix \\(A\\) and for \\(k=2\\) we have:\n\\[\nA =\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\]\n\\[\n\\quad\n\\]\n\\[\nA^2 =\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    5 & -3 & 2 \\\\\n    15 & -9 & 6 \\\\\n    10 & -6 & 4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0\n\\end{bmatrix}\n\\]\n\nSo, as I said there are much much more matrices, but I restricted here due to limited space. If you think you need more, definitely check this out wikipedia page.\n\nimport numpy as np\n\n\n# Diagonal Matrix\ndiagonal = np.diag([1, 2, 3])\nprint(\"Diagonal Matrix\", diagonal, sep=\"\\n\")\n\n\n# Lower Triangular Matrix\nlow_triang = np.tril([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Lower Triangular Matrix\", low_triang, sep=\"\\n\")\n\n\n# Upper Triangular Matrix\nupper_triang = np.triu([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Upper Triangular Matrix\", upper_triang, sep=\"\\n\")\n\n\n# Matrix of Zeros\nzeros = np.zeros((3, 3), dtype=int)\nprint(\"Matrix of Zeros\", zeros, sep=\"\\n\")\n\n\n# Matrix of Ones\nones = np.ones((3, 3), dtype=int)\nprint(\"Matrix of Ones\", ones, sep=\"\\n\")\n\n\n# Identity Matrix\nidentity = np.eye(3, dtype=int)\nprint(\"Identity Matrix\", identity, sep=\"\\n\")\n\nDiagonal Matrix\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nLower Triangular Matrix\n[[1 0 0]\n [4 5 0]\n [7 8 9]]\nUpper Triangular Matrix\n[[1 2 3]\n [0 5 6]\n [0 0 9]]\nMatrix of Zeros\n[[0 0 0]\n [0 0 0]\n [0 0 0]]\nMatrix of Ones\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\nIdentity Matrix\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n\n\n\n\n\nThe trace of \\(n\\times n\\) square matrix \\(A\\) is the sum of all elements on the main diagonal. It is defined only for square matrices and the formula is:\n\\[\ntr(A)=\\sum_{i=1}^{n}a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}\n\\]\nWhere \\(a_{ii}\\) denotes the entry on \\(i\\)-th row and \\(j\\)-th column of matrix A.\nFor example, Let \\(A\\) be a matrix,\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nThen the trace is:\n\\[\ntr(A)=\\sum_{i=1}^{3}a_{ii} = a_{11} + a_{22} + a_{33} = 1 + 5 + 9 = 15\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\ntrace = np.trace(A)\nprint(\"Trace: \", trace)\n\nTrace:  15\n\n\n\n\n\nThere is no plain English definition of determinant but I’ll try to explain it by examples to catch the main idea behind of that special number. However, we can consider determinant as a function which as an input accepts \\(n \\times n\\) matrix and output real or a complex number, that is called determinant of input matrix and is denoted by \\(det(A)\\) or \\(|A|\\).\nFor any \\(2 \\times 2\\) square matrix \\(A\\) determinant is calculated by:\n\\[\nA =\n\\begin{bmatrix}\n    a & b \\\\\n    c & d \\\\\n\\end{bmatrix}\n\\]\n\\[\ndet(A) = ad - bc\n\\]\nIt seems easy to calculate the determinant of any \\(2 \\times 2\\) matrix right? Now think about how to calculate determinant for higher dimensional matrices…did you find a way? If no, let me explain it step by step. If we have, say \\(3 \\times 3\\) matrix \\(A\\) and want to calculate determinant we need some other notions such as minors and co-factors of that matrix.\n\nA = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\ndeterminant = np.linalg.det(A)\nprint(\"Determinant: \", determinant)\n\nDeterminant:  -32.000000000000014\n\n\n\n\n\nA minor of matrix \\(A\\) is the determinant of some smaller square matrix. Precisely, the minor \\(M_{i,j}\\) is the determinant of matrix \\(A\\) with row \\(i\\) and column \\(j\\) omitted. Minor of matrix \\(A\\) is denoted by \\(M_{ij}\\), where \\(i\\) and \\(j\\) denotes element of \\(i\\)-th row and \\(j\\)-th column. Let have general matrix \\(A\\):\n\\[\nA =\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nWe can take rows or columns to find all minors. It’s up to you which one you take, rows or columns. Let take the columns. We take the first element of our matrix \\(a_{11}\\) and delete row and column along it. As the first element is \\(a_{11}\\), we have to delete first row and first column. After that, we take the second element of the first column which is \\(a_{21}\\) and do same or delete second row and first column. After that, we take the third element of the first column \\(a_{31}\\) and delete third row and first column. We have to do these for three columns. After all of that, we have:\n\\[\nM_{11} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    \\square & a_{22} & a_{23} \\\\\n    \\square & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{22} & a_{23} \\\\\n    a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\na_{22}a_{33} - a_{23}a_{32}\n\\] \\[\n\\quad\n\\] \\[\nM_{21} =\n\\begin{bmatrix}\n    \\square & a_{12} & a_{13} \\\\\n    \\square & \\square & \\square \\\\\n    \\square & a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{12} & a_{13} \\\\\n    a_{32} & a_{33}\n\\end{bmatrix}\n\\ =\na_{12}a_{33} - a_{13}a_{32}\n\\] \\[\n\\quad\n\\] \\[\nM_{31} =\n\\begin{bmatrix}\n    \\square & a_{12} & a_{13} \\\\\n    \\square & a_{22} & a_{23} \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{12} & a_{13} \\\\\n    a_{22} & a_{23}\n\\end{bmatrix}\n\\ =\na_{12}a_{23} - a_{13}a_{22}\n\\] \\[\n\\quad\n\\] \\[\nM_{12} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    a_{21} & \\square & a_{23} \\\\\n    a_{31} & \\square & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{21} & a_{23} \\\\\n    a_{31} & a_{33}\n\\end{bmatrix}\n\\ =\na_{21}a_{33} - a_{23}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{22} =\n\\begin{bmatrix}\n    a_{11} & \\square & a_{13} \\\\\n    \\square & \\square & \\square \\\\\n    a_{31} & \\square & a_{33}\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{13} \\\\\n    a_{31} & a_{33}\n\\end{bmatrix}\n\\ =\na_{11}a_{33} - a_{13}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{32} =\n\\begin{bmatrix}\n    a_{11} & \\square & a_{13} \\\\\n    a_{21} & \\square & a_{23} \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{13} \\\\\n    a_{21} & a_{13}\n\\end{bmatrix}\n\\ =\na_{11}a_{13} - a_{13}a_{21}\n\\] \\[\n\\quad\n\\] \\[\nM_{13} =\n\\begin{bmatrix}\n    \\square & \\square & \\square \\\\\n    a_{21} & a_{22} & \\square \\\\\n    a_{31} & a_{32} & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{21} & a_{22} \\\\\n    a_{31} & a_{32}\n\\end{bmatrix}\n\\ =\na_{21}a_{32} - a_{22}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{23} =\n\\begin{bmatrix}\n    a_{11} & a_{12} & \\square \\\\\n    \\square & \\square & \\square \\\\\n    a_{31} & a_{32} & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{12} \\\\\n    a_{31} & a_{32}\n\\end{bmatrix}\n\\ =\na_{11}a_{32} - a_{12}a_{31}\n\\] \\[\n\\quad\n\\] \\[\nM_{33} =\n\\begin{bmatrix}\n    a_{11} & a_{12} & \\square \\\\\n    a_{21} & a_{22} & \\square \\\\\n    \\square & \\square & \\square\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    a_{11} & a_{12} \\\\\n    a_{21} & a_{22}\n\\end{bmatrix}\n\\ =\na_{11}a_{22} - a_{12}a_{21}\n\\]\nThese nine scalars are minors of matrix \\(A\\). Once again, minor is not smaller matrix, it is determinant of a smaller matrix.\n\n\n\nWe left one more step to compute determinant of \\(3 \\times 3\\) matrix \\(A\\). This step is cofactor of matrix \\(A\\). The cofactor of matrix \\(A\\) is the minor, multiplied by \\((-1)^{i+j}\\) and is denoted by \\(C_{ij}\\)\n\\[\nC_{ij} =\n(-1)^{i+j} \\cdot M_{ij}\n\\]\nwhere \\(i\\) is the number of row and \\(j\\) is the number of column of matrix \\(A\\).\nIn the above case our co-factors are:\n\\[\nC_{11} =\n(-1)^{1+1} \\cdot M_{11}\n\\]\n\\[\nC_{21} =\n(-1)^{2+1} \\cdot M_{21}\n\\]\n\\[\nC_{31} =\n(-1)^{3+1} \\cdot M_{31}\n\\]\n\\[\nC_{12} =\n(-1)^{1+2} \\cdot M_{12}\n\\]\n\\[\nC_{22} =\n(-1)^{2+2} \\cdot M_{22}\n\\]\n\\[\nC_{32} =\n(-1)^{3+2} \\cdot M_{32}\n\\]\n\\[\nC_{13} =\n(-1)^{1+3} \\cdot M_{13}\n\\]\n\\[\nC_{23} =\n(-1)^{2+3} \\cdot M_{23}\n\\]\n\\[\nC_{33} =\n(-1)^{3+3} \\cdot M_{33}\n\\]\nSo, the sum of \\(i\\) and \\(j\\) in the power of \\((-1)\\) switch the sign of every minor.\n\n\n\nWe are ready to compute the determinant of our \\(3 \\times 3\\) matrix \\(A\\). We need to expand this matrix along one of the row or one of the column to compute the determinant. It’s up to you which one you take, row or column. Let take the first column. Now, what does expansion means? We have to fix either \\(i\\) if we choose a row, or \\(j\\) if we choose column. At first glance it seems confusing but an example will make sense. This expansion is called Laplace Expansion and is used to compute the determinant of any \\(n \\times n\\) matrix.\n\\[\ndet(A) = \\sum_{j\\prime=1}^{n}a_{ij\\prime}C_{ij\\prime}\n\\ =\n\\sum_{i\\prime=1}^{n}a_{i\\prime j}C_{i\\prime j}\n\\]\nwhere \\(i\\prime\\) means we fixed index \\(i\\) or row and we change only column index. In case of \\(j\\prime\\) we fixed index \\(j\\) or columns and change the only row. So, when \\(i\\) is fixed it is called row expansion and when \\(j\\) is fixed it’s called column expansion. \\(C_{ij}\\) is our co-factor.\nTo continue the above example, let expand our initial matrix \\(A\\) by the first column, meaning I fix \\(j\\) to be 1 and only change row index \\(i\\) from 1 to 3. In this particular case above formula is:\n\\[\ndet(A) = \\sum_{j\\prime=1}^{3}a_{ij\\prime}C_{ij\\prime}\n\\ =\na_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31}\n\\]\nInstead, if I choose first row, I have to fix row index \\(i\\) and change column index \\(j\\) from 1 to 3 and determinant formula is:\n\\[\ndet(A) = \\sum_{i\\prime=1}^{3}a_{i\\prime j}C_{i\\prime j}\n\\ =\na_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\n\\]\nBelow, you will see numerical example.\n\n\n\nWe can’t actually divide matrix by a matrix; but when we want to divide matrices, we can take advantage of the fact that division by a given number is the same as multiplication by the reciprocal of that number.\nFor matrix division, we use a related idea; we multiply matrix by the inverse of a matrix. If we have two matrices \\(A\\) and \\(B\\), we can do:\n\\[\nA \\div B = A \\cdot B^{-1}\n\\]\nHere, \\(B^{-1}\\) is the inverse of matrix \\(B\\). As taking inverse of a matrix requires computations and is not easy, let explain it below and then return here.\n\n\n\nThe matrix is said to be invertible if:\n\\[\nA \\cdot A^{-1} = I\n\\]\nwhere \\(I\\) is the identity matrix and \\(A^{-1}\\) is the inverse of \\(A\\). Generally, matrix inverse is only defined for square matrices, but there still exist ways to take the inverse of non-square matrices but this is out of the scope of this blog series and I will not consider.\nFor \\(2 \\times 2\\) matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n    a & b \\\\\n    c & d \\\\\n\\end{bmatrix}\n\\]\nInverse of \\(A\\) is:\n\\[\nA^{-1} =\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\n    d & -b \\\\\n    -c & a \\\\\n\\end{bmatrix}\n\\]\nWhat happened there?\n\nI swapped the positions of a and d\nI changed the signs of b and c\nI multiplied the resulting matrix by 1 over the determinant of the matrix \\(A\\)\n\nFor example,\n\\[\nA =\n\\begin{bmatrix}\n    6 & 2 \\\\\n    1 & 2 \\\\\n\\end{bmatrix}\n\\quad\nA^{-1} =\n\\frac{1}{(6 \\times 2) - (2 \\times 1)}\n\\begin{bmatrix}\n    2 & -2 \\\\\n    -1 & 6 \\\\\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    0.2 & -0.2 \\\\\n    -0.1 & 0.6 \\\\\n\\end{bmatrix}\n\\]\nto check if this is really the inverse of \\(A\\), multiply \\(A\\) by its inverse in order to get an identity matrix.\nNow let take the inverse of \\(3 \\times 3\\) matrix. This process is long and involves taking minors, co-factors and determinant. After that, above-mentioned operations should be understandable. It has to be mentioned that there are several ways to take matrix inverse but as I started here explaining minors, co-factors and determinant I use this technique to find inverse.\nWe can calculate the inverse by:\n\nstep 1: Calculate the matrix of minors\nstep 2: Turn the matrix of minors into the matrix of cofactors\nstep 3: Transpose the matrix of cofactors\nstep 4: Multiply transpose of cofactor by 1/determinant\n\nLet have matrix:\n\\[\nA =\n\\begin{bmatrix}\n    4 & 2 & 2 \\\\\n    6 & 2 & 4 \\\\\n    2 & 2 & 8\n\\end{bmatrix}\n\\]\nStep 1: Calculate the matrix of minors\n\\[\nM_{11} =\n\\begin{bmatrix}\\color{blue}4 & \\color{lightgray}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (4\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\n\\end{bmatrix}\n\\]\n\\[\nM_{12} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{blue}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(6\\times8) - (4\\times2) = 40\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{13} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{lightgray}2 & \\color{blue}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(6\\times2) - (2\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{21} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{blue}6 & \\color{lightgray}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (2\\times2) = 12\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{22} =\n\\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{blue}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(4\\times8) - (2\\times2) = 28\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{23} =\n\\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{lightgray}2 & \\color{blue}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{31} =\n\\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{blue}2 & \\color{lightgray}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(2\\times4) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{32} =\n\\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{blue}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times4) - (2\\times6) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & \\color{lightgray}?\\end{bmatrix}\n\\]\n\\[\nM_{33} =\n\\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{lightgray}2 & \\color{blue}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times6) = -4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & -4\\end{bmatrix}\n\\]\nOur matrix of minors is:\n\\[\nM =\n\\begin{bmatrix}\n    8 & 40 & 8 \\\\\n    12 & 28 & 4 \\\\\n    4 & 4 & -4\n\\end{bmatrix}\n\\]\nNote that I used rows to find minors, in contrast to columns in the previous example.\nStep 2: Turn the matrix of minors into the matrix of cofactors\nTo turn minors matrix into cofactor matrix, we just need to change the sign of elements in minors matrix according to the rule proposed above section.\nCofactor matrix is:\n\\[\nC =\n\\begin{bmatrix}\n    8 & -40 & 8 \\\\\n    -12 & 28 & -4 \\\\\n    4 & -4 & -4\n\\end{bmatrix}\n\\]\nStep 3: Transpose the matrix of cofactors\nWe need to take the transpose of the cofactor matrix. In other words, swap their positions over the main diagonal (the main diagonal stays the same).\n\\[\nC^{T}=\n\\begin{bmatrix}8 & \\color{green}-\\color{green}1\\color{green}2 & \\color{orange}4\\\\\\color{green}-\\color{green}4\\color{green}0 & 28 & \\color{purple}-\\color{purple}4\\\\\\color{orange}8 & \\color{purple}-\\color{purple}4 & -4\\end{bmatrix}\n\\]\nThis matrix is called Adjugate or Adjoint, which is simple the transpose of the cofactor matrix.\nStep 4: Multiply transpose of cofactor by \\(\\frac{1}{determinant}\\)\nAs we did all the necessary operations to have determinant, let compute it firstly and then multiply the adjoint matrix by \\(\\frac{1}{determinant}\\).\nUsing formula:\n\\[\ndet(A) = \\sum_{i\\prime=1}^{3}a_{i\\prime j}C_{i\\prime j}\n\\ =\na_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\n\\]\nWe have:\n\\[\ndet(A) = (4 \\times 8) + (2 \\times (-40)) + (2 \\times 8) = -32\n\\]\nNow the inverse is:\n\\[\nA^{-1} =\n\\frac{1}{-32}\n\\cdot\n\\begin{bmatrix}\n    8 & -40 & 8 \\\\\n    -12 & 28 & -4 \\\\\n    4 & -4 & -4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    -0.25 & 0.375 & -0.125 \\\\\n    1.25 & 0.875 & 0.125 \\\\\n    -0.25 & 0.125 & 0.125\n\\end{bmatrix}\n\\]\nLet’s verify that the original matrix multiplied by the inverse results in an identity matrix:\n\\[\nA \\cdot A^{-1} =\n\\begin{bmatrix}4 & 2 & 2\\\\6 & 2 & 4\\\\2 & 2 & 8\\end{bmatrix} \\cdot \\begin{bmatrix}-0.25 & 0.375 & -0.125\\\\1.25 & -0.875 & 0.125\\\\-0.25 & 0.125 & 0.125\\end{bmatrix} =\n\\begin{bmatrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{bmatrix} = I\n\\]\nDo you see how challenging can be finding the inverse of \\(4 \\times 4\\) matrix? That’s why we use calculators or computer program to compute it.\n\nA = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\ninverse = np.linalg.inv(A)\nprint(\"Inverse\", inverse, sep=\"\\n\")\n\nInverse\n[[-0.25   0.375 -0.125]\n [ 1.25  -0.875  0.125]\n [-0.25   0.125  0.125]]\n\n\n\n\n\nAs we already know how to compute the inverse of a matrix, the division is easy now. If we have two matrices:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nand\n\\[\nB =\n\\begin{bmatrix}\n    4 & 2 & 2 \\\\\n    6 & 2 & 4 \\\\\n    2 & 2 & 8\n\\end{bmatrix}\n\\]\nthen \\(A\\) divided by \\(B\\) is\n\\[\nA \\cdot B^{-1}\n\\ =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    -0.25 & 0.375 & -0.125 \\\\\n    1.25 & 0.875 & 0.125 \\\\\n    -0.25 & 0.125 & 0.125\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    1.5 & -1 & 0.5 \\\\\n    3.75 & -2.125 & 0.875 \\\\\n    6 & -3.25 & 1.25\n\\end{bmatrix}\n\\ \\equiv\nA \\div B\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nB = np.array([[4, 2, 2], [6, 2, 4], [2, 2, 8]])\n\n# A divided by B is dot product between A and inverse of B\ninv_B = np.linalg.inv(B)\nX = np.dot(A, inv_B)\n\nprint(X)\n\n[[ 1.5   -1.     0.5  ]\n [ 3.75  -2.125  0.875]\n [ 6.    -3.25   1.25 ]]\n\n\n\n\n\nIn the previous blog, I talked about the system of linear equations and we solved this system graphically and algebraically. One of the great things about matrices, is that they can help us solve systems of equations. For example, consider the following system of equations:\n\\[\n\\begin{cases}\n2x + 4y = 18 \\\\\n6x + 2y = 34\n\\end{cases}\n\\]\nWe can write this in matrix form, like this:\n\\[\n\\begin{bmatrix}\n    2 & 4 \\\\\n    6 & 2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    x \\\\\n    y\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\]\nIf we calculate the dot product between the matrix and vector on the left side, we can see clearly that this represents the original equations.\nNow let rename our matrices:\n\\[\nA =\n\\begin{bmatrix}\n    2 & 4 \\\\\n    6 & 2\n\\end{bmatrix}\n\\quad\nX =\n\\begin{bmatrix}\n    x \\\\\n    y\n\\end{bmatrix}\n\\quad\nB =\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\]\nThis can be represented as \\(AX = B\\) and we know that to find \\(X\\) we have to solve this: \\(B \\div A\\). Since we cannot divide matrices in this way, we have to use the previous technique. Find the inverse of \\(A\\) and multiply by \\(B\\).\nThe inverse of \\(A\\):\n\\[\nA^{-1} =\n\\begin{bmatrix}\n    -0.1 & 0.2 \\\\\n    0.3 & -0.1\n\\end{bmatrix}\n\\]\n\\[\nX =\n\\begin{bmatrix}\n    -0.1 & 0.2 \\\\\n    0.3 & -0.1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    18 \\\\\n    34\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    5 \\\\\n    2\n\\end{bmatrix}\n\\]\nNow, instead of \\(x\\) and \\(y\\) in the original equation put \\(5\\) and \\(2\\) and this will make equality true.\n\\[\n10 + 8 = 18\n\\] \\[\n30 + 4 = 34\n\\]\n\nA = np.array([[2, 4], [6, 2]])\n\nB = np.array([[18], [34]])\n\nA_inverse = np.linalg.inv(A)\n\nprint(\"The inverse of A is\", A_inverse, sep=\"\\n\")\n\nX = np.dot(A_inverse, B)\n\nprint(\"X =\", X, sep=\"\\n\")\n\nThe inverse of A is\n[[-0.1  0.2]\n [ 0.3 -0.1]]\nX =\n[[5.]\n [2.]]\n\n\n\n\n\nElementary Row Operations (ERO) play an important role in many matrix algebra applications, such as finding the inverse of a matrix and solving simultaneous linear equations. These topics are covered in advance part of the series. An ERO transforms a given matrix \\(A\\) into a new matrix \\(A^{'}\\) via one of the following operations:\n\nInterchange two rows (or columns)\nMultiply each element in a row (or column) by a non-zero number\nMultiply a row (or column) by a non-zero number and add the result to another row (or column)\n\nTo catch the idea behind this operations let do the example. We have a matrix \\(A\\) such that\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    1 & 3 & 5 & 6 \\\\\n    0 & 1 & 2 & 3\n\\end{bmatrix}\n\\]\nType 1 ERO that interchange rows 1 and 3 of \\(A\\) would yield\n\\[\nA^{'} =\n\\begin{bmatrix}\n    0 & 1 & 2 & 3 \\\\\n    1 & 3 & 5 & 6 \\\\\n    1 & 2 & 3 & 4\n\\end{bmatrix}\n\\]\nType 2 ERO that multiplies row 2 of \\(A\\) by 3 would yield\n\\[\nA^{'} =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    3 & 9 & 15 & 18 \\\\\n    0 & 1 & 2 & 3\n\\end{bmatrix}\n\\]\nType 3 ERO that multiplies row 2 of \\(A\\) by 4 and replace row 3 of \\(A\\) by \\(4 \\times (\\text{row 2 of A}) + \\text{row 3 of A}\\),\nwould yield row 3 of \\(A^{'}\\) to be\n\\(4 \\times [1 \\ 3 \\ 5 \\ 6] + [0 \\ 1 \\ 2 \\ 3] = [4 \\ 13 \\ 22 \\ 27]\\)\nand\n\\[\nA^{'} =\n\\begin{bmatrix}\n    1 & 2 & 3 & 4 \\\\\n    1 & 3 & 5 & 6 \\\\\n    4 & 13 & 22 & 27\n\\end{bmatrix}\n\\]\nExcept this, to perform an elementary row operation on the matrix \\(A\\), first we can perform the operation on the corresponding identity matrix to obtain an elementary matrix, then multiply \\(A\\) on the left by this elementary matrix. More precisely this means that we take one ERO, whichever we want and perform this operation on corresponding identity matrix of \\(A\\). If \\(A\\) has \\(m \\times n\\) dimension we have identity matrix \\(I_{m \\times n}\\). After that, we multiply \\(A\\) by this identity matrix. If we denote the elementary matrix by \\(E\\) then, we multiply \\(A\\) by \\(E\\) in the following way:\n\\[\n\\begin{equation}E_{1} \\cdot A \\end{equation}\n\\]\nwhere \\(E_{1}\\) is ERO one performed on identity matrix.\n\n\n\nThe maximum number of linearly independent rows in a matrix \\(A\\) is called the row rank of \\(A\\) and the maximum number of linearly independent columns in \\(A\\) is called the column rank of \\(A\\). If \\(A\\) is \\(n \\times m\\) matrix, that is if matrix \\(A\\) has \\(m\\) rows and \\(n\\) columns then the following inequality holds:\n\\[\n\\text{row rank of} \\ A \\leq m \\\\\n\\text{column rank of} \\ A \\leq n\n\\]\nFurthermore, for any matrix \\(A\\)\n\\[\n\\text{row rank of} \\ A = \\text{column rank of} \\ A\n\\]\nFrom the above inequality it follows that\n\\[\nRank(A) \\leq min(m, n)\n\\]\nThis means that if a matrix has, for example, 3 rows and 5 columns, its rank cannot be more than 3. The rank of a matrix would be zero if and only if the matrix had no elements. If a matrix had even one element, its minimum rank would be one. When all of the vectors in a matrix are linearly independent, the matrix is said to be full rank.\nTo calculate the rank of a matrix, we have to compute the determinant. It turns out that the rank of a matrix \\(A\\), denoted by \\(Rank(A)\\) is the size of the largest non-zero \\(m \\times m\\) sub-matrix with non-zero determinant. To simplify further, if the determinant of \\(4 \\times 4\\) matrix \\(A\\) is zero and any \\(3 \\times3\\) sub-matrix of original matrix \\(A\\) has non-zero determinant then the rank of the original matrix \\(A\\) is \\(3\\). So we can say that rank shows the “non-degenerateness” of the matrix \\(A\\).\nActually, there is no only one way to compute the rank. I will provide one more way in the advanced tutorial.\n\nA = np.array([[1, 2, 3], [2, 4, 6], [1, -3, 5]])\n\nrank_A = np.linalg.matrix_rank(A)\n\nprint(\"Rank(A) = \", rank_A)\n\n\n# matrix B has full rank\nB = np.array([[2, 2, -1], [4, 0, 2], [0, 6, -3]])\n\nrank_B = np.linalg.matrix_rank(B)\n\nprint(\"Rank(B) = \", rank_B)\n\nRank(A) =  2\nRank(B) =  3\n\n\n\n\n\nWe can rise a square matrix \\(A\\) in any nonnegative power just like any number. This is defined as the product of \\(A\\) by itself \\(n\\) times. If matrix \\(A\\) has inverse, then \\(A^{-n} = (A^{-1})^{n}\\) or take inverse of \\(A\\) and multiply by itself \\(n\\) times.\nFor example, if\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\]\nthen\n\\[\nA^{2} =\nA A\n\\ =\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4\n\\end{bmatrix}\n\\ =\n\\begin{bmatrix}\n    7 & 10 \\\\\n    15 & 22\n\\end{bmatrix}\n\\]\n\nA = np.array([[1, 2], [3, 4]])\n\nA_square = np.linalg.matrix_power(A, 2)\nprint(A_square)\n\n[[ 7 10]\n [15 22]]\n\n\n\n\n\nIn the previous post I talked about vector norms but did not mentioned matrix norm. There are three types of matrix norms:\n\nMatrix norms induced by vector norms\nEntrywise matrix norms\nSchatten norms\n\nHere, I will introduce only the first two types of matrix norm and depict one example of each to give the general idea of matrix norms.\nInduced norms usually are denoted by: \\[\\|A\\|_p\\]\nIn the special cases of \\(p = 1, 2, \\infty\\), the induced matrix norms can be computed by:\n\\[\n\\|A\\|_1 = max_{1\\leq j \\leq m} \\sum_{i = 1}^{m}|a_{ij}|\n\\]\nWhich is the maximum absolute column sum of the matrix \\(A\\)\n\\[\n\\|A\\|_2 = \\|A\\|_F = \\sigma_{max}(A)\n\\]\nWhere \\(\\|A\\|_F\\) is Frobenius Norm, which will be discussed below and \\(\\sigma_{max}(A)\\) is the spectral norm. The later will be discussed in the next post.\n\\[\n\\|A\\|_{\\infty} = max_{1\\leq i \\leq m} \\sum_{j = 1}^{n}|a_{ij}|\n\\]\nwhich is the maximum absolute row sum of the matrix \\(A\\).\nTo clarify this farther, let consider the following example:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\n\\[\n\\|A\\|_1 = max(|1| + |4| + |7|; |2| + |5| + |8|; |3| + |6| + |9|) = max(12; 15; 18) = 18\n\\] \\[\n\\quad\n\\] \\[\n\\|A\\|_{\\infty} = max(|1| + |2| + |3|; |4| + |5| + |6|; |7| + |8| + |9|) = max(6; 15; 24) = 24\n\\]\nImagine, we have a vector whose elements are matrices instead of scalars. Then norm defined here is entrywise matrix norm. The general formula for entrywise matrix norm is:\n\\[\n\\|A\\|_{p,q} = \\left(\\sum_{j=1}^{n}\\left(\\sum_{i=1}^{m}|a_{ij}^{p}\\right)^\\frac{q}{p}\\right)^\\frac{1}{q}\n\\]\nwhere \\(p,q \\geq 1\\)\nWhen \\(p=q=2\\) we have Frobenius Norm or Frobenius Inner Product:\n\\[\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^{n}|a_{ij}|^2}\n\\]\nand when \\(p=q=\\infty\\), we have Max Norm:\n\\[\n\\|A\\|_{max} = max_{ij}|a_{ij}|\n\\]\nIf we have matrix \\(A\\) such that:\n\\[\nA =\n\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\n\\end{bmatrix}\n\\]\nThen Frobenius Norm is:\n\\[\n\\|A\\|_{F} = \\sqrt{(|1|^2 + |2|^2 + |3|^2 + |4|^2 + |5|^2 + |6|^2 + |7|^2 + |8|^2 + |9|^2)} =\n\\]\n\\[\n\\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81} = \\sqrt{285} \\approx 16.89\n\\]\nFor two matrices \\(A\\) and \\(B\\) we have Frobenius Inner Product:\n\\[\n\\langle A,B \\rangle_{F} = \\sum_{i,j}\\overline{A_{i,j}}B_{i,j} = tr \\left(\\overline{A^T}B \\right)\n\\]\nWhere overline denotes the complex conjugate of a matrix.\nIf\n\\[\nA =\n\\begin{bmatrix}\n    2 & 0 \\\\\n    1 & 1 \\\\\n\\end{bmatrix}\n\\]\nand\n\\[\nB =\n\\begin{bmatrix}\n    8 & -3 \\\\\n    4 & 1 \\\\\n\\end{bmatrix}\n\\]\nthen\n\\[\n\\langle A,B \\rangle_{F} = 2 \\cdot 8 + 0 \\cdot (-3) + 1 \\cdot 4 + 1 \\cdot 1 = 21\n\\]\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nfrobenius_norm = np.linalg.norm(A, ord=\"fro\")\ncolumn_max_norm = np.linalg.norm(A, ord=1)\nrow_max_norm = np.linalg.norm(A, ord=np.inf)  # same as infinity norm\n\n# Frobenius Inner Product\nA = np.array([[2, 0], [1, 1]])\nB = np.array([[8, -3], [4, 1]])\n\n# numpy.vdot function flattens high dimensional arrays and takes dot product\nfrobenius_inner_product = np.vdot(A, B)\n\nprint(\"Frobenius Norm is: \", frobenius_norm)\nprint(\"Column Max Norm is: \", column_max_norm)\nprint(\"Row Max Norm is: \", row_max_norm)\nprint(\"Frobenius Inner Product is: \", frobenius_inner_product)\n\nFrobenius Norm is:  16.881943016134134\nColumn Max Norm is:  18.0\nRow Max Norm is:  24.0\nFrobenius Inner Product is:  21"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html",
    "href": "posts/mathematics/linear_algebra_6.html",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "Introduction to Linear Algebra with Python\nBasic Linear Algebra with Python\nIntermediate linear algebra\n\nIntermediate Linear Algebra with Python - Part I\nIntermediate Linear Algebra with Python - Part II\n\nAdvanced linear algebra\n\nAdvance Linear Algebra with Python - Part I\nAdvance Linear Algebra with Python - Part II\n\n\nIn this post I will introduce different types of matrix decompositions, mostly applicable to machine learning or deep learning.\nMatrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions.\n\n\nThe Cholesky Decomposition is the factorization of a given symmetric square matrix \\(A\\) into the product of a lower triangular matrix, denoted by \\(L\\) and its transpose \\(L^{T}\\). This decomposition is named after French artillery officer Andre-Louis Cholesky. The formula is:\n\\[\nA =\nLL^{T}\n\\]\nFor rough sense, let \\(A\\) be\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nthen we can represent \\(A\\) as\n\\[\nA = LL^{T} =\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0 \\\\\nl_{31} & l_{32} & l_{33}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nl_{11} & l_{12} & l_{13} \\\\\n0 & l_{22} & l_{23} \\\\\n0 & 0 & a_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\nl_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\nl_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\nl_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n\\end{bmatrix}\n\\]\nThe diagonal elements of matrix \\(L\\) can be calculated by the following formulas:\n\\[\nl_{11} = \\sqrt{a_{11}}\n\\quad \\quad\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n\\quad \\quad\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n\\]\nand in general, for diagonal elements of the matrix \\(L\\) we have:\n\\[\nl_{kk} =\n\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n\\]\nFor the elements below the main diagonal, \\(l_{ik}\\) where \\(i &gt; k\\), the formulas are\n\\[\nl_{21} = \\frac{1}{l_{11}}a_{21}\n\\quad \\quad\nl_{31} = \\frac{1}{l_{11}}a_{31}\n\\quad \\quad\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n\\]\nand the general formula is\n\\[\nl_{ik} =\n\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n\\]\nMessy formulas! Consider a numerical example to see what happen under the hood. We have a matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n25 & 15 & -5 \\\\\n15 & 18 & 0 \\\\\n-5 & 0 & 11\n\\end{bmatrix}\n\\]\nAccording to the above formulas, let find a lower triangular matrix \\(L\\). We have\n\\[\nl_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n\\] \\[\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n\\] \\[\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n\\]\nSeems, we have missing non-diagonal elements, which are\n\\[\nl_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n\\] \\[\nl_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n\\] \\[\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n\\]\nSo, our matrix \\(L\\) is\n\\[\nL =\n\\begin{bmatrix}\n5 & 0 & 0 \\\\\n3 & 3 & 0 \\\\\n-1 & 1 & 3\n\\end{bmatrix}\n\\quad \\quad\nL^{T} =\n\\begin{bmatrix}\n5 & 3 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\]\nMultiplication of this matrices is up to you.\n\nimport numpy as np\n\n\nA = np.array([[25, 15, -5], [15, 18, 0], [-5, 0, 11]])\n\n# Cholesky decomposition, find lower triangular matrix L\nL = np.linalg.cholesky(A)\n\n# Take transpose\nL_T = np.transpose(L)\n\n# Check if it's correct\nA == np.dot(L, L_T)\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\n\n\nQR decomposition is another type of matrix factorization, where a given \\(m \\times n\\) matrix \\(A\\) is decomposed into two matrices, \\(Q\\) which is orthogonal matrix, which in turn means that \\(QQ^{T} = Q^{T}Q = I\\) and the inverse of \\(Q\\) equal to its transpose, \\(Q^{T} = Q^{-1}\\), and \\(R\\) which is upper triangular matrix. Hence, the formula is given by\n\\[\nA =\nQR\n\\]\nAs \\(Q\\) is an orthogonal matrix, there are three methods to find \\(Q\\), one is Gramm-Schmidt Process, second is Householder Transformation, and third is Givens Rotation. These methods are out of the scope of this blog post series and hence I’m going to explain all of them in a separate blog post.\n\nA = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n\n# QR decomposition\nQ, R = np.linalg.qr(A)\n\nprint(\"Q = \", Q, sep=\"\\n\")\nprint(\"R = \", R, sep=\"\\n\")\nprint(\"A = QR\", np.dot(Q, R), sep=\"\\n\")\n\nQ = \n[[-0.85714286  0.39428571  0.33142857]\n [-0.42857143 -0.90285714 -0.03428571]\n [ 0.28571429 -0.17142857  0.94285714]]\nR = \n[[ -14.  -21.   14.]\n [   0. -175.   70.]\n [   0.    0.  -35.]]\nA = QR\n[[ 12. -51.   4.]\n [  6. 167. -68.]\n [ -4.  24. -41.]]\n\n\n\n\n\nHere is the question. What’s the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n\\[\nA = Q \\Lambda Q^{-1}\n\\]\n\\(A\\) is \\(n\\times n\\) square matrix, \\(Q\\) is the matrix whose columns are the eigenvectors, which in turn are linearly independent and \\(\\Lambda\\) is diagonal matrix of eigenvalues of \\(A\\) and these eigenvalues are not necessarily distinct.\nTo see the detailed steps of this decomposition, consider the aforementioned example of the matrix \\(A\\) for which we already found eigenvalues and eigenvectors.\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\] \\[\nQ =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n-2 & 0 & 1 \\\\\n1 & 0 & 2\n\\end{bmatrix}\n\\] \\[\n\\Lambda =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 11\n\\end{bmatrix}\n\\] \\[\nQ^{-1} =\n\\begin{bmatrix}\n0 & -0.4 & 0.2 \\\\\n1 & 0 & 0 \\\\\n0 & 0.2 & 0.4\n\\end{bmatrix}\n\\]\nWe have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply \\(Q\\) by \\(\\Lambda\\) and by \\(Q^{-1}\\). We have to get original matrix \\(A\\)\nFurthermore, if matrix \\(A\\) is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n\\[\nA = Q \\Lambda Q^{T}\n\\]\nThe only difference between this formula and above formula is that the matrix \\(A\\) is \\(n\\times n\\) real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n\\[\nA =\n\\begin{bmatrix}\n6 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n\\]\nThe matrix is symmetric because of the original matrix equal to its transpose, \\(A = A^{T}\\)\nIts eigenvalues are \\(\\lambda_{1} = 7\\) and \\(\\lambda_{2} = 2\\) and corresponding eigenvectors are\n\\[\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n0.89442719 \\\\\n0.4472136\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n-0.4472136 \\\\\n0.89442719\n\\end{bmatrix}\n\\]\nAnd in this set up, matrices \\(Q\\), \\(\\Lambda\\) and \\(Q^{T}\\) are the following:\n\\[\nQ =\n\\begin{bmatrix}\n0.89442719 & -0.4472136 \\\\\n0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n\\] \\[\n\\Lambda =\n\\begin{bmatrix}\n7 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n\\] \\[\nQ^{T} =\n\\begin{bmatrix}\n0.89442719 & 0.4472136 \\\\\n-0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n\\]\nTaking matrix product gives initial matrix \\(A\\).\nEigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.\n\n# Eigendecomposition for non-symmetric matrix\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\neigenvalues1, eigenvectors1 = np.linalg.eig(A)\n# Form diagonal matrix from eigenvalues\nL1 = np.diag(eigenvalues1)\n\n# Separate eigenvector matrix and take its inverse\nQ1 = eigenvectors1\ninv_Q = np.linalg.inv(Q1)\nB = np.dot(np.dot(Q1, L1), inv_Q)\n\n# Check if B equal to A\nprint(\"Decomposed matrix B: \", B, sep=\"\\n\")\n\n# Numpy produces normalized eigenvectors and don't be confused with my calculations above\n\n\n# Eigendecomposition for symmetric matrix\nC = np.array([[6, 2], [2, 3]])\neigenvalues2, eigenvectors2 = np.linalg.eig(C)\n\n# Eigenvalues and Eigenvectors\nL2 = np.diag(eigenvalues2)\nQ2 = eigenvectors2\nQ2_T = Q2.T\n\nD = np.dot(np.dot(Q2, L2), Q2.T)\n\n# Check if D equal to C\nprint(\"Decomposed matrix D: \", D, sep=\"\\n\")\n\nDecomposed matrix B: \n[[2. 0. 0.]\n [0. 3. 4.]\n [0. 4. 9.]]\nDecomposed matrix D: \n[[6. 2.]\n [2. 3.]]\n\n\n\n\n\nSingular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square \\(n \\times n\\) matrices, while Singular Value Decomposition (SVD) is applicable for any \\(m \\times n\\) matrices.\nSVD for a \\(m \\times n\\) matrix \\(A\\) is computed by the following formula:\n\\[\nA = U \\ D \\ V^{T}\n\\]\nWhere, \\(U\\)’s columns are left singular vectors of \\(A\\), \\(V\\)’s columns are right singular vectors of \\(A\\) and \\(D\\) is a diagonal matrix, not necessarily square matrix, containing singular values of \\(A\\) on main diagonal. Singular values of \\(m \\times n\\) matrix \\(A\\) are the square roots of the eigenvalues of \\(A^{T}A\\), which is a square matrix. If our initial matrix \\(A\\) is square or \\(n \\times n\\) then singular values coincide eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\nMatrices, \\(U\\), \\(D\\), and \\(V\\) can be found by transforming \\(A\\) into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying \\(A\\) by its transpose \\(A^{T}\\). After that, matrices \\(U\\), \\(D\\) and \\(V\\) are the following:\n\n\\(U\\) corresponds to the eigenvectors of \\(AA^{T}\\)\n\\(V\\) corresponds to eigenvectors of \\(A^{T}A\\)\n\\(D\\) corresponds to eigenvalues, either \\(AA^{T}\\) or \\(A^{T}A\\), which are the same\n\nTheory almost always seems confusing. Consider a numerical example and Python code below for clarification.\nLet our initial matrix \\(A\\) be:\n\\[\nA =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n\\sqrt{2} & 2 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\]\nHere, to use SVD first we need to find \\(AA^{T}\\) and \\(A^{T}A\\).\n\\[\nAA^{T} =\n\\begin{bmatrix}\n2 & 2 & 2 \\\\\n2 & 6 & 2 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\quad\nA^{T}A =\n\\begin{bmatrix}\n2 & 2\\sqrt{2} & 0 \\\\\n2\\sqrt{2} & 6 & 2 \\\\\n0 & 2 & 2\n\\end{bmatrix}\n\\]\nIn the next step, we have to find eigenvalues and eigenvectors for \\(AA^{T}\\) and \\(A^{T}A\\). The characteristic polynomial is\n\\[\n-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n\\]\nwith roots equal to \\(\\lambda_{1} = 8\\), \\(\\lambda_{2} = 2\\), and \\(\\lambda_{3} = 0\\). Note that these eigenvalues are the same for the \\(A^{T}A\\). We need singular values which are square root from eigenvalues. Let denote them by \\(\\sigma\\) such as \\(\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}\\), \\(\\sigma_{2} = \\sqrt{2}\\) and \\(\\sigma_{3} = \\sqrt{0} = 0\\). We now can construct diagonal matrix of singular values:\n\\[\nD =\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nNow we have to find matrices \\(U\\) and \\(V\\). We have everything what we need. First find eigenvectors of \\(AA^{T}\\) for \\(\\lambda_{1} = 8\\), \\(\\lambda_{2} = 2\\), and \\(\\lambda_{3} = 0\\), which are the following:\n\\[\nU_{1} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}}\\\\\n\\frac{2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{6}}\n\\end{bmatrix}\n\\quad\nU_{2} =\n\\begin{bmatrix}\n-\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}} \\\\\n-\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n\\quad\nU_{3} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}\\\\\n0 \\\\\n-\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\nNote that eigenvectors are normalized.\nAs we have eigenvectors, our \\(U\\) matrix is:\n\\[\nU =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\nIn the same fashion, we can find matrix \\(V\\), which is:\n\\[\nV =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n\\]\nAccording to the formula we have\n\\[\nA = U \\ D \\ V^{T} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n^{T} = A\n\\]\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nA = np.array([[0, 1, 0], [np.sqrt(2), 2, 0], [0, 1, 1]])\nU, D, V = np.linalg.svd(A)\n\nprint(\"U = \", U, sep=\"\\n\")\nprint(\"D = \", D, sep=\"\\n\")\nprint(\"V = \", V, sep=\"\\n\")\n\nB = np.dot(U, np.dot(np.diag(D), V))\nprint(\"B = \", B, sep=\"\\n\")\n\nU = \n[[-0.32099833  0.14524317 -0.93587632]\n [-0.87192053 -0.43111301  0.23215547]\n [-0.36974946  0.8905313   0.26502706]]\nD = \n[2.75398408 1.09310654 0.46977627]\nV = \n[[-0.44774472 -0.8840243  -0.13425984]\n [-0.55775521  0.15876626  0.81467932]\n [ 0.69888038 -0.43965249  0.56415592]]\nB = \n[[ 0.          1.         -0.        ]\n [ 1.41421356  2.          0.        ]\n [ 0.          1.          1.        ]]\n\n\n\n\n\nHere, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let’s get started. If a matrix \\(A\\) can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by:\n\\[\nA^{-1} =\nQ \\Lambda^{-1} Q^{-1}\n\\]\nMatrices, \\(Q\\), and \\(\\Lambda\\) are already known for us. Consider an example:\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n4 & 3\n\\end{bmatrix}\n\\]\nIts eigenvalues are \\(\\lambda_{1} = -1\\) and \\(\\lambda_{2} = 5\\) and eigenvectors are:\n\\[\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n-0.70710678 \\\\\n0.70710678\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n0.4472136 \\\\\n-0.89442719\n\\end{bmatrix}\n\\]\nLet calculate the inverse of \\(A\\)\n\\[\nA^{-1} = Q \\Lambda^{-1} Q^{-1} =\n\\] \\[\n=\n\\begin{bmatrix}\n-0.70710678 & -0.4472136 \\\\\n0.70710678 & -0.89442719\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-1 & -0 \\\\\n0 & 0.2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-0.94280904 & 0.47140452 \\\\\n-0.74535599 & -0.74535599\n\\end{bmatrix} =\n\\] \\[\n=\n\\begin{bmatrix}\n-0.6 & 0.4 \\\\\n0.8 & -0.2\n\\end{bmatrix}\n\\]\n\nA = np.array([[1, 2], [4, 3]])\n\n# Eigenvalues and Eigenvectors\nL, Q = np.linalg.eig(A)\n# Diagonal eigenvalues\nL = np.diag(L)\n# Inverse\ninv_L = np.linalg.inv(L)\n# Inverse of igenvector matrix\ninv_Q = np.linalg.inv(Q)\n# Calculate the inverse of A\ninv_A = np.dot(Q, np.dot(inv_L, inv_Q))\n\n# Print the inverse\nprint(\"The inverse of A is: \", inv_A, sep=\"\\n\")\n\nThe inverse of A is: \n[[-0.6  0.4]\n [ 0.8 -0.2]]"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html#cholesky-decomposition",
    "href": "posts/mathematics/linear_algebra_6.html#cholesky-decomposition",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "The Cholesky Decomposition is the factorization of a given symmetric square matrix \\(A\\) into the product of a lower triangular matrix, denoted by \\(L\\) and its transpose \\(L^{T}\\). This decomposition is named after French artillery officer Andre-Louis Cholesky. The formula is:\n\\[\nA =\nLL^{T}\n\\]\nFor rough sense, let \\(A\\) be\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nthen we can represent \\(A\\) as\n\\[\nA = LL^{T} =\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0 \\\\\nl_{31} & l_{32} & l_{33}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nl_{11} & l_{12} & l_{13} \\\\\n0 & l_{22} & l_{23} \\\\\n0 & 0 & a_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\nl_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\nl_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\nl_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n\\end{bmatrix}\n\\]\nThe diagonal elements of matrix \\(L\\) can be calculated by the following formulas:\n\\[\nl_{11} = \\sqrt{a_{11}}\n\\quad \\quad\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n\\quad \\quad\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n\\]\nand in general, for diagonal elements of the matrix \\(L\\) we have:\n\\[\nl_{kk} =\n\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n\\]\nFor the elements below the main diagonal, \\(l_{ik}\\) where \\(i &gt; k\\), the formulas are\n\\[\nl_{21} = \\frac{1}{l_{11}}a_{21}\n\\quad \\quad\nl_{31} = \\frac{1}{l_{11}}a_{31}\n\\quad \\quad\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n\\]\nand the general formula is\n\\[\nl_{ik} =\n\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n\\]\nMessy formulas! Consider a numerical example to see what happen under the hood. We have a matrix \\(A\\)\n\\[\nA =\n\\begin{bmatrix}\n25 & 15 & -5 \\\\\n15 & 18 & 0 \\\\\n-5 & 0 & 11\n\\end{bmatrix}\n\\]\nAccording to the above formulas, let find a lower triangular matrix \\(L\\). We have\n\\[\nl_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n\\] \\[\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n\\] \\[\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n\\]\nSeems, we have missing non-diagonal elements, which are\n\\[\nl_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n\\] \\[\nl_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n\\] \\[\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n\\]\nSo, our matrix \\(L\\) is\n\\[\nL =\n\\begin{bmatrix}\n5 & 0 & 0 \\\\\n3 & 3 & 0 \\\\\n-1 & 1 & 3\n\\end{bmatrix}\n\\quad \\quad\nL^{T} =\n\\begin{bmatrix}\n5 & 3 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\]\nMultiplication of this matrices is up to you.\n\nimport numpy as np\n\n\nA = np.array([[25, 15, -5], [15, 18, 0], [-5, 0, 11]])\n\n# Cholesky decomposition, find lower triangular matrix L\nL = np.linalg.cholesky(A)\n\n# Take transpose\nL_T = np.transpose(L)\n\n# Check if it's correct\nA == np.dot(L, L_T)\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html#qr-decomposition",
    "href": "posts/mathematics/linear_algebra_6.html#qr-decomposition",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "QR decomposition is another type of matrix factorization, where a given \\(m \\times n\\) matrix \\(A\\) is decomposed into two matrices, \\(Q\\) which is orthogonal matrix, which in turn means that \\(QQ^{T} = Q^{T}Q = I\\) and the inverse of \\(Q\\) equal to its transpose, \\(Q^{T} = Q^{-1}\\), and \\(R\\) which is upper triangular matrix. Hence, the formula is given by\n\\[\nA =\nQR\n\\]\nAs \\(Q\\) is an orthogonal matrix, there are three methods to find \\(Q\\), one is Gramm-Schmidt Process, second is Householder Transformation, and third is Givens Rotation. These methods are out of the scope of this blog post series and hence I’m going to explain all of them in a separate blog post.\n\nA = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n\n# QR decomposition\nQ, R = np.linalg.qr(A)\n\nprint(\"Q = \", Q, sep=\"\\n\")\nprint(\"R = \", R, sep=\"\\n\")\nprint(\"A = QR\", np.dot(Q, R), sep=\"\\n\")\n\nQ = \n[[-0.85714286  0.39428571  0.33142857]\n [-0.42857143 -0.90285714 -0.03428571]\n [ 0.28571429 -0.17142857  0.94285714]]\nR = \n[[ -14.  -21.   14.]\n [   0. -175.   70.]\n [   0.    0.  -35.]]\nA = QR\n[[ 12. -51.   4.]\n [  6. 167. -68.]\n [ -4.  24. -41.]]"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html#eigen-decomposition",
    "href": "posts/mathematics/linear_algebra_6.html#eigen-decomposition",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "Here is the question. What’s the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n\\[\nA = Q \\Lambda Q^{-1}\n\\]\n\\(A\\) is \\(n\\times n\\) square matrix, \\(Q\\) is the matrix whose columns are the eigenvectors, which in turn are linearly independent and \\(\\Lambda\\) is diagonal matrix of eigenvalues of \\(A\\) and these eigenvalues are not necessarily distinct.\nTo see the detailed steps of this decomposition, consider the aforementioned example of the matrix \\(A\\) for which we already found eigenvalues and eigenvectors.\n\\[\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n\\] \\[\nQ =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n-2 & 0 & 1 \\\\\n1 & 0 & 2\n\\end{bmatrix}\n\\] \\[\n\\Lambda =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 11\n\\end{bmatrix}\n\\] \\[\nQ^{-1} =\n\\begin{bmatrix}\n0 & -0.4 & 0.2 \\\\\n1 & 0 & 0 \\\\\n0 & 0.2 & 0.4\n\\end{bmatrix}\n\\]\nWe have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply \\(Q\\) by \\(\\Lambda\\) and by \\(Q^{-1}\\). We have to get original matrix \\(A\\)\nFurthermore, if matrix \\(A\\) is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n\\[\nA = Q \\Lambda Q^{T}\n\\]\nThe only difference between this formula and above formula is that the matrix \\(A\\) is \\(n\\times n\\) real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n\\[\nA =\n\\begin{bmatrix}\n6 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n\\]\nThe matrix is symmetric because of the original matrix equal to its transpose, \\(A = A^{T}\\)\nIts eigenvalues are \\(\\lambda_{1} = 7\\) and \\(\\lambda_{2} = 2\\) and corresponding eigenvectors are\n\\[\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n0.89442719 \\\\\n0.4472136\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n-0.4472136 \\\\\n0.89442719\n\\end{bmatrix}\n\\]\nAnd in this set up, matrices \\(Q\\), \\(\\Lambda\\) and \\(Q^{T}\\) are the following:\n\\[\nQ =\n\\begin{bmatrix}\n0.89442719 & -0.4472136 \\\\\n0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n\\] \\[\n\\Lambda =\n\\begin{bmatrix}\n7 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n\\] \\[\nQ^{T} =\n\\begin{bmatrix}\n0.89442719 & 0.4472136 \\\\\n-0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n\\]\nTaking matrix product gives initial matrix \\(A\\).\nEigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.\n\n# Eigendecomposition for non-symmetric matrix\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\neigenvalues1, eigenvectors1 = np.linalg.eig(A)\n# Form diagonal matrix from eigenvalues\nL1 = np.diag(eigenvalues1)\n\n# Separate eigenvector matrix and take its inverse\nQ1 = eigenvectors1\ninv_Q = np.linalg.inv(Q1)\nB = np.dot(np.dot(Q1, L1), inv_Q)\n\n# Check if B equal to A\nprint(\"Decomposed matrix B: \", B, sep=\"\\n\")\n\n# Numpy produces normalized eigenvectors and don't be confused with my calculations above\n\n\n# Eigendecomposition for symmetric matrix\nC = np.array([[6, 2], [2, 3]])\neigenvalues2, eigenvectors2 = np.linalg.eig(C)\n\n# Eigenvalues and Eigenvectors\nL2 = np.diag(eigenvalues2)\nQ2 = eigenvectors2\nQ2_T = Q2.T\n\nD = np.dot(np.dot(Q2, L2), Q2.T)\n\n# Check if D equal to C\nprint(\"Decomposed matrix D: \", D, sep=\"\\n\")\n\nDecomposed matrix B: \n[[2. 0. 0.]\n [0. 3. 4.]\n [0. 4. 9.]]\nDecomposed matrix D: \n[[6. 2.]\n [2. 3.]]"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html#singular-value-decomposition",
    "href": "posts/mathematics/linear_algebra_6.html#singular-value-decomposition",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square \\(n \\times n\\) matrices, while Singular Value Decomposition (SVD) is applicable for any \\(m \\times n\\) matrices.\nSVD for a \\(m \\times n\\) matrix \\(A\\) is computed by the following formula:\n\\[\nA = U \\ D \\ V^{T}\n\\]\nWhere, \\(U\\)’s columns are left singular vectors of \\(A\\), \\(V\\)’s columns are right singular vectors of \\(A\\) and \\(D\\) is a diagonal matrix, not necessarily square matrix, containing singular values of \\(A\\) on main diagonal. Singular values of \\(m \\times n\\) matrix \\(A\\) are the square roots of the eigenvalues of \\(A^{T}A\\), which is a square matrix. If our initial matrix \\(A\\) is square or \\(n \\times n\\) then singular values coincide eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\nMatrices, \\(U\\), \\(D\\), and \\(V\\) can be found by transforming \\(A\\) into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying \\(A\\) by its transpose \\(A^{T}\\). After that, matrices \\(U\\), \\(D\\) and \\(V\\) are the following:\n\n\\(U\\) corresponds to the eigenvectors of \\(AA^{T}\\)\n\\(V\\) corresponds to eigenvectors of \\(A^{T}A\\)\n\\(D\\) corresponds to eigenvalues, either \\(AA^{T}\\) or \\(A^{T}A\\), which are the same\n\nTheory almost always seems confusing. Consider a numerical example and Python code below for clarification.\nLet our initial matrix \\(A\\) be:\n\\[\nA =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n\\sqrt{2} & 2 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\]\nHere, to use SVD first we need to find \\(AA^{T}\\) and \\(A^{T}A\\).\n\\[\nAA^{T} =\n\\begin{bmatrix}\n2 & 2 & 2 \\\\\n2 & 6 & 2 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\quad\nA^{T}A =\n\\begin{bmatrix}\n2 & 2\\sqrt{2} & 0 \\\\\n2\\sqrt{2} & 6 & 2 \\\\\n0 & 2 & 2\n\\end{bmatrix}\n\\]\nIn the next step, we have to find eigenvalues and eigenvectors for \\(AA^{T}\\) and \\(A^{T}A\\). The characteristic polynomial is\n\\[\n-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n\\]\nwith roots equal to \\(\\lambda_{1} = 8\\), \\(\\lambda_{2} = 2\\), and \\(\\lambda_{3} = 0\\). Note that these eigenvalues are the same for the \\(A^{T}A\\). We need singular values which are square root from eigenvalues. Let denote them by \\(\\sigma\\) such as \\(\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}\\), \\(\\sigma_{2} = \\sqrt{2}\\) and \\(\\sigma_{3} = \\sqrt{0} = 0\\). We now can construct diagonal matrix of singular values:\n\\[\nD =\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nNow we have to find matrices \\(U\\) and \\(V\\). We have everything what we need. First find eigenvectors of \\(AA^{T}\\) for \\(\\lambda_{1} = 8\\), \\(\\lambda_{2} = 2\\), and \\(\\lambda_{3} = 0\\), which are the following:\n\\[\nU_{1} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}}\\\\\n\\frac{2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{6}}\n\\end{bmatrix}\n\\quad\nU_{2} =\n\\begin{bmatrix}\n-\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}} \\\\\n-\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n\\quad\nU_{3} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}\\\\\n0 \\\\\n-\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\nNote that eigenvectors are normalized.\nAs we have eigenvectors, our \\(U\\) matrix is:\n\\[\nU =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\]\nIn the same fashion, we can find matrix \\(V\\), which is:\n\\[\nV =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n\\]\nAccording to the formula we have\n\\[\nA = U \\ D \\ V^{T} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n^{T} = A\n\\]\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nA = np.array([[0, 1, 0], [np.sqrt(2), 2, 0], [0, 1, 1]])\nU, D, V = np.linalg.svd(A)\n\nprint(\"U = \", U, sep=\"\\n\")\nprint(\"D = \", D, sep=\"\\n\")\nprint(\"V = \", V, sep=\"\\n\")\n\nB = np.dot(U, np.dot(np.diag(D), V))\nprint(\"B = \", B, sep=\"\\n\")\n\nU = \n[[-0.32099833  0.14524317 -0.93587632]\n [-0.87192053 -0.43111301  0.23215547]\n [-0.36974946  0.8905313   0.26502706]]\nD = \n[2.75398408 1.09310654 0.46977627]\nV = \n[[-0.44774472 -0.8840243  -0.13425984]\n [-0.55775521  0.15876626  0.81467932]\n [ 0.69888038 -0.43965249  0.56415592]]\nB = \n[[ 0.          1.         -0.        ]\n [ 1.41421356  2.          0.        ]\n [ 0.          1.          1.        ]]"
  },
  {
    "objectID": "posts/mathematics/linear_algebra_6.html#inverse-of-a-square-full-rank-matrix",
    "href": "posts/mathematics/linear_algebra_6.html#inverse-of-a-square-full-rank-matrix",
    "title": "Advance Linear Algebra with Python - Part II",
    "section": "",
    "text": "Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let’s get started. If a matrix \\(A\\) can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by:\n\\[\nA^{-1} =\nQ \\Lambda^{-1} Q^{-1}\n\\]\nMatrices, \\(Q\\), and \\(\\Lambda\\) are already known for us. Consider an example:\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n4 & 3\n\\end{bmatrix}\n\\]\nIts eigenvalues are \\(\\lambda_{1} = -1\\) and \\(\\lambda_{2} = 5\\) and eigenvectors are:\n\\[\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n-0.70710678 \\\\\n0.70710678\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n0.4472136 \\\\\n-0.89442719\n\\end{bmatrix}\n\\]\nLet calculate the inverse of \\(A\\)\n\\[\nA^{-1} = Q \\Lambda^{-1} Q^{-1} =\n\\] \\[\n=\n\\begin{bmatrix}\n-0.70710678 & -0.4472136 \\\\\n0.70710678 & -0.89442719\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-1 & -0 \\\\\n0 & 0.2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-0.94280904 & 0.47140452 \\\\\n-0.74535599 & -0.74535599\n\\end{bmatrix} =\n\\] \\[\n=\n\\begin{bmatrix}\n-0.6 & 0.4 \\\\\n0.8 & -0.2\n\\end{bmatrix}\n\\]\n\nA = np.array([[1, 2], [4, 3]])\n\n# Eigenvalues and Eigenvectors\nL, Q = np.linalg.eig(A)\n# Diagonal eigenvalues\nL = np.diag(L)\n# Inverse\ninv_L = np.linalg.inv(L)\n# Inverse of igenvector matrix\ninv_Q = np.linalg.inv(Q)\n# Calculate the inverse of A\ninv_A = np.dot(Q, np.dot(inv_L, inv_Q))\n\n# Print the inverse\nprint(\"The inverse of A is: \", inv_A, sep=\"\\n\")\n\nThe inverse of A is: \n[[-0.6  0.4]\n [ 0.8 -0.2]]"
  },
  {
    "objectID": "posts/time_series_analysis/singular_spectrum_analysis.html",
    "href": "posts/time_series_analysis/singular_spectrum_analysis.html",
    "title": "Singular Spectrum Analysis with Python",
    "section": "",
    "text": "In this article, I will talk about Singular Spectrum Analysis (SSA) and its implementation in Python. For the sake of simplicity, I will use the univariate time series and will implement basic SSA without forecasting capability. However, it is not hard to extend this method to multivariate time series and add forecasting capability as well.\nIn Time Series Analysis (TSA), the technique called Singular Spectrum Analysis (SSA) has been developed to use it for time series decomposition, forecasting, and noise reduction. It is a nonparametric technique, combining some elements of classical time series analysis, multivariate statistics, dynamical systems, and signal processing. Since it is nonparametric, it works with arbitrary statistical processes, whether linear or nonlinear, stationary or non-stationary, Gaussian or non-Gaussian.\n\n\nLet dive into the technical details of the SSA. It consists of the two main steps: decomposition and reconstruction. Each above step, in turn, consists of the two sub-steps: embedding followed by singular value decomposition (SVD) and eigentriple grouping followed by diagonal averaging.\n\n\nAt that stage, we have to perform embedding followed by SVD. Let consider following time series: \\(\\mathbb{X} = (x_{1}, \\cdots, x_{N})\\) of length \\(N\\). Now let pick a window size \\(L\\) and embed the time series into a trajectory matrix of dimension \\(L \\times K\\), where \\(K = N - L + 1\\):\nWindow length \\(L\\) is the only hyper-parameter of the SSA. It is important to choose the right window length, because if the window length is too small, then the result will be noisy, and if the window length is too large, then the result will be too smooth. So, the window length should be chosen carefully.\nThe trajectory matrix of the series \\(\\mathbb{X}\\) is defined as follows:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\cdots & x_K \\\\\nx_2 & x_3 & x_4 & \\cdots & x_{K+1} \\\\\nx_3 & x_4 & x_5 & \\cdots & x_{K+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n   x_L & x_{L+1} & x_{L+2} & \\cdots & x_N\n\\end{bmatrix}\n\\]\nWe can see that, the first column of the trajectory matrix is the original time series up to a length \\(L\\), the second column is the original time series shifted by one time step, and so on. We also see that our matrix \\(\\mathbf{X}\\) has equal elements \\(x_{ij}\\) on the anti-diagonals \\(i + j = const\\). Hence, the matrix \\(\\mathbf{X}\\) is called Hankel matrix.\nInstead of implementing Hankel matrix from scratch, we can use scipy to hankelize any given time series. The function below takes time series and window length as input and returns Hankel matrix.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nimport seaborn as sns\n\n\ndef embed(time_series, window_length):\n    \"\"\"\n    Embed the time series into a Hankel matrix. This is the trajectory matrix.\n    \"\"\"\n    K = len(time_series) - window_length + 1\n    trajectory_matrix = scipy.linalg.hankel(time_series, np.zeros(window_length)).T[:, : K]\n    return trajectory_matrix\n\nLet consider following time series as an example:\n\ntime_series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nwindow_length = 4\n\ntrajectory_matrix = embed(time_series, window_length)\n\ntrajectory_matrix\n\narray([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n       [ 2.,  3.,  4.,  5.,  6.,  7.,  8.],\n       [ 3.,  4.,  5.,  6.,  7.,  8.,  9.],\n       [ 4.,  5.,  6.,  7.,  8.,  9., 10.]])\n\n\nThe second step is to perform SVD on the trajectory matrix. First, we multiply the trajectory matrix by its transpose to form the matrix \\(\\mathbf{S} = XX^{T}\\) and then perform SVD on the resulting matrix.\nFor more information how SVD works check my previous blog.\nAfter performing SVD, we get eigenvalues, sorted in decreasing order and left singular vectors of the matrix \\(\\mathbf{S}\\). We’re not interested in the right singular vectors, so we can discard them. For the next step, we also need to calculate the rank \\(d\\) of the trajectory matrix \\(\\mathbf{X}\\).\nThe function below takes trajectory matrix as input and returns rank, eigenvalues, and left singular vectors.\n\ndef decompose(trajectory_matrix):\n    \"\"\"\n    Decompose the trajectory matrix into its singular values and vectors.\n    \"\"\"\n    S = np.matmul(trajectory_matrix, trajectory_matrix.T)\n    singular_vectors, singular_values, _ = scipy.linalg.svd(S)\n    singular_values = np.sqrt(singular_values)\n    rank = np.linalg.matrix_rank(trajectory_matrix)\n    return rank, singular_values, singular_vectors\n\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\nprint(\"Rank: \", rank)\nprint(\"Singular values: \", singular_values, sep=\"\\n\")\nprint(\"Singular vectors: \", singular_vectors, sep=\"\\n\")\n\nRank:  2\nSingular values: \n[31.46491008  1.98983259  0.00000016  0.00000009]\nSingular vectors: \n[[-0.37304882 -0.7488889   0.54567073 -0.04736515]\n [-0.45239454 -0.30877044 -0.69225707  0.46987248]\n [-0.53174025  0.13134802 -0.25249804 -0.79764951]\n [-0.61108597  0.57146648  0.39908439  0.37514218]]\n\n\n\n\n\nAt that stage, we have to perform eigentriple grouping followed by diagonal averaging. The SVD step of the trajectory matrix is not finished. I split it into two parts for better understanding. The first part was actual decomposition, and the second part is grouping to form elementary matrices.\nThe function below takes trajectory matrix, rank, eigenvalues, and left singular vectors as input and returns grouped matrices. The rank of each matrix is 1, hence the name elementary matrices. The collection \\((\\sqrt{\\lambda_{i}}, U_{i}, V_i{})\\) is called eigentriple of the SVD. Not to be confused with \\(V_{i}\\). It is not the right singular vector, but the following matrix \\(V_{i} = \\\\ \\mathbf{X}^{T}U_{i}/{\\sqrt{\\lambda_{i}}}\\)\n\ndef group(trajectory_matrix, rank, singular_values, singular_vectors):\n    \"\"\"\n    Group the singular values and vectors into matrices.\n    \"\"\"\n    V = {i: np.matmul(trajectory_matrix.T, singular_vectors[:, i]) / singular_values[i] for i in range(rank)}\n    # The rank of each matrix is 1\n    X = {\n        i: np.matmul(singular_values[i] * singular_vectors[:, i].reshape(-1, 1), V[i].reshape(-1, 1).T)\n        for i in range(rank)\n    }\n    return X\n\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\nX\n\n{0: array([[ 1.98365074,  2.71791138,  3.45217202,  4.18643267,  4.92069331,\n          5.65495395,  6.38921459],\n        [ 2.40556386,  3.29599826,  4.18643267,  5.07686707,  5.96730148,\n          6.85773588,  7.74817029],\n        [ 2.82747698,  3.87408514,  4.92069331,  5.96730148,  7.01390964,\n          8.06051781,  9.10712598],\n        [ 3.2493901 ,  4.45217202,  5.65495395,  6.85773588,  8.06051781,\n          9.26329974, 10.46608167]]),\n 1: array([[-0.98365074, -0.71791138, -0.45217202, -0.18643267,  0.07930669,\n          0.34504605,  0.61078541],\n        [-0.40556386, -0.29599826, -0.18643267, -0.07686707,  0.03269852,\n          0.14226412,  0.25182971],\n        [ 0.17252302,  0.12591486,  0.07930669,  0.03269852, -0.01390964,\n         -0.06051781, -0.10712598],\n        [ 0.7506099 ,  0.54782798,  0.34504605,  0.14226412, -0.06051781,\n         -0.26329974, -0.46608167]])}\n\n\nAt the diagonal averaging step, each matrix \\(\\mathbf{X_{I_{j}}}\\) of the grouped decomposition is henkelized and then the obtained Hankel matrix is transformed into a new series of length \\(N\\). Diagonal averaging applied to the resulted matrix \\(\\mathbf{X_{I_{k}}}\\) produces a reconstructed series \\(\\mathbb{\\tilde{X^{(k)}}} = (\\tilde{x}^{(k)}_{1}, \\cdots, \\tilde{x}^{(k)}_{N})\\). Therefore, the initial series is decomposed into a sum of \\(m\\) reconstructed subseries:\n\\[\nx_{n} = \\sum_{k=1}^{m} \\tilde{x}^{(k)}_{n}, \\quad n = 1, \\cdots, N\n\\]\nThe function below takes grouped matrices as input and returns reconstructed time series.\n\ndef diagonal_averaging(X, m):\n    \"\"\"\n    Perform anti-diagonal averaging of the given hankel matrix\n    \"\"\"\n    if m &gt; len(X):\n        raise ValueError(\"Number of singular values cannot be greater than the rank of the trajectory matrix.\")\n\n    result = []\n    matrix = np.sum([X[i] for i in range(m)], axis=(0))\n    rows, columns = matrix.shape\n    rows_star, columns_star = min(rows, columns), max(rows, columns)\n\n    for k in range(1 - columns_star, rows_star):\n        identity_matrix = np.eye(columns_star, k=k, dtype=\"bool\")[::-1][:rows_star, :]\n        identity_matrix_sum = np.sum(identity_matrix)\n        mask = np.ma.masked_array(matrix, mask=1 - identity_matrix)\n        average = mask.sum() / identity_matrix_sum\n        result.append(average)\n\n    return pd.DataFrame(result).rename(columns={0: \"reconstruction\"})\n\nHere, I pick the first matrix of the grouped decomposition and perform diagonal averaging. The result is a reconstructed time series.\n\nreconstructed_time_series = diagonal_averaging(X, 1)\n\nreconstructed_time_series\n\n\n\n\n\n\n\n\nreconstruction\n\n\n\n\n0\n1.983651\n\n\n1\n2.561738\n\n\n2\n3.191882\n\n\n3\n3.874085\n\n\n4\n4.842606\n\n\n5\n5.811128\n\n\n6\n6.779649\n\n\n7\n7.956402\n\n\n8\n9.185213\n\n\n9\n10.466082\n\n\n\n\n\n\n\nHere is the result for the first two matrices. We see that it produces exactly the same series as initial one.\n\nreconstructed_time_series = diagonal_averaging(X, 2)\n\nreconstructed_time_series\n\n\n\n\n\n\n\n\nreconstruction\n\n\n\n\n0\n1.0\n\n\n1\n2.0\n\n\n2\n3.0\n\n\n3\n4.0\n\n\n4\n5.0\n\n\n5\n6.0\n\n\n6\n7.0\n\n\n7\n8.0\n\n\n8\n9.0\n\n\n9\n10.0\n\n\n\n\n\n\n\nWe can also visualize how much each singular value contributes to the variance.\n\ndef plot_contributions(trajectory_matrix, singular_values):\n    \"\"\"View the contribution to variance of each singular value and its corresponding signal\"\"\"\n    lambdas = np.power(singular_values, 2)\n    norm = np.linalg.norm(trajectory_matrix)\n    contributions = pd.DataFrame((lambdas / (norm**2)).round(4), columns=[\"contribution\"])\n    # Filter out the contributions that are zero\n    contributions = contributions[contributions[\"contribution\"] &gt; 0]\n    # Adjust the scale of the contributions\n    contributions[\"contribution\"] = (1 / contributions[\"contribution\"]).max() * 1.1 - (\n        1 / contributions[\"contribution\"]\n    )\n\n    # Plot the contributions\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(11, 4))\n    plt.axes().get_yaxis().set_ticks([])\n    sns.barplot(data=contributions, x=contributions.index, y=\"contribution\")\n    plt.title(\"Non-zero contributions of singular values to variance\")\n    plt.xlabel(\"Singular value\")\n    plt.ylabel(\"Contribution\")\n    plt.show()\n\n\nplot_contributions(trajectory_matrix, singular_values)\n\n\n\n\n\n\n\n\nFrom the above plot we see that the first singular value contributes the most to the variance. The second singular value contributes less.\n\n\n\nI guess, the above example does not make much sense as it’s not something real. Let consider real life data and go thru all the steps one by one. You can download the data from here.\nLet read the data and replicate above steps.\n\nts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\");\n\n/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_23011/3609878243.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  ts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\n\n\n\n\n\n\n\nWe see that the data is characterized with some seasonality. Due to that reason, we will use a window size of 24.\n\ntrajectory_matrix = embed(ts, window_length=24)\n\ntrajectory_matrix\n\narray([[ 6345., 20965., 51355., ..., 13655.,  4565.,  2840.],\n       [20965., 51355., 26435., ...,  4565.,  2840., 15380.],\n       [51355., 26435.,  9830., ...,  2840., 15380., 14340.],\n       ...,\n       [ 4725.,  9435.,  7495., ..., 17965.,  7505.,  2480.],\n       [ 9435.,  7495.,  7830., ...,  7505.,  2480.,  5725.],\n       [ 7495.,  7830., 13850., ...,  2480.,  5725., 17405.]],\n      shape=(24, 594))\n\n\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\n\nreconstructed = diagonal_averaging(X, m=12)\n\nts[\"reconstruction\"] = reconstructed[\"reconstruction\"].values\n\n\n# Plot original and reconstructed series\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\", label=\"Original\")\nsns.lineplot(data=ts, x=ts.index, y=\"reconstruction\", label=\"Reconstructed\");"
  },
  {
    "objectID": "posts/time_series_analysis/singular_spectrum_analysis.html#singular-spectrum-analysis",
    "href": "posts/time_series_analysis/singular_spectrum_analysis.html#singular-spectrum-analysis",
    "title": "Singular Spectrum Analysis with Python",
    "section": "",
    "text": "Let dive into the technical details of the SSA. It consists of the two main steps: decomposition and reconstruction. Each above step, in turn, consists of the two sub-steps: embedding followed by singular value decomposition (SVD) and eigentriple grouping followed by diagonal averaging.\n\n\nAt that stage, we have to perform embedding followed by SVD. Let consider following time series: \\(\\mathbb{X} = (x_{1}, \\cdots, x_{N})\\) of length \\(N\\). Now let pick a window size \\(L\\) and embed the time series into a trajectory matrix of dimension \\(L \\times K\\), where \\(K = N - L + 1\\):\nWindow length \\(L\\) is the only hyper-parameter of the SSA. It is important to choose the right window length, because if the window length is too small, then the result will be noisy, and if the window length is too large, then the result will be too smooth. So, the window length should be chosen carefully.\nThe trajectory matrix of the series \\(\\mathbb{X}\\) is defined as follows:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\cdots & x_K \\\\\nx_2 & x_3 & x_4 & \\cdots & x_{K+1} \\\\\nx_3 & x_4 & x_5 & \\cdots & x_{K+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n   x_L & x_{L+1} & x_{L+2} & \\cdots & x_N\n\\end{bmatrix}\n\\]\nWe can see that, the first column of the trajectory matrix is the original time series up to a length \\(L\\), the second column is the original time series shifted by one time step, and so on. We also see that our matrix \\(\\mathbf{X}\\) has equal elements \\(x_{ij}\\) on the anti-diagonals \\(i + j = const\\). Hence, the matrix \\(\\mathbf{X}\\) is called Hankel matrix.\nInstead of implementing Hankel matrix from scratch, we can use scipy to hankelize any given time series. The function below takes time series and window length as input and returns Hankel matrix.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nimport seaborn as sns\n\n\ndef embed(time_series, window_length):\n    \"\"\"\n    Embed the time series into a Hankel matrix. This is the trajectory matrix.\n    \"\"\"\n    K = len(time_series) - window_length + 1\n    trajectory_matrix = scipy.linalg.hankel(time_series, np.zeros(window_length)).T[:, : K]\n    return trajectory_matrix\n\nLet consider following time series as an example:\n\ntime_series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nwindow_length = 4\n\ntrajectory_matrix = embed(time_series, window_length)\n\ntrajectory_matrix\n\narray([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n       [ 2.,  3.,  4.,  5.,  6.,  7.,  8.],\n       [ 3.,  4.,  5.,  6.,  7.,  8.,  9.],\n       [ 4.,  5.,  6.,  7.,  8.,  9., 10.]])\n\n\nThe second step is to perform SVD on the trajectory matrix. First, we multiply the trajectory matrix by its transpose to form the matrix \\(\\mathbf{S} = XX^{T}\\) and then perform SVD on the resulting matrix.\nFor more information how SVD works check my previous blog.\nAfter performing SVD, we get eigenvalues, sorted in decreasing order and left singular vectors of the matrix \\(\\mathbf{S}\\). We’re not interested in the right singular vectors, so we can discard them. For the next step, we also need to calculate the rank \\(d\\) of the trajectory matrix \\(\\mathbf{X}\\).\nThe function below takes trajectory matrix as input and returns rank, eigenvalues, and left singular vectors.\n\ndef decompose(trajectory_matrix):\n    \"\"\"\n    Decompose the trajectory matrix into its singular values and vectors.\n    \"\"\"\n    S = np.matmul(trajectory_matrix, trajectory_matrix.T)\n    singular_vectors, singular_values, _ = scipy.linalg.svd(S)\n    singular_values = np.sqrt(singular_values)\n    rank = np.linalg.matrix_rank(trajectory_matrix)\n    return rank, singular_values, singular_vectors\n\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\nprint(\"Rank: \", rank)\nprint(\"Singular values: \", singular_values, sep=\"\\n\")\nprint(\"Singular vectors: \", singular_vectors, sep=\"\\n\")\n\nRank:  2\nSingular values: \n[31.46491008  1.98983259  0.00000016  0.00000009]\nSingular vectors: \n[[-0.37304882 -0.7488889   0.54567073 -0.04736515]\n [-0.45239454 -0.30877044 -0.69225707  0.46987248]\n [-0.53174025  0.13134802 -0.25249804 -0.79764951]\n [-0.61108597  0.57146648  0.39908439  0.37514218]]\n\n\n\n\n\nAt that stage, we have to perform eigentriple grouping followed by diagonal averaging. The SVD step of the trajectory matrix is not finished. I split it into two parts for better understanding. The first part was actual decomposition, and the second part is grouping to form elementary matrices.\nThe function below takes trajectory matrix, rank, eigenvalues, and left singular vectors as input and returns grouped matrices. The rank of each matrix is 1, hence the name elementary matrices. The collection \\((\\sqrt{\\lambda_{i}}, U_{i}, V_i{})\\) is called eigentriple of the SVD. Not to be confused with \\(V_{i}\\). It is not the right singular vector, but the following matrix \\(V_{i} = \\\\ \\mathbf{X}^{T}U_{i}/{\\sqrt{\\lambda_{i}}}\\)\n\ndef group(trajectory_matrix, rank, singular_values, singular_vectors):\n    \"\"\"\n    Group the singular values and vectors into matrices.\n    \"\"\"\n    V = {i: np.matmul(trajectory_matrix.T, singular_vectors[:, i]) / singular_values[i] for i in range(rank)}\n    # The rank of each matrix is 1\n    X = {\n        i: np.matmul(singular_values[i] * singular_vectors[:, i].reshape(-1, 1), V[i].reshape(-1, 1).T)\n        for i in range(rank)\n    }\n    return X\n\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\nX\n\n{0: array([[ 1.98365074,  2.71791138,  3.45217202,  4.18643267,  4.92069331,\n          5.65495395,  6.38921459],\n        [ 2.40556386,  3.29599826,  4.18643267,  5.07686707,  5.96730148,\n          6.85773588,  7.74817029],\n        [ 2.82747698,  3.87408514,  4.92069331,  5.96730148,  7.01390964,\n          8.06051781,  9.10712598],\n        [ 3.2493901 ,  4.45217202,  5.65495395,  6.85773588,  8.06051781,\n          9.26329974, 10.46608167]]),\n 1: array([[-0.98365074, -0.71791138, -0.45217202, -0.18643267,  0.07930669,\n          0.34504605,  0.61078541],\n        [-0.40556386, -0.29599826, -0.18643267, -0.07686707,  0.03269852,\n          0.14226412,  0.25182971],\n        [ 0.17252302,  0.12591486,  0.07930669,  0.03269852, -0.01390964,\n         -0.06051781, -0.10712598],\n        [ 0.7506099 ,  0.54782798,  0.34504605,  0.14226412, -0.06051781,\n         -0.26329974, -0.46608167]])}\n\n\nAt the diagonal averaging step, each matrix \\(\\mathbf{X_{I_{j}}}\\) of the grouped decomposition is henkelized and then the obtained Hankel matrix is transformed into a new series of length \\(N\\). Diagonal averaging applied to the resulted matrix \\(\\mathbf{X_{I_{k}}}\\) produces a reconstructed series \\(\\mathbb{\\tilde{X^{(k)}}} = (\\tilde{x}^{(k)}_{1}, \\cdots, \\tilde{x}^{(k)}_{N})\\). Therefore, the initial series is decomposed into a sum of \\(m\\) reconstructed subseries:\n\\[\nx_{n} = \\sum_{k=1}^{m} \\tilde{x}^{(k)}_{n}, \\quad n = 1, \\cdots, N\n\\]\nThe function below takes grouped matrices as input and returns reconstructed time series.\n\ndef diagonal_averaging(X, m):\n    \"\"\"\n    Perform anti-diagonal averaging of the given hankel matrix\n    \"\"\"\n    if m &gt; len(X):\n        raise ValueError(\"Number of singular values cannot be greater than the rank of the trajectory matrix.\")\n\n    result = []\n    matrix = np.sum([X[i] for i in range(m)], axis=(0))\n    rows, columns = matrix.shape\n    rows_star, columns_star = min(rows, columns), max(rows, columns)\n\n    for k in range(1 - columns_star, rows_star):\n        identity_matrix = np.eye(columns_star, k=k, dtype=\"bool\")[::-1][:rows_star, :]\n        identity_matrix_sum = np.sum(identity_matrix)\n        mask = np.ma.masked_array(matrix, mask=1 - identity_matrix)\n        average = mask.sum() / identity_matrix_sum\n        result.append(average)\n\n    return pd.DataFrame(result).rename(columns={0: \"reconstruction\"})\n\nHere, I pick the first matrix of the grouped decomposition and perform diagonal averaging. The result is a reconstructed time series.\n\nreconstructed_time_series = diagonal_averaging(X, 1)\n\nreconstructed_time_series\n\n\n\n\n\n\n\n\nreconstruction\n\n\n\n\n0\n1.983651\n\n\n1\n2.561738\n\n\n2\n3.191882\n\n\n3\n3.874085\n\n\n4\n4.842606\n\n\n5\n5.811128\n\n\n6\n6.779649\n\n\n7\n7.956402\n\n\n8\n9.185213\n\n\n9\n10.466082\n\n\n\n\n\n\n\nHere is the result for the first two matrices. We see that it produces exactly the same series as initial one.\n\nreconstructed_time_series = diagonal_averaging(X, 2)\n\nreconstructed_time_series\n\n\n\n\n\n\n\n\nreconstruction\n\n\n\n\n0\n1.0\n\n\n1\n2.0\n\n\n2\n3.0\n\n\n3\n4.0\n\n\n4\n5.0\n\n\n5\n6.0\n\n\n6\n7.0\n\n\n7\n8.0\n\n\n8\n9.0\n\n\n9\n10.0\n\n\n\n\n\n\n\nWe can also visualize how much each singular value contributes to the variance.\n\ndef plot_contributions(trajectory_matrix, singular_values):\n    \"\"\"View the contribution to variance of each singular value and its corresponding signal\"\"\"\n    lambdas = np.power(singular_values, 2)\n    norm = np.linalg.norm(trajectory_matrix)\n    contributions = pd.DataFrame((lambdas / (norm**2)).round(4), columns=[\"contribution\"])\n    # Filter out the contributions that are zero\n    contributions = contributions[contributions[\"contribution\"] &gt; 0]\n    # Adjust the scale of the contributions\n    contributions[\"contribution\"] = (1 / contributions[\"contribution\"]).max() * 1.1 - (\n        1 / contributions[\"contribution\"]\n    )\n\n    # Plot the contributions\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(11, 4))\n    plt.axes().get_yaxis().set_ticks([])\n    sns.barplot(data=contributions, x=contributions.index, y=\"contribution\")\n    plt.title(\"Non-zero contributions of singular values to variance\")\n    plt.xlabel(\"Singular value\")\n    plt.ylabel(\"Contribution\")\n    plt.show()\n\n\nplot_contributions(trajectory_matrix, singular_values)\n\n\n\n\n\n\n\n\nFrom the above plot we see that the first singular value contributes the most to the variance. The second singular value contributes less.\n\n\n\nI guess, the above example does not make much sense as it’s not something real. Let consider real life data and go thru all the steps one by one. You can download the data from here.\nLet read the data and replicate above steps.\n\nts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\");\n\n/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_23011/3609878243.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  ts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\n\n\n\n\n\n\n\nWe see that the data is characterized with some seasonality. Due to that reason, we will use a window size of 24.\n\ntrajectory_matrix = embed(ts, window_length=24)\n\ntrajectory_matrix\n\narray([[ 6345., 20965., 51355., ..., 13655.,  4565.,  2840.],\n       [20965., 51355., 26435., ...,  4565.,  2840., 15380.],\n       [51355., 26435.,  9830., ...,  2840., 15380., 14340.],\n       ...,\n       [ 4725.,  9435.,  7495., ..., 17965.,  7505.,  2480.],\n       [ 9435.,  7495.,  7830., ...,  7505.,  2480.,  5725.],\n       [ 7495.,  7830., 13850., ...,  2480.,  5725., 17405.]],\n      shape=(24, 594))\n\n\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\n\nreconstructed = diagonal_averaging(X, m=12)\n\nts[\"reconstruction\"] = reconstructed[\"reconstruction\"].values\n\n\n# Plot original and reconstructed series\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\", label=\"Original\")\nsns.lineplot(data=ts, x=ts.index, y=\"reconstruction\", label=\"Reconstructed\");"
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_4.html",
    "href": "posts/pandas/pandas_tips_and_tricks_4.html",
    "title": "Lost in Pandas - Part 4",
    "section": "",
    "text": "Introduction\nI’ll show how to assign new value for each unique group in Pandas DataFrame.\n\nimport pandas as pd\n\n\ndata = {\n    \"ID\": [1, 1, 1, 2, 2, 3, 3],\n    \"class\": [\"Lower\", \"Moderate\", \"Moderate\", \"Lower\", \"High\", \"High\", \"Lower\"]\n}\n\ndf = pd.DataFrame(data)\n\ndf\n\n\n\n\n\n\n\n\nID\nclass\n\n\n\n\n0\n1\nLower\n\n\n1\n1\nModerate\n\n\n2\n1\nModerate\n\n\n3\n2\nLower\n\n\n4\n2\nHigh\n\n\n5\n3\nHigh\n\n\n6\n3\nLower\n\n\n\n\n\n\n\nNow, let assign new values according to class column and then rearrange class column in order to assign new group names.\n\nvalue_map = {\"Lower\": 33, \"Moderate\": 34, \"High\": 40}\n\ndf[\"new\"] = df[\"class\"].map(value_map)\n\ndf\n\n\n\n\n\n\n\n\nID\nclass\nnew\n\n\n\n\n0\n1\nLower\n33\n\n\n1\n1\nModerate\n34\n\n\n2\n1\nModerate\n34\n\n\n3\n2\nLower\n33\n\n\n4\n2\nHigh\n40\n\n\n5\n3\nHigh\n40\n\n\n6\n3\nLower\n33\n\n\n\n\n\n\n\n\ndf[\"Max\"] = (df.set_index(\"class\")\n                .groupby(\"ID\")[\"new\"]\n                .transform(\"idxmax\").values)\n\ndf\n\n\n\n\n\n\n\n\nID\nclass\nnew\nMax\n\n\n\n\n0\n1\nLower\n33\nModerate\n\n\n1\n1\nModerate\n34\nModerate\n\n\n2\n1\nModerate\n34\nModerate\n\n\n3\n2\nLower\n33\nHigh\n\n\n4\n2\nHigh\n40\nHigh\n\n\n5\n3\nHigh\n40\nHigh\n\n\n6\n3\nLower\n33\nHigh\n\n\n\n\n\n\n\n\ndf[\"Max\"] = df[\"Max\"].mask(df[\"ID\"].duplicated(), \"\")\n\ndf\n\n\n\n\n\n\n\n\nID\nclass\nnew\nMax\n\n\n\n\n0\n1\nLower\n33\nModerate\n\n\n1\n1\nModerate\n34\n\n\n\n2\n1\nModerate\n34\n\n\n\n3\n2\nLower\n33\nHigh\n\n\n4\n2\nHigh\n40\n\n\n\n5\n3\nHigh\n40\nHigh\n\n\n6\n3\nLower\n33"
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_2.html",
    "href": "posts/pandas/pandas_tips_and_tricks_2.html",
    "title": "Lost in Pandas - Part 2",
    "section": "",
    "text": "I will show you how to fill missing values by the average of its before and after values if our dataframe has the following form:\n\n\nHow can we perform groupby and fill Nan’s with its preceding and following values?\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = {\n    \"type\": [\n        \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\",\n        \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\",\n    ],\n    \"date\": [\n        \"2018-09\", \"2018-10\", \"2018-11\", \"2018-12\", \"2019-01\", \"2019-02\", \"2019-03\",\n        \"2019-04\", \"2019-05\", \"2019-06\", \"2019-07\", \"2019-08\", \"2019-09\", \"2019-10\",\n        \"2019-11\", \"2018-09\", \"2018-10\", \"2018-11\", \"2018-12\", \"2019-01\", \"2019-02\",\n        \"2019-03\", \"2019-04\", \"2019-05\", \"2019-06\", \"2019-07\", \"2019-08\", \"2019-09\",\n        \"2019-10\", \"2019-11\",\n    ],\n    \"v1\": [\n        10, np.nan, np.nan, 20, np.nan, np.nan, 30, np.nan, np.nan, 40, np.nan,\n        np.nan, 50, np.nan, np.nan, 60, np.nan, np.nan, 70, np.nan, np.nan, 80,\n        np.nan, np.nan, 90, np.nan, np.nan, 100, np.nan, np.nan,\n    ],\n    \"v2\": [\n        10, np.nan, np.nan, 20, np.nan, np.nan, 30, np.nan, np.nan, 40, np.nan,\n        np.nan, 50, np.nan, np.nan, 60, np.nan, np.nan, 70, np.nan, np.nan, 80,\n        np.nan, np.nan, 90, np.nan, np.nan, 100, np.nan, np.nan,\n    ],\n}\n\n\ndf = pd.DataFrame(data)\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\nNaN\nNaN\n\n\n2\na\n2018-11\nNaN\nNaN\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\nNaN\nNaN\n\n\n\n\n\n\n\nWe see that values in type and date column are repetitive. Moreover, it does not matter what values are in other two columns, unless it is numeric type. Our aim is to fill these missing values by the average of its before and after value for v1 and v2 columns. We also notice that sorting and then filling won’t give desired result. We need something different. But before we find that solution we need to convert columns in numeric type in order to compute average by using Pandas to_numeric() method with parameter “errors” set to “coerce”, because Pandas DataFrame astype() method won’t work in this case.\nWe have two solution here. To use both of them let make copy of initial dataframe. The first uses groupby and then applies aggregator with “backward fill” and “forward fill” and then again groupby and then computes mean. The second solution does almost the same as the first but uses apply() method instead of aggregation.\n\n# The first solution\n\ndf_first = df.copy(deep=True)\n\ndf_first[[\"v1\", \"v2\"]] = (df_first.groupby(\"type\")[[\"v1\", \"v2\"]]\n                                  .agg([\"bfill\", \"ffill\"])\n                                  .groupby(level=0, axis=1)\n                                  .mean())\n\ndf_first\n\n/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_23109/1351098040.py:7: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n  .groupby(level=0, axis=1)\n\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n15.0\n15.0\n\n\n2\na\n2018-11\n15.0\n15.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n25.0\n25.0\n\n\n5\na\n2019-02\n25.0\n25.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n35.0\n35.0\n\n\n8\na\n2019-05\n35.0\n35.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n45.0\n45.0\n\n\n11\na\n2019-08\n45.0\n45.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n65.0\n65.0\n\n\n17\nb\n2018-11\n65.0\n65.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n75.0\n75.0\n\n\n20\nb\n2019-02\n75.0\n75.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n85.0\n85.0\n\n\n23\nb\n2019-05\n85.0\n85.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n95.0\n95.0\n\n\n26\nb\n2019-08\n95.0\n95.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\n\n# The second solution\n\ndf_second = df.copy(deep=True)\n\ng = df_second.groupby([\"type\"], group_keys=False)[[\"v1\", \"v2\"]]\n\ndf_second[[\"v1\", \"v2\"]] = (g.ffill() + g.bfill()) / 2\n\ndf_second[[\"v1\", \"v2\"]] = g.apply(lambda x: x.bfill().ffill())\n\n\ndf_second\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n20.0\n20.0\n\n\n2\na\n2018-11\n20.0\n20.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n30.0\n30.0\n\n\n5\na\n2019-02\n30.0\n30.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n40.0\n40.0\n\n\n8\na\n2019-05\n40.0\n40.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n50.0\n50.0\n\n\n11\na\n2019-08\n50.0\n50.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n70.0\n70.0\n\n\n17\nb\n2018-11\n70.0\n70.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n80.0\n80.0\n\n\n20\nb\n2019-02\n80.0\n80.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n90.0\n90.0\n\n\n23\nb\n2019-05\n90.0\n90.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n100.0\n100.0\n\n\n26\nb\n2019-08\n100.0\n100.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\nAbove two methods gave us desirable results. Let try some plain method to achieve the same. Below is the dry, plain method which fill missing values by backward and forward average. However by using this we introduce bug which will be hard to detect. Let see.\n\n# The third method\n\ndf_third = df.copy(deep=True)\n\ndf_third[[\"v1\", \"v2\"]] = (df_third[[\"v1\", \"v2\"]].ffill() + df_third[[\"v1\", \"v2\"]].bfill()) / 2\n\ndf_third[[\"v1\", \"v2\"]] = df_third[[\"v1\", \"v2\"]].bfill().ffill()\n\ndf_third\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n15.0\n15.0\n\n\n2\na\n2018-11\n15.0\n15.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n25.0\n25.0\n\n\n5\na\n2019-02\n25.0\n25.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n35.0\n35.0\n\n\n8\na\n2019-05\n35.0\n35.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n45.0\n45.0\n\n\n11\na\n2019-08\n45.0\n45.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n55.0\n55.0\n\n\n14\na\n2019-11\n55.0\n55.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n65.0\n65.0\n\n\n17\nb\n2018-11\n65.0\n65.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n75.0\n75.0\n\n\n20\nb\n2019-02\n75.0\n75.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n85.0\n85.0\n\n\n23\nb\n2019-05\n85.0\n85.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n95.0\n95.0\n\n\n26\nb\n2019-08\n95.0\n95.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\nAt the first glance everything seems ok, but let check the equality of these three dataframes with double equality sign and all method.\n\ndf_first == df_second\n\n# The result of the first and the second method are equal\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nFalse\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\n\n\n3\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nTrue\nFalse\nFalse\n\n\n5\nTrue\nTrue\nFalse\nFalse\n\n\n6\nTrue\nTrue\nTrue\nTrue\n\n\n7\nTrue\nTrue\nFalse\nFalse\n\n\n8\nTrue\nTrue\nFalse\nFalse\n\n\n9\nTrue\nTrue\nTrue\nTrue\n\n\n10\nTrue\nTrue\nFalse\nFalse\n\n\n11\nTrue\nTrue\nFalse\nFalse\n\n\n12\nTrue\nTrue\nTrue\nTrue\n\n\n13\nTrue\nTrue\nTrue\nTrue\n\n\n14\nTrue\nTrue\nTrue\nTrue\n\n\n15\nTrue\nTrue\nTrue\nTrue\n\n\n16\nTrue\nTrue\nFalse\nFalse\n\n\n17\nTrue\nTrue\nFalse\nFalse\n\n\n18\nTrue\nTrue\nTrue\nTrue\n\n\n19\nTrue\nTrue\nFalse\nFalse\n\n\n20\nTrue\nTrue\nFalse\nFalse\n\n\n21\nTrue\nTrue\nTrue\nTrue\n\n\n22\nTrue\nTrue\nFalse\nFalse\n\n\n23\nTrue\nTrue\nFalse\nFalse\n\n\n24\nTrue\nTrue\nTrue\nTrue\n\n\n25\nTrue\nTrue\nFalse\nFalse\n\n\n26\nTrue\nTrue\nFalse\nFalse\n\n\n27\nTrue\nTrue\nTrue\nTrue\n\n\n28\nTrue\nTrue\nTrue\nTrue\n\n\n29\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\ndf_third == df_first\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nTrue\nTrue\n\n\n2\nTrue\nTrue\nTrue\nTrue\n\n\n3\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nTrue\nTrue\nTrue\n\n\n5\nTrue\nTrue\nTrue\nTrue\n\n\n6\nTrue\nTrue\nTrue\nTrue\n\n\n7\nTrue\nTrue\nTrue\nTrue\n\n\n8\nTrue\nTrue\nTrue\nTrue\n\n\n9\nTrue\nTrue\nTrue\nTrue\n\n\n10\nTrue\nTrue\nTrue\nTrue\n\n\n11\nTrue\nTrue\nTrue\nTrue\n\n\n12\nTrue\nTrue\nTrue\nTrue\n\n\n13\nTrue\nTrue\nFalse\nFalse\n\n\n14\nTrue\nTrue\nFalse\nFalse\n\n\n15\nTrue\nTrue\nTrue\nTrue\n\n\n16\nTrue\nTrue\nTrue\nTrue\n\n\n17\nTrue\nTrue\nTrue\nTrue\n\n\n18\nTrue\nTrue\nTrue\nTrue\n\n\n19\nTrue\nTrue\nTrue\nTrue\n\n\n20\nTrue\nTrue\nTrue\nTrue\n\n\n21\nTrue\nTrue\nTrue\nTrue\n\n\n22\nTrue\nTrue\nTrue\nTrue\n\n\n23\nTrue\nTrue\nTrue\nTrue\n\n\n24\nTrue\nTrue\nTrue\nTrue\n\n\n25\nTrue\nTrue\nTrue\nTrue\n\n\n26\nTrue\nTrue\nTrue\nTrue\n\n\n27\nTrue\nTrue\nTrue\nTrue\n\n\n28\nTrue\nTrue\nTrue\nTrue\n\n\n29\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nAt index position 13 and 14 we have False values. That was the reason of inequality. But why do we have these False values? The dry plain method calculated the average of non-missing values sequentially not by type. At index 13 and 14 the average is 50 not 55, because the third method calculated the average between 50 and 60 instead of 50 and NaN. That resulted the bug!\n\ndf.iloc[12:16]\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\nNaN\nNaN\n\n\n14\na\n2019-11\nNaN\nNaN\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n\n\n\n\n\n\ndf_first.iloc[12:16]\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n\n\n\n\n\n\ndf_third.iloc[12:16]  # Not Correct\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n55.0\n55.0\n\n\n14\na\n2019-11\n55.0\n55.0\n\n\n15\nb\n2018-09\n60.0\n60.0"
  },
  {
    "objectID": "posts/pandas/pandas_tips_and_tricks_2.html#problem-statement",
    "href": "posts/pandas/pandas_tips_and_tricks_2.html#problem-statement",
    "title": "Lost in Pandas - Part 2",
    "section": "",
    "text": "How can we perform groupby and fill Nan’s with its preceding and following values?\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = {\n    \"type\": [\n        \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\",\n        \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\",\n    ],\n    \"date\": [\n        \"2018-09\", \"2018-10\", \"2018-11\", \"2018-12\", \"2019-01\", \"2019-02\", \"2019-03\",\n        \"2019-04\", \"2019-05\", \"2019-06\", \"2019-07\", \"2019-08\", \"2019-09\", \"2019-10\",\n        \"2019-11\", \"2018-09\", \"2018-10\", \"2018-11\", \"2018-12\", \"2019-01\", \"2019-02\",\n        \"2019-03\", \"2019-04\", \"2019-05\", \"2019-06\", \"2019-07\", \"2019-08\", \"2019-09\",\n        \"2019-10\", \"2019-11\",\n    ],\n    \"v1\": [\n        10, np.nan, np.nan, 20, np.nan, np.nan, 30, np.nan, np.nan, 40, np.nan,\n        np.nan, 50, np.nan, np.nan, 60, np.nan, np.nan, 70, np.nan, np.nan, 80,\n        np.nan, np.nan, 90, np.nan, np.nan, 100, np.nan, np.nan,\n    ],\n    \"v2\": [\n        10, np.nan, np.nan, 20, np.nan, np.nan, 30, np.nan, np.nan, 40, np.nan,\n        np.nan, 50, np.nan, np.nan, 60, np.nan, np.nan, 70, np.nan, np.nan, 80,\n        np.nan, np.nan, 90, np.nan, np.nan, 100, np.nan, np.nan,\n    ],\n}\n\n\ndf = pd.DataFrame(data)\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\nNaN\nNaN\n\n\n2\na\n2018-11\nNaN\nNaN\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\nNaN\nNaN\n\n\n\n\n\n\n\nWe see that values in type and date column are repetitive. Moreover, it does not matter what values are in other two columns, unless it is numeric type. Our aim is to fill these missing values by the average of its before and after value for v1 and v2 columns. We also notice that sorting and then filling won’t give desired result. We need something different. But before we find that solution we need to convert columns in numeric type in order to compute average by using Pandas to_numeric() method with parameter “errors” set to “coerce”, because Pandas DataFrame astype() method won’t work in this case.\nWe have two solution here. To use both of them let make copy of initial dataframe. The first uses groupby and then applies aggregator with “backward fill” and “forward fill” and then again groupby and then computes mean. The second solution does almost the same as the first but uses apply() method instead of aggregation.\n\n# The first solution\n\ndf_first = df.copy(deep=True)\n\ndf_first[[\"v1\", \"v2\"]] = (df_first.groupby(\"type\")[[\"v1\", \"v2\"]]\n                                  .agg([\"bfill\", \"ffill\"])\n                                  .groupby(level=0, axis=1)\n                                  .mean())\n\ndf_first\n\n/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_23109/1351098040.py:7: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n  .groupby(level=0, axis=1)\n\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n15.0\n15.0\n\n\n2\na\n2018-11\n15.0\n15.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n25.0\n25.0\n\n\n5\na\n2019-02\n25.0\n25.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n35.0\n35.0\n\n\n8\na\n2019-05\n35.0\n35.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n45.0\n45.0\n\n\n11\na\n2019-08\n45.0\n45.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n65.0\n65.0\n\n\n17\nb\n2018-11\n65.0\n65.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n75.0\n75.0\n\n\n20\nb\n2019-02\n75.0\n75.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n85.0\n85.0\n\n\n23\nb\n2019-05\n85.0\n85.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n95.0\n95.0\n\n\n26\nb\n2019-08\n95.0\n95.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\n\n# The second solution\n\ndf_second = df.copy(deep=True)\n\ng = df_second.groupby([\"type\"], group_keys=False)[[\"v1\", \"v2\"]]\n\ndf_second[[\"v1\", \"v2\"]] = (g.ffill() + g.bfill()) / 2\n\ndf_second[[\"v1\", \"v2\"]] = g.apply(lambda x: x.bfill().ffill())\n\n\ndf_second\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n20.0\n20.0\n\n\n2\na\n2018-11\n20.0\n20.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n30.0\n30.0\n\n\n5\na\n2019-02\n30.0\n30.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n40.0\n40.0\n\n\n8\na\n2019-05\n40.0\n40.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n50.0\n50.0\n\n\n11\na\n2019-08\n50.0\n50.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n70.0\n70.0\n\n\n17\nb\n2018-11\n70.0\n70.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n80.0\n80.0\n\n\n20\nb\n2019-02\n80.0\n80.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n90.0\n90.0\n\n\n23\nb\n2019-05\n90.0\n90.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n100.0\n100.0\n\n\n26\nb\n2019-08\n100.0\n100.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\nAbove two methods gave us desirable results. Let try some plain method to achieve the same. Below is the dry, plain method which fill missing values by backward and forward average. However by using this we introduce bug which will be hard to detect. Let see.\n\n# The third method\n\ndf_third = df.copy(deep=True)\n\ndf_third[[\"v1\", \"v2\"]] = (df_third[[\"v1\", \"v2\"]].ffill() + df_third[[\"v1\", \"v2\"]].bfill()) / 2\n\ndf_third[[\"v1\", \"v2\"]] = df_third[[\"v1\", \"v2\"]].bfill().ffill()\n\ndf_third\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\na\n2018-09\n10.0\n10.0\n\n\n1\na\n2018-10\n15.0\n15.0\n\n\n2\na\n2018-11\n15.0\n15.0\n\n\n3\na\n2018-12\n20.0\n20.0\n\n\n4\na\n2019-01\n25.0\n25.0\n\n\n5\na\n2019-02\n25.0\n25.0\n\n\n6\na\n2019-03\n30.0\n30.0\n\n\n7\na\n2019-04\n35.0\n35.0\n\n\n8\na\n2019-05\n35.0\n35.0\n\n\n9\na\n2019-06\n40.0\n40.0\n\n\n10\na\n2019-07\n45.0\n45.0\n\n\n11\na\n2019-08\n45.0\n45.0\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n55.0\n55.0\n\n\n14\na\n2019-11\n55.0\n55.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n16\nb\n2018-10\n65.0\n65.0\n\n\n17\nb\n2018-11\n65.0\n65.0\n\n\n18\nb\n2018-12\n70.0\n70.0\n\n\n19\nb\n2019-01\n75.0\n75.0\n\n\n20\nb\n2019-02\n75.0\n75.0\n\n\n21\nb\n2019-03\n80.0\n80.0\n\n\n22\nb\n2019-04\n85.0\n85.0\n\n\n23\nb\n2019-05\n85.0\n85.0\n\n\n24\nb\n2019-06\n90.0\n90.0\n\n\n25\nb\n2019-07\n95.0\n95.0\n\n\n26\nb\n2019-08\n95.0\n95.0\n\n\n27\nb\n2019-09\n100.0\n100.0\n\n\n28\nb\n2019-10\n100.0\n100.0\n\n\n29\nb\n2019-11\n100.0\n100.0\n\n\n\n\n\n\n\nAt the first glance everything seems ok, but let check the equality of these three dataframes with double equality sign and all method.\n\ndf_first == df_second\n\n# The result of the first and the second method are equal\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nFalse\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\n\n\n3\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nTrue\nFalse\nFalse\n\n\n5\nTrue\nTrue\nFalse\nFalse\n\n\n6\nTrue\nTrue\nTrue\nTrue\n\n\n7\nTrue\nTrue\nFalse\nFalse\n\n\n8\nTrue\nTrue\nFalse\nFalse\n\n\n9\nTrue\nTrue\nTrue\nTrue\n\n\n10\nTrue\nTrue\nFalse\nFalse\n\n\n11\nTrue\nTrue\nFalse\nFalse\n\n\n12\nTrue\nTrue\nTrue\nTrue\n\n\n13\nTrue\nTrue\nTrue\nTrue\n\n\n14\nTrue\nTrue\nTrue\nTrue\n\n\n15\nTrue\nTrue\nTrue\nTrue\n\n\n16\nTrue\nTrue\nFalse\nFalse\n\n\n17\nTrue\nTrue\nFalse\nFalse\n\n\n18\nTrue\nTrue\nTrue\nTrue\n\n\n19\nTrue\nTrue\nFalse\nFalse\n\n\n20\nTrue\nTrue\nFalse\nFalse\n\n\n21\nTrue\nTrue\nTrue\nTrue\n\n\n22\nTrue\nTrue\nFalse\nFalse\n\n\n23\nTrue\nTrue\nFalse\nFalse\n\n\n24\nTrue\nTrue\nTrue\nTrue\n\n\n25\nTrue\nTrue\nFalse\nFalse\n\n\n26\nTrue\nTrue\nFalse\nFalse\n\n\n27\nTrue\nTrue\nTrue\nTrue\n\n\n28\nTrue\nTrue\nTrue\nTrue\n\n\n29\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\ndf_third == df_first\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nTrue\nTrue\n\n\n2\nTrue\nTrue\nTrue\nTrue\n\n\n3\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nTrue\nTrue\nTrue\n\n\n5\nTrue\nTrue\nTrue\nTrue\n\n\n6\nTrue\nTrue\nTrue\nTrue\n\n\n7\nTrue\nTrue\nTrue\nTrue\n\n\n8\nTrue\nTrue\nTrue\nTrue\n\n\n9\nTrue\nTrue\nTrue\nTrue\n\n\n10\nTrue\nTrue\nTrue\nTrue\n\n\n11\nTrue\nTrue\nTrue\nTrue\n\n\n12\nTrue\nTrue\nTrue\nTrue\n\n\n13\nTrue\nTrue\nFalse\nFalse\n\n\n14\nTrue\nTrue\nFalse\nFalse\n\n\n15\nTrue\nTrue\nTrue\nTrue\n\n\n16\nTrue\nTrue\nTrue\nTrue\n\n\n17\nTrue\nTrue\nTrue\nTrue\n\n\n18\nTrue\nTrue\nTrue\nTrue\n\n\n19\nTrue\nTrue\nTrue\nTrue\n\n\n20\nTrue\nTrue\nTrue\nTrue\n\n\n21\nTrue\nTrue\nTrue\nTrue\n\n\n22\nTrue\nTrue\nTrue\nTrue\n\n\n23\nTrue\nTrue\nTrue\nTrue\n\n\n24\nTrue\nTrue\nTrue\nTrue\n\n\n25\nTrue\nTrue\nTrue\nTrue\n\n\n26\nTrue\nTrue\nTrue\nTrue\n\n\n27\nTrue\nTrue\nTrue\nTrue\n\n\n28\nTrue\nTrue\nTrue\nTrue\n\n\n29\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nAt index position 13 and 14 we have False values. That was the reason of inequality. But why do we have these False values? The dry plain method calculated the average of non-missing values sequentially not by type. At index 13 and 14 the average is 50 not 55, because the third method calculated the average between 50 and 60 instead of 50 and NaN. That resulted the bug!\n\ndf.iloc[12:16]\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\nNaN\nNaN\n\n\n14\na\n2019-11\nNaN\nNaN\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n\n\n\n\n\n\ndf_first.iloc[12:16]\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n50.0\n50.0\n\n\n14\na\n2019-11\n50.0\n50.0\n\n\n15\nb\n2018-09\n60.0\n60.0\n\n\n\n\n\n\n\n\ndf_third.iloc[12:16]  # Not Correct\n\n\n\n\n\n\n\n\ntype\ndate\nv1\nv2\n\n\n\n\n12\na\n2019-09\n50.0\n50.0\n\n\n13\na\n2019-10\n55.0\n55.0\n\n\n14\na\n2019-11\n55.0\n55.0\n\n\n15\nb\n2018-09\n60.0\n60.0"
  },
  {
    "objectID": "open_source.html",
    "href": "open_source.html",
    "title": "Open Source Projects",
    "section": "",
    "text": "There will go open source projects list.\n\nProject 1\nProject 2\nProject 3\n\nThis is a pyodide-enabled code cell in a Quarto HTML document.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  }
]