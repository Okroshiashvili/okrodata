{"title":"Basics of Information Theory with Python","markdown":{"yaml":{"title":"Basics of Information Theory with Python","author":"Nodar Okroshiashvili","date":"2020-07-12","categories":["General"],"tags":["Information Theory","Information Gain","Python","Numpy"],"keywords":["information theory in python","entropy","self information","joint entropy in python","conditional entropy in python"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nInformation flows around us. It's everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams. For computers, all of them are represented by only two digits 0 and 1, and they carry some information. \"**Information theory** studies the transmission, processing, extraction, and utilization of information.\"[wikipedia](https://en.wikipedia.org/wiki/Information_theory)\nIn simple words, with information theory, given different kinds of signals, we try to measure how much information is presented in each of those signals. The theory itself originates from the original work of [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) named [*A Mathematical Theory of Communication*](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)\n\nIt will be helpful to see how machine learning and information theory are related. According to \"Dive Into Deep Learning\" hence d2l considers this relationship to be\n\n> Machine learning aims to extract interesting signals from data and make critical predictions.\n> On the other hand, information theory studies encoding, decoding, transmitting, and manipulating information.\n> As a result, information theory provides a fundamental language for discussing the information processing in machine learned systems.[source](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)\n\nInformation theory is tightly connected to mathematics and statistics. We will see later on how, but before that,\nit's worth to say where is used the concepts of information theory in statistics and mathematics.\nWe all know or have heard about *random variables* that are drawn from some probability distribution.\nFrom linear algebra, we also know how to measure the distance between two points, or between two planes.\nBut, how can we measure the distance between two probability distribution? In other words, how similar or dissimilar are these two probability distribution? Information theory gives us the ability to answer this question and quantify the similarity measure between two distributions. Before we continue,\nlet me outline the measurement unit of information theory. Shannon introduced the **bit** as the unit of information.\nThe series of 0 and 1 encode any data. Accordingly, the sequence of binary digits of length $n$ contains *$n$ bits* of information. That has been said, we can review concepts of information theory.\n\nThere are a few main concepts in information theory, and I will go through each of them in a detailed manner. First in line is:\n\n## Self-Information\n\nTo understand this concept well, I will review two examples—one from statistics and probability and the second from the information theory. Let start with statistics and probability. Imagine we conduct an experiment giving several outcomes with a different probability. For example, rolling the fair dice with uniform probability $\\frac{1}{6}$ of returning numbers from 1 to 6. Now, consider three outcomes, defined as $A=\\{outcome \\leq 6\\}$\n$B=\\{outcome is odd\\}$, and $C=\\{outcome=1\\}$ over probability space $\\Omega$, which in turn contains all the outcomes. **Self-information**, sometimes stated as **information content** or **surprisal** indicates how much unlikely the event $A$, or $B$, or $C$ is, how much surprised we are by observing either event. Here is the question: How can we convert probability $p$ of an event into a number of bits? Claude Shannon gave us the formula for that:\n\n$$\nI(X) = - \\log_2(p)\n$$\n\nFor our three events, $A$, $B$, and $C$ the self-information or surprisal is the following:\n\n$$\nI(A) = - \\log_2(1) = 0\n$$\n\n$$\nI(B) = - \\log_2(\\frac{3}{6}) = 1\n$$\n\n$$\nI(C) = - \\log_2(\\frac{1}{6}) = 2.58\n$$\n\nFrom an information theory perspective, if we have a series of binary digits of the length $n$, then the probability of getting 0 or 1 is $\\frac{1}{2^{n}}$. According to Shannon, self-information is the bits of information we receive from observing the event $X$. Let $X$ be the following code: ```0101```, then its information content is **4 bits** according to our formula:\n\n$$\nI(X) = I(0101) = - \\log_2(\\frac{1}{2^{4}}) = 4\n$$\n\n```{python}\n\nimport numpy as np\n\n```\n\n```{python}\n\ndef self_information(p):\n    return -np.log2(p)\n\n```\n\n```{python}\n\nself_information(1 / 2**4)\n\n```\n\nThe main takeaway here is that if a particular event has 100% probability, its self-information is $-\\log_2(1) = 0$, meaning that it does not carry any information, and we have no surprise at all. Whereas, if the probability would be close to zero, or we can effectively say it's zero, then self-information is $-\\log_2(0) = \\infty$. This implies that the rare events have high surprisal or high information content.\n\nWe see that information content only measures the information of a single event. To generalize this notion for any discrete and/or continues event, we will get the idea of **Entropy**.\n\n## Entropy\n\nIf we have any random variable $X$, whether it will be a discrete or continuous and $X$ follows a probability distribution $P$ with ```p.d.f``` if it's continuous or ```p.m.f``` if it's discrete. Can we calculate the average value of $X$? Yes, we can. From statistics, the formula of the average or a.k.a expectation is\n\n$$\n\\mathbb E(X) = \\sum_{i=1}^{k} x_{i} \\cdot p_{i}\n$$\n\nWhere $x_{i}$ is one particular event with its probability $p_{i}$. The same is in information theory. The **Entropy** of a random variable $X$ is the expectation of its self-information, given by:\n\n$$\nH(X) = - \\sum_{i} p_{i} \\log_{2} p_{i}\n$$\n\nIn Python it looks the following:\n\n```{python}\n\n# np.nansum return the sum of NaNs. Treats them as zeros.\n\ndef entropy(p):\n    out = np.nansum(-p * np.log2(p))\n    return out\n\n```\n\n```{python}\n\nentropy(np.array([0.1, 0.5, 0.1, 0.3]))\n\n```\n\nHere, we only consider one random variable, $X$, and its expected surprisal. What if we have two random variables $X$ and $Y$? How can we measure their joint information content? In other words, we are interested what information is included in $X$ and $Y$ compared to each separately. Here comes the **Joint Entropy**\n\n## Joint Entropy\n\nTo review this concept let me introduce two random variables $X$ and $Y$ and they follow the probability distribution denoted by $p_{X}(x)$ and $p_Y(y)$, respectively. $(X, Y)$ has joint probability $p_{X, Y}(x, y)$. The **Joint Entropy** hence is defined as:\n\n$$\nH(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log_{2} p_{X, Y}(x, y)\n$$\n\nHere are two important facts. If $X = Y$ this implies that $H(X,Y) = H(X) = H(Y)$ and if $X$ and $Y$ are independent, then $H(X, Y) = H(X) + H(Y)$.\n\n```{python}\n\ndef joint_entropy(p_xy):\n    out = np.nansum(-p_xy * np.log2(p_xy))\n    return out\n\n```\n\n```{python}\n\njoint_entropy(np.array([[0.1, 0.5, 0.8], [0.1, 0.3, 0.02]]))\n\n```\n\nAs we see, joint entropy indicates the amount of information in the pair of two random variables. What if we are interested to know how much information is contained, say in $Y$ but not in $X$?\n\n## Conditional Entropy\n\nThe **conditional entropy** is used to measure the relationship between variables. The following formula gives this measurement:\n\n$$\nH(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log_{2} p(y \\mid x)\n$$\n\nLet investigate how conditional entropy is related to entropy and joint entropy. Using the above formula, we can conclude that:\n\n$$\nH(Y \\mid X) = H(X, Y) - H(X)\n$$\n\nmeaning that the information contained in $Y$ given $X$ equals information jointly contained in $X$ and $Y$ minus the amount of information only contained in $X$.\n\n```{python}\n\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy / p_x\n    out = np.nansum(-p_xy * np.log2(p_y_given_x))\n    return out\n\n```\n\n```{python}\n\nconditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n\n```\n\nKnowing conditional entropy means knowing the amount of information contained in $Y$ but not in $X$. Now let see how much information is shared between $X$ and $Y$.\n\n## Mutual Information\n\nTo find the **mutual information** between two random variables $X$ and $Y$, let start the process by finding all the information in both $X$ and $Y$ together and then subtract the part which is not shared. The information both in $X$ and $Y$ is $H(X, Y)$. Subtracting two conditional entropies gives:\n\n$$\nI(X, Y) = H(X, Y) - H(Y \\mid X) − H(X \\mid Y)\n$$\n\nThis means that we have to subtract the information only contained in $X$ and $Y$ to all the information at hand.\nThis relationship is perfectly described by this picture.\n\n![mutual_information](mutual_information.png)\n\nThe concept of mutual information likewise correlation coefficient, allow us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them.\n\n```{python}\n\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    out = np.nansum(p_xy * np.log2(p))\n    return out\n\n```\n\n```{python}\n\nmutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]), np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n\n```\n\nAs in the case of the correlation coefficient, mutual information has some notable properties:\n\n* Mutual information is symmetric\n* Mutual information is non-negative\n* $I(X, Y) = 0$ iff $X$ and $Y$ are independent\n\nWe can interpret the mutual information $I(X, Y)$ as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent.\n\n## Kullback–Leibler Divergence - Relative Entropy\n\nI asked the question about measuring the distance between two probability distributions. The time has come to answer this question precisely. If we have random variable $X$ which follows probability distributin $P$ and has ```p.d.f``` or ```p.m.f``` $p(x)$. Imagine we estimated $P$ with other probability distribution $Q$, which in turn has ```p.d.f``` or ```p.m.f``` $q(x)$. The distance between thse two probability distribution is measured by **Kullback–Leibler (KL) Divergence**:\n\n$$\nD_{\\mathrm{KL}}(P\\|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n$$\n\nThe lower value of the $KL$ divergence, the closer our estimate is to the actual distribution.\n\n* The KL divergence is non-symmetric or equivalently, $D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P), \\text{ if } P \\neq Q$\n* The KL divergence is non-negative or equivalently, $D_{\\mathrm{KL}}(P\\|Q) \\geq 0$\n\n```{python}\n\ndef kl_divergence(p, q):\n    kl = p * np.log2(p / q)\n    out = np.nansum(kl)\n    return np.abs(out)\n\n```\n\n```{python}\n\np = np.random.normal(1, 2, size=1000)\nq = np.random.normal(1, 2, size=1000)\n\nkl_divergence(p, q)\n\n```\n\n## Cross Entropy\n\nTo understand **Cross-Entropy**, let me use the example from the KL divergence part. Now, imagine we perform classification tasks, where $y$ is the true label, and $\\hat{y}$ is estimated label by our model. **Cross-Entropy** denoted by $\\mathrm{CE}(y, \\hat{y})$ is used as a objective function in many classification tasks in deep learning. The formula is the following:\n\n$$\n\\mathrm{CE} (P, Q) = H(P) + D_{\\mathrm{KL}}(P\\|Q)\n$$\n\nThe two terms on the right-hand side are self-information and KL divergence. $P$ is the distribution of the true labels,\nand $Q$ is the distribution of the estimated labels. As we are only interested in knowing how far we are from the actual label and $H(P)$ is also given, the above formula is reduced to minimize only the second term (KL divergence) at the right-hand side. Hence, we have\n\n$$\n\\mathrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log_{2}{p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)}\n$$\n\n```{python}\n\ndef cross_entropy(y_hat, y):\n    ce = -np.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n\n```\n\n```{python}\n\nlabels = np.array([0, 2])\npreds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n\n```\n\n\n# Conclusion\n\nBy reviewing these concepts from the information theory, we have some rough sense of how it's related to the statistics and mathematics and is used in machine learning and deep learning. There is much more to discover, and that's up to you how far you want to go. Moreover, even interesting is how information theory is related to the coding theory, in gambling and musical composition.\n\n#### References\n* [Dive Into Deep Learning](https://d2l.ai/index.html)\n* [Information theory](https://en.wikipedia.org/wiki/Information_theory)\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nInformation flows around us. It's everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams. For computers, all of them are represented by only two digits 0 and 1, and they carry some information. \"**Information theory** studies the transmission, processing, extraction, and utilization of information.\"[wikipedia](https://en.wikipedia.org/wiki/Information_theory)\nIn simple words, with information theory, given different kinds of signals, we try to measure how much information is presented in each of those signals. The theory itself originates from the original work of [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) named [*A Mathematical Theory of Communication*](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)\n\nIt will be helpful to see how machine learning and information theory are related. According to \"Dive Into Deep Learning\" hence d2l considers this relationship to be\n\n> Machine learning aims to extract interesting signals from data and make critical predictions.\n> On the other hand, information theory studies encoding, decoding, transmitting, and manipulating information.\n> As a result, information theory provides a fundamental language for discussing the information processing in machine learned systems.[source](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)\n\nInformation theory is tightly connected to mathematics and statistics. We will see later on how, but before that,\nit's worth to say where is used the concepts of information theory in statistics and mathematics.\nWe all know or have heard about *random variables* that are drawn from some probability distribution.\nFrom linear algebra, we also know how to measure the distance between two points, or between two planes.\nBut, how can we measure the distance between two probability distribution? In other words, how similar or dissimilar are these two probability distribution? Information theory gives us the ability to answer this question and quantify the similarity measure between two distributions. Before we continue,\nlet me outline the measurement unit of information theory. Shannon introduced the **bit** as the unit of information.\nThe series of 0 and 1 encode any data. Accordingly, the sequence of binary digits of length $n$ contains *$n$ bits* of information. That has been said, we can review concepts of information theory.\n\nThere are a few main concepts in information theory, and I will go through each of them in a detailed manner. First in line is:\n\n## Self-Information\n\nTo understand this concept well, I will review two examples—one from statistics and probability and the second from the information theory. Let start with statistics and probability. Imagine we conduct an experiment giving several outcomes with a different probability. For example, rolling the fair dice with uniform probability $\\frac{1}{6}$ of returning numbers from 1 to 6. Now, consider three outcomes, defined as $A=\\{outcome \\leq 6\\}$\n$B=\\{outcome is odd\\}$, and $C=\\{outcome=1\\}$ over probability space $\\Omega$, which in turn contains all the outcomes. **Self-information**, sometimes stated as **information content** or **surprisal** indicates how much unlikely the event $A$, or $B$, or $C$ is, how much surprised we are by observing either event. Here is the question: How can we convert probability $p$ of an event into a number of bits? Claude Shannon gave us the formula for that:\n\n$$\nI(X) = - \\log_2(p)\n$$\n\nFor our three events, $A$, $B$, and $C$ the self-information or surprisal is the following:\n\n$$\nI(A) = - \\log_2(1) = 0\n$$\n\n$$\nI(B) = - \\log_2(\\frac{3}{6}) = 1\n$$\n\n$$\nI(C) = - \\log_2(\\frac{1}{6}) = 2.58\n$$\n\nFrom an information theory perspective, if we have a series of binary digits of the length $n$, then the probability of getting 0 or 1 is $\\frac{1}{2^{n}}$. According to Shannon, self-information is the bits of information we receive from observing the event $X$. Let $X$ be the following code: ```0101```, then its information content is **4 bits** according to our formula:\n\n$$\nI(X) = I(0101) = - \\log_2(\\frac{1}{2^{4}}) = 4\n$$\n\n```{python}\n\nimport numpy as np\n\n```\n\n```{python}\n\ndef self_information(p):\n    return -np.log2(p)\n\n```\n\n```{python}\n\nself_information(1 / 2**4)\n\n```\n\nThe main takeaway here is that if a particular event has 100% probability, its self-information is $-\\log_2(1) = 0$, meaning that it does not carry any information, and we have no surprise at all. Whereas, if the probability would be close to zero, or we can effectively say it's zero, then self-information is $-\\log_2(0) = \\infty$. This implies that the rare events have high surprisal or high information content.\n\nWe see that information content only measures the information of a single event. To generalize this notion for any discrete and/or continues event, we will get the idea of **Entropy**.\n\n## Entropy\n\nIf we have any random variable $X$, whether it will be a discrete or continuous and $X$ follows a probability distribution $P$ with ```p.d.f``` if it's continuous or ```p.m.f``` if it's discrete. Can we calculate the average value of $X$? Yes, we can. From statistics, the formula of the average or a.k.a expectation is\n\n$$\n\\mathbb E(X) = \\sum_{i=1}^{k} x_{i} \\cdot p_{i}\n$$\n\nWhere $x_{i}$ is one particular event with its probability $p_{i}$. The same is in information theory. The **Entropy** of a random variable $X$ is the expectation of its self-information, given by:\n\n$$\nH(X) = - \\sum_{i} p_{i} \\log_{2} p_{i}\n$$\n\nIn Python it looks the following:\n\n```{python}\n\n# np.nansum return the sum of NaNs. Treats them as zeros.\n\ndef entropy(p):\n    out = np.nansum(-p * np.log2(p))\n    return out\n\n```\n\n```{python}\n\nentropy(np.array([0.1, 0.5, 0.1, 0.3]))\n\n```\n\nHere, we only consider one random variable, $X$, and its expected surprisal. What if we have two random variables $X$ and $Y$? How can we measure their joint information content? In other words, we are interested what information is included in $X$ and $Y$ compared to each separately. Here comes the **Joint Entropy**\n\n## Joint Entropy\n\nTo review this concept let me introduce two random variables $X$ and $Y$ and they follow the probability distribution denoted by $p_{X}(x)$ and $p_Y(y)$, respectively. $(X, Y)$ has joint probability $p_{X, Y}(x, y)$. The **Joint Entropy** hence is defined as:\n\n$$\nH(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log_{2} p_{X, Y}(x, y)\n$$\n\nHere are two important facts. If $X = Y$ this implies that $H(X,Y) = H(X) = H(Y)$ and if $X$ and $Y$ are independent, then $H(X, Y) = H(X) + H(Y)$.\n\n```{python}\n\ndef joint_entropy(p_xy):\n    out = np.nansum(-p_xy * np.log2(p_xy))\n    return out\n\n```\n\n```{python}\n\njoint_entropy(np.array([[0.1, 0.5, 0.8], [0.1, 0.3, 0.02]]))\n\n```\n\nAs we see, joint entropy indicates the amount of information in the pair of two random variables. What if we are interested to know how much information is contained, say in $Y$ but not in $X$?\n\n## Conditional Entropy\n\nThe **conditional entropy** is used to measure the relationship between variables. The following formula gives this measurement:\n\n$$\nH(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log_{2} p(y \\mid x)\n$$\n\nLet investigate how conditional entropy is related to entropy and joint entropy. Using the above formula, we can conclude that:\n\n$$\nH(Y \\mid X) = H(X, Y) - H(X)\n$$\n\nmeaning that the information contained in $Y$ given $X$ equals information jointly contained in $X$ and $Y$ minus the amount of information only contained in $X$.\n\n```{python}\n\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy / p_x\n    out = np.nansum(-p_xy * np.log2(p_y_given_x))\n    return out\n\n```\n\n```{python}\n\nconditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n\n```\n\nKnowing conditional entropy means knowing the amount of information contained in $Y$ but not in $X$. Now let see how much information is shared between $X$ and $Y$.\n\n## Mutual Information\n\nTo find the **mutual information** between two random variables $X$ and $Y$, let start the process by finding all the information in both $X$ and $Y$ together and then subtract the part which is not shared. The information both in $X$ and $Y$ is $H(X, Y)$. Subtracting two conditional entropies gives:\n\n$$\nI(X, Y) = H(X, Y) - H(Y \\mid X) − H(X \\mid Y)\n$$\n\nThis means that we have to subtract the information only contained in $X$ and $Y$ to all the information at hand.\nThis relationship is perfectly described by this picture.\n\n![mutual_information](mutual_information.png)\n\nThe concept of mutual information likewise correlation coefficient, allow us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them.\n\n```{python}\n\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    out = np.nansum(p_xy * np.log2(p))\n    return out\n\n```\n\n```{python}\n\nmutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]), np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n\n```\n\nAs in the case of the correlation coefficient, mutual information has some notable properties:\n\n* Mutual information is symmetric\n* Mutual information is non-negative\n* $I(X, Y) = 0$ iff $X$ and $Y$ are independent\n\nWe can interpret the mutual information $I(X, Y)$ as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent.\n\n## Kullback–Leibler Divergence - Relative Entropy\n\nI asked the question about measuring the distance between two probability distributions. The time has come to answer this question precisely. If we have random variable $X$ which follows probability distributin $P$ and has ```p.d.f``` or ```p.m.f``` $p(x)$. Imagine we estimated $P$ with other probability distribution $Q$, which in turn has ```p.d.f``` or ```p.m.f``` $q(x)$. The distance between thse two probability distribution is measured by **Kullback–Leibler (KL) Divergence**:\n\n$$\nD_{\\mathrm{KL}}(P\\|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n$$\n\nThe lower value of the $KL$ divergence, the closer our estimate is to the actual distribution.\n\n* The KL divergence is non-symmetric or equivalently, $D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P), \\text{ if } P \\neq Q$\n* The KL divergence is non-negative or equivalently, $D_{\\mathrm{KL}}(P\\|Q) \\geq 0$\n\n```{python}\n\ndef kl_divergence(p, q):\n    kl = p * np.log2(p / q)\n    out = np.nansum(kl)\n    return np.abs(out)\n\n```\n\n```{python}\n\np = np.random.normal(1, 2, size=1000)\nq = np.random.normal(1, 2, size=1000)\n\nkl_divergence(p, q)\n\n```\n\n## Cross Entropy\n\nTo understand **Cross-Entropy**, let me use the example from the KL divergence part. Now, imagine we perform classification tasks, where $y$ is the true label, and $\\hat{y}$ is estimated label by our model. **Cross-Entropy** denoted by $\\mathrm{CE}(y, \\hat{y})$ is used as a objective function in many classification tasks in deep learning. The formula is the following:\n\n$$\n\\mathrm{CE} (P, Q) = H(P) + D_{\\mathrm{KL}}(P\\|Q)\n$$\n\nThe two terms on the right-hand side are self-information and KL divergence. $P$ is the distribution of the true labels,\nand $Q$ is the distribution of the estimated labels. As we are only interested in knowing how far we are from the actual label and $H(P)$ is also given, the above formula is reduced to minimize only the second term (KL divergence) at the right-hand side. Hence, we have\n\n$$\n\\mathrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log_{2}{p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)}\n$$\n\n```{python}\n\ndef cross_entropy(y_hat, y):\n    ce = -np.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n\n```\n\n```{python}\n\nlabels = np.array([0, 2])\npreds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n\n```\n\n\n# Conclusion\n\nBy reviewing these concepts from the information theory, we have some rough sense of how it's related to the statistics and mathematics and is used in machine learning and deep learning. There is much more to discover, and that's up to you how far you want to go. Moreover, even interesting is how information theory is related to the coding theory, in gambling and musical composition.\n\n#### References\n* [Dive Into Deep Learning](https://d2l.ai/index.html)\n* [Information theory](https://en.wikipedia.org/wiki/Information_theory)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"information_theory.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.23","theme":["flatly"],"title-block-banner":true,"comments":{"giscus":{"repo":"Okroshiashvili/okrodata","reactions-enabled":true,"input-position":"bottom","theme":"light","language":"en","loading":"lazy"}},"title":"Basics of Information Theory with Python","author":"Nodar Okroshiashvili","date":"2020-07-12","categories":["General"],"tags":["Information Theory","Information Gain","Python","Numpy"],"keywords":["information theory in python","entropy","self information","joint entropy in python","conditional entropy in python"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}