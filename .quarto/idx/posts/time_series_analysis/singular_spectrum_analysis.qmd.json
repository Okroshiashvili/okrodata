{"title":"Singular Spectrum Analysis with Python","markdown":{"yaml":{"title":"Singular Spectrum Analysis with Python","author":"Nodar Okroshiashvili","date":"2021-08-10","categories":["Time Series Analysis"],"tags":["Singular spectrum analysis","Python","Time Series Analysis"],"keywords":["time series","singular spectrum analysis","python","numpy","scipy","matplotlib","pandas","statsmodels","time series decomposition","singular value decomposition"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nIn this article, I will talk about `Singular Spectrum Analysis (SSA)` and its implementation in Python.\nFor the sake of simplicity, I will use the univariate time series and will implement basic SSA without forecasting capability. However, it is not hard to extend this method to multivariate time series and add forecasting capability as well.\n\n\nIn Time Series Analysis (TSA), the technique called `Singular Spectrum Analysis` (SSA) has been developed to use it for time series decomposition, forecasting, and noise reduction. It is a nonparametric technique, combining some elements of classical time series analysis, multivariate statistics, dynamical systems, and signal processing. Since it is nonparametric, it works with arbitrary statistical processes, whether linear or nonlinear, stationary or non-stationary, Gaussian or non-Gaussian.\n\n\n## Singular Spectrum Analysis\n\nLet dive into the technical details of the SSA. It consists of the two main steps: **decomposition** and **reconstruction**. Each above step, in turn, consists of the two sub-steps: **embedding followed by singular value decomposition (SVD)** and **eigentriple grouping followed by diagonal averaging**.\n\n\n### Decomposition\n\nAt that stage, we have to perform embedding followed by SVD. Let consider following time series: $\\mathbb{X} = (x_{1}, \\cdots, x_{N})$ of length $N$. Now let pick a window size $L$ and embed the time series into a trajectory matrix of dimension $L \\times K$, where $K = N - L + 1$:\n\nWindow length $L$ is the only hyper-parameter of the SSA. It is important to choose the right window length, because if the window length is too small, then the result will be noisy, and if the window length is too large, then the result will be too smooth. So, the window length should be chosen carefully.\n\nThe **trajectory matrix** of the series $\\mathbb{X}$ is defined as follows:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\cdots & x_K \\\\\nx_2 & x_3 & x_4 & \\cdots & x_{K+1} \\\\\nx_3 & x_4 & x_5 & \\cdots & x_{K+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n   x_L & x_{L+1} & x_{L+2} & \\cdots & x_N\n\\end{bmatrix}\n$$\n\nWe can see that, the first column of the trajectory matrix is the original time series up to a length $L$, the second column is the original time series shifted by one time step, and so on. We also see that our matrix $\\mathbf{X}$ has equal elements $x_{ij}$ on the anti-diagonals $i + j = const$. Hence, the matrix $\\mathbf{X}$ is called **[Hankel matrix](https://en.wikipedia.org/wiki/Hankel_matrix)**.\n\nInstead of implementing Hankel matrix from scratch, we can use `scipy` to hankelize any given time series. The function below takes time series and window length as input and returns Hankel matrix.\n\n```{python}\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nimport seaborn as sns\n\n```\n\n```{python}\n\ndef embed(time_series, window_length):\n    \"\"\"\n    Embed the time series into a Hankel matrix. This is the trajectory matrix.\n    \"\"\"\n    K = len(time_series) - window_length + 1\n    trajectory_matrix = scipy.linalg.hankel(time_series, np.zeros(window_length)).T[:, : K]\n    return trajectory_matrix\n\n```\n\nLet consider following time series as an example:\n\n```{python}\n\ntime_series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nwindow_length = 4\n\ntrajectory_matrix = embed(time_series, window_length)\n\ntrajectory_matrix\n\n```\n\nThe second step is to perform SVD on the trajectory matrix. First, we multiply the trajectory matrix by its transpose to form the matrix $\\mathbf{S} = XX^{T}$ and then perform SVD on the resulting matrix.\n\n[For more information how SVD works check my previous blog.](../mathematics/linear_algebra_6.qmd#singular-value-decomposition)\n\nAfter performing SVD, we get eigenvalues, sorted in decreasing order and left singular vectors of the matrix $\\mathbf{S}$. We're not interested in the right singular vectors, so we can discard them. For the next step, we also need to calculate the rank $d$ of the trajectory matrix $\\mathbf{X}$.\n\nThe function below takes trajectory matrix as input and returns rank, eigenvalues, and left singular vectors.\n\n```{python}\n\ndef decompose(trajectory_matrix):\n    \"\"\"\n    Decompose the trajectory matrix into its singular values and vectors.\n    \"\"\"\n    S = np.matmul(trajectory_matrix, trajectory_matrix.T)\n    singular_vectors, singular_values, _ = scipy.linalg.svd(S)\n    singular_values = np.sqrt(singular_values)\n    rank = np.linalg.matrix_rank(trajectory_matrix)\n    return rank, singular_values, singular_vectors\n\n```\n\n```{python}\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\nprint(\"Rank: \", rank)\nprint(\"Singular values: \", singular_values, sep=\"\\n\")\nprint(\"Singular vectors: \", singular_vectors, sep=\"\\n\")\n\n```\n\n\n### Reconstruction\n\nAt that stage, we have to perform eigentriple grouping followed by diagonal averaging. The SVD step of the trajectory matrix is not finished. I split it into two parts for better understanding. The first part was actual decomposition, and the second part is grouping to form elementary matrices.\n\nThe function below takes trajectory matrix, rank, eigenvalues, and left singular vectors as input and returns grouped matrices. The rank of each matrix is 1, hence the name **elementary matrices**. The collection $(\\sqrt{\\lambda_{i}}, U_{i}, V_i{})$ is called **eigentriple** of the SVD. Not to be confused with $V_{i}$. It is not the right singular vector, but the following matrix $V_{i} = \\\\ \\mathbf{X}^{T}U_{i}/{\\sqrt{\\lambda_{i}}}$\n\n```{python}\n\ndef group(trajectory_matrix, rank, singular_values, singular_vectors):\n    \"\"\"\n    Group the singular values and vectors into matrices.\n    \"\"\"\n    V = {i: np.matmul(trajectory_matrix.T, singular_vectors[:, i]) / singular_values[i] for i in range(rank)}\n    # The rank of each matrix is 1\n    X = {\n        i: np.matmul(singular_values[i] * singular_vectors[:, i].reshape(-1, 1), V[i].reshape(-1, 1).T)\n        for i in range(rank)\n    }\n    return X\n\n```\n\n```{python}\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\nX\n\n```\n\nAt the diagonal averaging step, each matrix $\\mathbf{X_{I_{j}}}$ of the grouped decomposition is henkelized and then the obtained Hankel matrix is transformed into a new series of length $N$. Diagonal averaging applied to the resulted matrix $\\mathbf{X_{I_{k}}}$ produces a reconstructed series $\\mathbb{\\tilde{X^{(k)}}} = (\\tilde{x}^{(k)}_{1}, \\cdots, \\tilde{x}^{(k)}_{N})$. Therefore, the initial series is decomposed into a sum of $m$ reconstructed subseries:\n\n$$\nx_{n} = \\sum_{k=1}^{m} \\tilde{x}^{(k)}_{n}, \\quad n = 1, \\cdots, N\n$$\n\nThe function below takes grouped matrices as input and returns reconstructed time series.\n\n```{python}\n\ndef diagonal_averaging(X, m):\n    \"\"\"\n    Perform anti-diagonal averaging of the given hankel matrix\n    \"\"\"\n    if m > len(X):\n        raise ValueError(\"Number of singular values cannot be greater than the rank of the trajectory matrix.\")\n\n    result = []\n    matrix = np.sum([X[i] for i in range(m)], axis=(0))\n    rows, columns = matrix.shape\n    rows_star, columns_star = min(rows, columns), max(rows, columns)\n\n    for k in range(1 - columns_star, rows_star):\n        identity_matrix = np.eye(columns_star, k=k, dtype=\"bool\")[::-1][:rows_star, :]\n        identity_matrix_sum = np.sum(identity_matrix)\n        mask = np.ma.masked_array(matrix, mask=1 - identity_matrix)\n        average = mask.sum() / identity_matrix_sum\n        result.append(average)\n\n    return pd.DataFrame(result).rename(columns={0: \"reconstruction\"})\n\n```\n\nHere, I pick the first matrix of the grouped decomposition and perform diagonal averaging. The result is a reconstructed time series.\n\n```{python}\n\nreconstructed_time_series = diagonal_averaging(X, 1)\n\nreconstructed_time_series\n\n```\n\nHere is the result for the first two matrices. We see that it produces exactly the same series as initial one.\n\n```{python}\n\nreconstructed_time_series = diagonal_averaging(X, 2)\n\nreconstructed_time_series\n\n```\n\n\nWe can also visualize how much each singular value contributes to the variance.\n\n```{python}\n\ndef plot_contributions(trajectory_matrix, singular_values):\n    \"\"\"View the contribution to variance of each singular value and its corresponding signal\"\"\"\n    lambdas = np.power(singular_values, 2)\n    norm = np.linalg.norm(trajectory_matrix)\n    contributions = pd.DataFrame((lambdas / (norm**2)).round(4), columns=[\"contribution\"])\n    # Filter out the contributions that are zero\n    contributions = contributions[contributions[\"contribution\"] > 0]\n    # Adjust the scale of the contributions\n    contributions[\"contribution\"] = (1 / contributions[\"contribution\"]).max() * 1.1 - (\n        1 / contributions[\"contribution\"]\n    )\n\n    # Plot the contributions\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(11, 4))\n    plt.axes().get_yaxis().set_ticks([])\n    sns.barplot(data=contributions, x=contributions.index, y=\"contribution\")\n    plt.title(\"Non-zero contributions of singular values to variance\")\n    plt.xlabel(\"Singular value\")\n    plt.ylabel(\"Contribution\")\n    plt.show()\n\n```\n\n\n```{python}\n\nplot_contributions(trajectory_matrix, singular_values)\n\n```\n\nFrom the above plot we see that the first singular value contributes the most to the variance. The second singular value contributes less.\n\n### Real Life Example\n\nI guess, the above example does not make much sense as it's not something real. Let consider real life data and go thru all the steps one by one. You can download [the data from here](https://www.kaggle.com/datasets/nodarokroshiashvili/time-series).\n\nLet read the data and replicate above steps.\n\n```{python}\n\nts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\");\n\n```\n\n\nWe see that the data is characterized with some seasonality. Due to that reason, we will use a window size of 24.\n\n```{python}\n\ntrajectory_matrix = embed(ts, window_length=24)\n\ntrajectory_matrix\n\n```\n\n```{python}\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\n```\n\n```{python}\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\n```\n\n```{python}\n\nreconstructed = diagonal_averaging(X, m=12)\n\nts[\"reconstruction\"] = reconstructed[\"reconstruction\"].values\n\n```\n\n\n```{python}\n\n# Plot original and reconstructed series\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\", label=\"Original\")\nsns.lineplot(data=ts, x=ts.index, y=\"reconstruction\", label=\"Reconstructed\");\n\n```\n\n\n# Conclusion\n\nThe result seems quite impressive. The implementation was not hard and the result is quite good. Adding forecasting capability and extending this method to multivariate time series should not be hard. For more detailed information, please refer to the references below.\n\n\n#### References\n* [Singular Spectrum Analysis for Time Series](https://www.researchgate.net/publication/260124592_Singular_Spectrum_Analysis_for_Time_Series)\n* [A Brief Introduction to Singular Spectrum Analysis](https://ssa.cf.ac.uk/ssa2010/a_brief_introduction_to_ssa.pdf)\n* [Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis)\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nIn this article, I will talk about `Singular Spectrum Analysis (SSA)` and its implementation in Python.\nFor the sake of simplicity, I will use the univariate time series and will implement basic SSA without forecasting capability. However, it is not hard to extend this method to multivariate time series and add forecasting capability as well.\n\n\nIn Time Series Analysis (TSA), the technique called `Singular Spectrum Analysis` (SSA) has been developed to use it for time series decomposition, forecasting, and noise reduction. It is a nonparametric technique, combining some elements of classical time series analysis, multivariate statistics, dynamical systems, and signal processing. Since it is nonparametric, it works with arbitrary statistical processes, whether linear or nonlinear, stationary or non-stationary, Gaussian or non-Gaussian.\n\n\n## Singular Spectrum Analysis\n\nLet dive into the technical details of the SSA. It consists of the two main steps: **decomposition** and **reconstruction**. Each above step, in turn, consists of the two sub-steps: **embedding followed by singular value decomposition (SVD)** and **eigentriple grouping followed by diagonal averaging**.\n\n\n### Decomposition\n\nAt that stage, we have to perform embedding followed by SVD. Let consider following time series: $\\mathbb{X} = (x_{1}, \\cdots, x_{N})$ of length $N$. Now let pick a window size $L$ and embed the time series into a trajectory matrix of dimension $L \\times K$, where $K = N - L + 1$:\n\nWindow length $L$ is the only hyper-parameter of the SSA. It is important to choose the right window length, because if the window length is too small, then the result will be noisy, and if the window length is too large, then the result will be too smooth. So, the window length should be chosen carefully.\n\nThe **trajectory matrix** of the series $\\mathbb{X}$ is defined as follows:\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\cdots & x_K \\\\\nx_2 & x_3 & x_4 & \\cdots & x_{K+1} \\\\\nx_3 & x_4 & x_5 & \\cdots & x_{K+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n   x_L & x_{L+1} & x_{L+2} & \\cdots & x_N\n\\end{bmatrix}\n$$\n\nWe can see that, the first column of the trajectory matrix is the original time series up to a length $L$, the second column is the original time series shifted by one time step, and so on. We also see that our matrix $\\mathbf{X}$ has equal elements $x_{ij}$ on the anti-diagonals $i + j = const$. Hence, the matrix $\\mathbf{X}$ is called **[Hankel matrix](https://en.wikipedia.org/wiki/Hankel_matrix)**.\n\nInstead of implementing Hankel matrix from scratch, we can use `scipy` to hankelize any given time series. The function below takes time series and window length as input and returns Hankel matrix.\n\n```{python}\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.linalg\nimport seaborn as sns\n\n```\n\n```{python}\n\ndef embed(time_series, window_length):\n    \"\"\"\n    Embed the time series into a Hankel matrix. This is the trajectory matrix.\n    \"\"\"\n    K = len(time_series) - window_length + 1\n    trajectory_matrix = scipy.linalg.hankel(time_series, np.zeros(window_length)).T[:, : K]\n    return trajectory_matrix\n\n```\n\nLet consider following time series as an example:\n\n```{python}\n\ntime_series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nwindow_length = 4\n\ntrajectory_matrix = embed(time_series, window_length)\n\ntrajectory_matrix\n\n```\n\nThe second step is to perform SVD on the trajectory matrix. First, we multiply the trajectory matrix by its transpose to form the matrix $\\mathbf{S} = XX^{T}$ and then perform SVD on the resulting matrix.\n\n[For more information how SVD works check my previous blog.](../mathematics/linear_algebra_6.qmd#singular-value-decomposition)\n\nAfter performing SVD, we get eigenvalues, sorted in decreasing order and left singular vectors of the matrix $\\mathbf{S}$. We're not interested in the right singular vectors, so we can discard them. For the next step, we also need to calculate the rank $d$ of the trajectory matrix $\\mathbf{X}$.\n\nThe function below takes trajectory matrix as input and returns rank, eigenvalues, and left singular vectors.\n\n```{python}\n\ndef decompose(trajectory_matrix):\n    \"\"\"\n    Decompose the trajectory matrix into its singular values and vectors.\n    \"\"\"\n    S = np.matmul(trajectory_matrix, trajectory_matrix.T)\n    singular_vectors, singular_values, _ = scipy.linalg.svd(S)\n    singular_values = np.sqrt(singular_values)\n    rank = np.linalg.matrix_rank(trajectory_matrix)\n    return rank, singular_values, singular_vectors\n\n```\n\n```{python}\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\nprint(\"Rank: \", rank)\nprint(\"Singular values: \", singular_values, sep=\"\\n\")\nprint(\"Singular vectors: \", singular_vectors, sep=\"\\n\")\n\n```\n\n\n### Reconstruction\n\nAt that stage, we have to perform eigentriple grouping followed by diagonal averaging. The SVD step of the trajectory matrix is not finished. I split it into two parts for better understanding. The first part was actual decomposition, and the second part is grouping to form elementary matrices.\n\nThe function below takes trajectory matrix, rank, eigenvalues, and left singular vectors as input and returns grouped matrices. The rank of each matrix is 1, hence the name **elementary matrices**. The collection $(\\sqrt{\\lambda_{i}}, U_{i}, V_i{})$ is called **eigentriple** of the SVD. Not to be confused with $V_{i}$. It is not the right singular vector, but the following matrix $V_{i} = \\\\ \\mathbf{X}^{T}U_{i}/{\\sqrt{\\lambda_{i}}}$\n\n```{python}\n\ndef group(trajectory_matrix, rank, singular_values, singular_vectors):\n    \"\"\"\n    Group the singular values and vectors into matrices.\n    \"\"\"\n    V = {i: np.matmul(trajectory_matrix.T, singular_vectors[:, i]) / singular_values[i] for i in range(rank)}\n    # The rank of each matrix is 1\n    X = {\n        i: np.matmul(singular_values[i] * singular_vectors[:, i].reshape(-1, 1), V[i].reshape(-1, 1).T)\n        for i in range(rank)\n    }\n    return X\n\n```\n\n```{python}\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\nX\n\n```\n\nAt the diagonal averaging step, each matrix $\\mathbf{X_{I_{j}}}$ of the grouped decomposition is henkelized and then the obtained Hankel matrix is transformed into a new series of length $N$. Diagonal averaging applied to the resulted matrix $\\mathbf{X_{I_{k}}}$ produces a reconstructed series $\\mathbb{\\tilde{X^{(k)}}} = (\\tilde{x}^{(k)}_{1}, \\cdots, \\tilde{x}^{(k)}_{N})$. Therefore, the initial series is decomposed into a sum of $m$ reconstructed subseries:\n\n$$\nx_{n} = \\sum_{k=1}^{m} \\tilde{x}^{(k)}_{n}, \\quad n = 1, \\cdots, N\n$$\n\nThe function below takes grouped matrices as input and returns reconstructed time series.\n\n```{python}\n\ndef diagonal_averaging(X, m):\n    \"\"\"\n    Perform anti-diagonal averaging of the given hankel matrix\n    \"\"\"\n    if m > len(X):\n        raise ValueError(\"Number of singular values cannot be greater than the rank of the trajectory matrix.\")\n\n    result = []\n    matrix = np.sum([X[i] for i in range(m)], axis=(0))\n    rows, columns = matrix.shape\n    rows_star, columns_star = min(rows, columns), max(rows, columns)\n\n    for k in range(1 - columns_star, rows_star):\n        identity_matrix = np.eye(columns_star, k=k, dtype=\"bool\")[::-1][:rows_star, :]\n        identity_matrix_sum = np.sum(identity_matrix)\n        mask = np.ma.masked_array(matrix, mask=1 - identity_matrix)\n        average = mask.sum() / identity_matrix_sum\n        result.append(average)\n\n    return pd.DataFrame(result).rename(columns={0: \"reconstruction\"})\n\n```\n\nHere, I pick the first matrix of the grouped decomposition and perform diagonal averaging. The result is a reconstructed time series.\n\n```{python}\n\nreconstructed_time_series = diagonal_averaging(X, 1)\n\nreconstructed_time_series\n\n```\n\nHere is the result for the first two matrices. We see that it produces exactly the same series as initial one.\n\n```{python}\n\nreconstructed_time_series = diagonal_averaging(X, 2)\n\nreconstructed_time_series\n\n```\n\n\nWe can also visualize how much each singular value contributes to the variance.\n\n```{python}\n\ndef plot_contributions(trajectory_matrix, singular_values):\n    \"\"\"View the contribution to variance of each singular value and its corresponding signal\"\"\"\n    lambdas = np.power(singular_values, 2)\n    norm = np.linalg.norm(trajectory_matrix)\n    contributions = pd.DataFrame((lambdas / (norm**2)).round(4), columns=[\"contribution\"])\n    # Filter out the contributions that are zero\n    contributions = contributions[contributions[\"contribution\"] > 0]\n    # Adjust the scale of the contributions\n    contributions[\"contribution\"] = (1 / contributions[\"contribution\"]).max() * 1.1 - (\n        1 / contributions[\"contribution\"]\n    )\n\n    # Plot the contributions\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(11, 4))\n    plt.axes().get_yaxis().set_ticks([])\n    sns.barplot(data=contributions, x=contributions.index, y=\"contribution\")\n    plt.title(\"Non-zero contributions of singular values to variance\")\n    plt.xlabel(\"Singular value\")\n    plt.ylabel(\"Contribution\")\n    plt.show()\n\n```\n\n\n```{python}\n\nplot_contributions(trajectory_matrix, singular_values)\n\n```\n\nFrom the above plot we see that the first singular value contributes the most to the variance. The second singular value contributes less.\n\n### Real Life Example\n\nI guess, the above example does not make much sense as it's not something real. Let consider real life data and go thru all the steps one by one. You can download [the data from here](https://www.kaggle.com/datasets/nodarokroshiashvili/time-series).\n\nLet read the data and replicate above steps.\n\n```{python}\n\nts = pd.read_csv(\"TS.csv\", parse_dates=True, index_col=\"date\")\n\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\");\n\n```\n\n\nWe see that the data is characterized with some seasonality. Due to that reason, we will use a window size of 24.\n\n```{python}\n\ntrajectory_matrix = embed(ts, window_length=24)\n\ntrajectory_matrix\n\n```\n\n```{python}\n\nrank, singular_values, singular_vectors = decompose(trajectory_matrix)\n\n```\n\n```{python}\n\nX = group(trajectory_matrix, rank, singular_values, singular_vectors)\n\n```\n\n```{python}\n\nreconstructed = diagonal_averaging(X, m=12)\n\nts[\"reconstruction\"] = reconstructed[\"reconstruction\"].values\n\n```\n\n\n```{python}\n\n# Plot original and reconstructed series\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(11, 4))\n\nsns.lineplot(data=ts, x=ts.index, y=\"price\", label=\"Original\")\nsns.lineplot(data=ts, x=ts.index, y=\"reconstruction\", label=\"Reconstructed\");\n\n```\n\n\n# Conclusion\n\nThe result seems quite impressive. The implementation was not hard and the result is quite good. Adding forecasting capability and extending this method to multivariate time series should not be hard. For more detailed information, please refer to the references below.\n\n\n#### References\n* [Singular Spectrum Analysis for Time Series](https://www.researchgate.net/publication/260124592_Singular_Spectrum_Analysis_for_Time_Series)\n* [A Brief Introduction to Singular Spectrum Analysis](https://ssa.cf.ac.uk/ssa2010/a_brief_introduction_to_ssa.pdf)\n* [Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"singular_spectrum_analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.23","theme":["flatly"],"title-block-banner":true,"comments":{"giscus":{"repo":"Okroshiashvili/okrodata","reactions-enabled":true,"input-position":"bottom","theme":"light","language":"en","loading":"lazy"}},"title":"Singular Spectrum Analysis with Python","author":"Nodar Okroshiashvili","date":"2021-08-10","categories":["Time Series Analysis"],"tags":["Singular spectrum analysis","Python","Time Series Analysis"],"keywords":["time series","singular spectrum analysis","python","numpy","scipy","matplotlib","pandas","statsmodels","time series decomposition","singular value decomposition"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}