{"title":"Advance Linear Algebra with Python - Part II","markdown":{"yaml":{"title":"Advance Linear Algebra with Python - Part II","author":"Nodar Okroshiashvili","date":"2021-05-13","categories":["Mathematics"],"tags":["Linear Algebra","Advance Topics"],"keywords":["advance linear algebra","matrix decompositions in python","linear algebra advances in python","advance linear algebra for machine learning"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis is the **sixth** post in the blog series about linear algebra, solely devoted to various matrix decompositions.\n\n\n1. [Introduction to Linear Algebra with Python](linear_algebra_1.qmd)\n2. [Basic Linear Algebra with Python](linear_algebra_2.qmd)\n3. Intermediate linear algebra\n   1. [Intermediate Linear Algebra with Python - Part I](linear_algebra_3.qmd)\n   2. [Intermediate Linear Algebra with Python - Part II](linear_algebra_4.qmd)\n4. **Advanced linear algebra**\n   1. [Advance Linear Algebra with Python - Part I](linear_algebra_5.qmd)\n   2. **Advance Linear Algebra with Python - Part II**\n\n\nIn this post I will introduce different types of matrix decompositions, mostly applicable to machine learning or deep learning.\n\nMatrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices.\nFactorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix.\nThese techniques have a wide variety of uses and consequently, there exist several types of decompositions.\n\n\n## Cholesky Decomposition\n\nThe Cholesky Decomposition is the factorization of a given **symmetric** square matrix $A$ into the product of a\nlower triangular matrix, denoted by $L$ and its transpose $L^{T}$. This decomposition is named after French artillery\nofficer [Andre-Louis Cholesky](https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky). The formula is:\n\n$$\nA =\nLL^{T}\n$$\n\nFor rough sense, let $A$ be\n\n$$\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n$$\n\nthen we can represent $A$ as\n\n$$\nA = LL^{T} =\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0 \\\\\nl_{31} & l_{32} & l_{33}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nl_{11} & l_{12} & l_{13} \\\\\n0 & l_{22} & l_{23} \\\\\n0 & 0 & a_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\nl_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\nl_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\nl_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n\\end{bmatrix}\n$$\n\nThe diagonal elements of matrix $L$ can be calculated by the following formulas:\n\n$$\nl_{11} = \\sqrt{a_{11}}\n\\quad \\quad\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n\\quad \\quad\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n$$\n\nand in general, for diagonal elements of the matrix $L$ we have:\n\n$$\nl_{kk} =\n\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n$$\n\nFor the elements below the main diagonal, $l_{ik}$ where $i > k$, the formulas are\n\n$$\nl_{21} = \\frac{1}{l_{11}}a_{21}\n\\quad \\quad\nl_{31} = \\frac{1}{l_{11}}a_{31}\n\\quad \\quad\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n$$\n\nand the general formula is\n\n$$\nl_{ik} =\n\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n$$\n\nMessy formulas! Consider a numerical example to see what happen under the hood. We have a matrix $A$\n\n$$\nA =\n\\begin{bmatrix}\n25 & 15 & -5 \\\\\n15 & 18 & 0 \\\\\n-5 & 0 & 11\n\\end{bmatrix}\n$$\n\nAccording to the above formulas, let find a lower triangular matrix $L$. We have\n\n$$\nl_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n$$\n$$\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n$$\n$$\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n$$\n\nSeems, we have missing non-diagonal elements, which are\n\n$$\nl_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n$$\n$$\nl_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n$$\n$$\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n$$\n\nSo, our matrix $L$ is\n\n$$\nL =\n\\begin{bmatrix}\n5 & 0 & 0 \\\\\n3 & 3 & 0 \\\\\n-1 & 1 & 3\n\\end{bmatrix}\n\\quad \\quad\nL^{T} =\n\\begin{bmatrix}\n5 & 3 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n$$\n\nMultiplication of this matrices is up to you.\n\n```{python}\n\nimport numpy as np\n\n```\n\n```{python}\n\nA = np.array([[25, 15, -5], [15, 18, 0], [-5, 0, 11]])\n\n# Cholesky decomposition, find lower triangular matrix L\nL = np.linalg.cholesky(A)\n\n# Take transpose\nL_T = np.transpose(L)\n\n# Check if it's correct\nA == np.dot(L, L_T)\n\n```\n\n\n## QR Decomposition\n\nQR decomposition is another type of matrix factorization, where a given $m \\times n$ matrix $A$ is decomposed into\ntwo matrices, $Q$ which is orthogonal matrix, which in turn means that $QQ^{T} = Q^{T}Q = I$ and the inverse of $Q$\nequal to its transpose, $Q^{T} = Q^{-1}$, and $R$ which is upper triangular matrix. Hence, the formula is given by\n\n$$\nA =\nQR\n$$\n\nAs $Q$ is an orthogonal matrix, there are three methods to find $Q$, one is [Gramm-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process),\nsecond is [Householder Transformation](https://en.wikipedia.org/wiki/Householder_transformation),\nand third is [Givens Rotation](https://en.wikipedia.org/wiki/Givens_rotation). These methods are out of the scope of this blog post series and hence I'm going to explain all of them in a separate blog post.\n\n```{python}\n\nA = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n\n# QR decomposition\nQ, R = np.linalg.qr(A)\n\nprint(\"Q = \", Q, sep=\"\\n\")\nprint(\"R = \", R, sep=\"\\n\")\nprint(\"A = QR\", np.dot(Q, R), sep=\"\\n\")\n\n```\n\n\n## Eigen Decomposition\n\nHere is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform\nmatrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the\neigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n\n$$\nA = Q \\Lambda Q^{-1}\n$$\n\n$A$ is $n\\times n$ square matrix, $Q$ is the matrix whose columns are the eigenvectors, which in turn are linearly\nindependent and $\\Lambda$ is diagonal matrix of eigenvalues of $A$ and these eigenvalues are not necessarily distinct.\n\nTo see the detailed steps of this decomposition, consider the aforementioned example of the matrix $A$\nfor which we already found eigenvalues and eigenvectors.\n\n$$\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n$$\n$$\nQ =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n-2 & 0 & 1 \\\\\n1 & 0 & 2\n\\end{bmatrix}\n$$\n$$\n\\Lambda =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 11\n\\end{bmatrix}\n$$\n$$\nQ^{-1} =\n\\begin{bmatrix}\n0 & -0.4 & 0.2 \\\\\n1 & 0 & 0 \\\\\n0 & 0.2 & 0.4\n\\end{bmatrix}\n$$\n\nWe have all the matrices and now take matrix multiplication according to the above formula. Particularly,\nmultiply $Q$ by $\\Lambda$ and by $Q^{-1}$. We have to get original matrix $A$\n\nFurthermore, if matrix $A$ is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n\n$$\nA = Q \\Lambda Q^{T}\n$$\n\nThe only difference between this formula and above formula is that the matrix $A$ is $n\\times n$ real symmetric square\nmatrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real\nsymmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n\n$$\nA =\n\\begin{bmatrix}\n6 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n$$\n\nThe matrix is symmetric because of the original matrix equal to its transpose, $A = A^{T}$\n\nIts eigenvalues are $\\lambda_{1} = 7$ and $\\lambda_{2} = 2$ and corresponding eigenvectors are\n\n$$\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n0.89442719 \\\\\n0.4472136\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n-0.4472136 \\\\\n0.89442719\n\\end{bmatrix}\n$$\n\nAnd in this set up, matrices $Q$, $\\Lambda$ and $Q^{T}$ are the following:\n\n$$\nQ =\n\\begin{bmatrix}\n0.89442719 & -0.4472136 \\\\\n0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n$$\n$$\n\\Lambda = \n\\begin{bmatrix}\n7 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n$$\n$$\nQ^{T} =\n\\begin{bmatrix}\n0.89442719 & 0.4472136 \\\\\n-0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n$$\n\nTaking matrix product gives initial matrix $A$.\n\n**Eigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.**\n\n```{python}\n\n# Eigendecomposition for non-symmetric matrix\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\neigenvalues1, eigenvectors1 = np.linalg.eig(A)\n# Form diagonal matrix from eigenvalues\nL1 = np.diag(eigenvalues1)\n\n# Separate eigenvector matrix and take its inverse\nQ1 = eigenvectors1\ninv_Q = np.linalg.inv(Q1)\nB = np.dot(np.dot(Q1, L1), inv_Q)\n\n# Check if B equal to A\nprint(\"Decomposed matrix B: \", B, sep=\"\\n\")\n\n# Numpy produces normalized eigenvectors and don't be confused with my calculations above\n\n\n# Eigendecomposition for symmetric matrix\nC = np.array([[6, 2], [2, 3]])\neigenvalues2, eigenvectors2 = np.linalg.eig(C)\n\n# Eigenvalues and Eigenvectors\nL2 = np.diag(eigenvalues2)\nQ2 = eigenvectors2\nQ2_T = Q2.T\n\nD = np.dot(np.dot(Q2, L2), Q2.T)\n\n# Check if D equal to C\nprint(\"Decomposed matrix D: \", D, sep=\"\\n\")\n\n```\n\n\n## Singular Value Decomposition\n\nSingular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition.\nIn this context, generalization means that eigendecomposition is applicable only for square $n \\times n$ matrices,\nwhile Singular Value Decomposition (SVD) is applicable for any $m \\times n$ matrices.\n\nSVD for a $m \\times n$ matrix $A$ is computed by the following formula:\n\n$$\nA = U \\ D \\ V^{T}\n$$\n\nWhere, $U$'s columns are *left singular vectors* of $A$, $V$'s columns are *right singular vectors* of $A$ and $D$  is a\ndiagonal matrix, not necessarily square matrix, containing **singular values** of $A$ on main diagonal.\nSingular values of $m \\times n$ matrix $A$ are the **square roots of the eigenvalues** of $A^{T}A$, which is a square matrix.\nIf our initial matrix $A$ is square or $n \\times n$ then singular values **coincide** eigenvalues.\nMoreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\n\nMatrices, $U$, $D$, and $V$ can be found by transforming $A$ into a square matrix and computing eigenvalues and\neigenvectors of this transformed matrix. This transformation is done by multiplying $A$ by its transpose $A^{T}$.\nAfter that, matrices $U$, $D$ and $V$ are the following:\n\n* $U$ corresponds to the eigenvectors of $AA^{T}$\n\n* $V$ corresponds to eigenvectors of $A^{T}A$\n\n* $D$ corresponds to eigenvalues, either $AA^{T}$ or $A^{T}A$, which are the same\n\nTheory almost always seems confusing. Consider a numerical example and Python code below for clarification.\n\nLet our initial matrix $A$ be:\n\n$$\nA =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n\\sqrt{2} & 2 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n$$\n\nHere, to use SVD first we need to find $AA^{T}$ and $A^{T}A$.\n\n$$\nAA^{T} =\n\\begin{bmatrix}\n2 & 2 & 2 \\\\\n2 & 6 & 2 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\quad\nA^{T}A =\n\\begin{bmatrix}\n2 & 2\\sqrt{2} & 0 \\\\\n2\\sqrt{2} & 6 & 2 \\\\\n0 & 2 & 2\n\\end{bmatrix}\n$$\n\nIn the next step, we have to find eigenvalues and eigenvectors for $AA^{T}$ and $A^{T}A$. The characteristic polynomial is\n\n$$\n-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n$$\n\nwith roots equal to $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$. Note that these eigenvalues are\nthe same for the $A^{T}A$. We need singular values which are square root from eigenvalues.\nLet denote them by $\\sigma$ such as $\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_{2} = \\sqrt{2}$ and $\\sigma_{3} = \\sqrt{0} = 0$.\nWe now can construct diagonal matrix of singular values:\n\n$$\nD =\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n\nNow we have to find matrices $U$ and $V$. We have everything what we need. First find eigenvectors\nof $AA^{T}$ for $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$, which are the following:\n\n$$\nU_{1} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}}\\\\\n\\frac{2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{6}}\n\\end{bmatrix}\n\\quad\nU_{2} =\n\\begin{bmatrix}\n-\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}} \\\\\n-\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n\\quad\nU_{3} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}\\\\\n0 \\\\\n-\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\n\nNote that eigenvectors are normalized.\n\nAs we have eigenvectors, our $U$ matrix is:\n\n$$\nU =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\n\nIn the same fashion, we can find matrix $V$, which is:\n\n$$\nV =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n$$\n\nAccording to the formula we have\n\n$$\nA = U \\ D \\ V^{T} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n^{T} = A\n$$\n\n```{python}\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nA = np.array([[0, 1, 0], [np.sqrt(2), 2, 0], [0, 1, 1]])\nU, D, V = np.linalg.svd(A)\n\nprint(\"U = \", U, sep=\"\\n\")\nprint(\"D = \", D, sep=\"\\n\")\nprint(\"V = \", V, sep=\"\\n\")\n\nB = np.dot(U, np.dot(np.diag(D), V))\nprint(\"B = \", B, sep=\"\\n\")\n\n```\n\n\n## Inverse of a Square Full Rank Matrix\n\nHere, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition.\nLet's get started. If a matrix $A$ can be eigendecomposed and it has no any eigenvalue equal to zero,\nthen this matrix has the inverse and this inverse is given by:\n\n$$\nA^{-1} =\nQ \\Lambda^{-1} Q^{-1}\n$$\n\nMatrices, $Q$, and $\\Lambda$ are already known for us. Consider an example:\n\n$$\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n4 & 3\n\\end{bmatrix}\n$$\n\nIts eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = 5$ and eigenvectors are:\n\n$$\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n-0.70710678 \\\\\n0.70710678\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n0.4472136 \\\\\n-0.89442719\n\\end{bmatrix}\n$$\n\nLet calculate the inverse of $A$\n\n$$\nA^{-1} = Q \\Lambda^{-1} Q^{-1} =\n$$\n$$\n=\n\\begin{bmatrix}\n-0.70710678 & -0.4472136 \\\\\n0.70710678 & -0.89442719\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-1 & -0 \\\\\n0 & 0.2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-0.94280904 & 0.47140452 \\\\\n-0.74535599 & -0.74535599\n\\end{bmatrix} =\n$$\n$$\n=\n\\begin{bmatrix}\n-0.6 & 0.4 \\\\\n0.8 & -0.2\n\\end{bmatrix}\n$$\n\n```{python}\n\nA = np.array([[1, 2], [4, 3]])\n\n# Eigenvalues and Eigenvectors\nL, Q = np.linalg.eig(A)\n# Diagonal eigenvalues\nL = np.diag(L)\n# Inverse\ninv_L = np.linalg.inv(L)\n# Inverse of igenvector matrix\ninv_Q = np.linalg.inv(Q)\n# Calculate the inverse of A\ninv_A = np.dot(Q, np.dot(inv_L, inv_Q))\n\n# Print the inverse\nprint(\"The inverse of A is: \", inv_A, sep=\"\\n\")\n\n```\n\n\n# Conclusion\n\nIn conclusion, my aim was to make linear algebra tutorials, which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books, which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant topics for machine learning and deep learning.\n\nBased on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should \nbe continuous. Meaning, that you have to practice more and more.\n\n\n#### References\n* [Cholesky Decomposition](https://rosettacode.org/wiki/Cholesky_decomposition)\n* [Matrix Decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition)\n* [Introduction To Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nThis is the **sixth** post in the blog series about linear algebra, solely devoted to various matrix decompositions.\n\n\n1. [Introduction to Linear Algebra with Python](linear_algebra_1.qmd)\n2. [Basic Linear Algebra with Python](linear_algebra_2.qmd)\n3. Intermediate linear algebra\n   1. [Intermediate Linear Algebra with Python - Part I](linear_algebra_3.qmd)\n   2. [Intermediate Linear Algebra with Python - Part II](linear_algebra_4.qmd)\n4. **Advanced linear algebra**\n   1. [Advance Linear Algebra with Python - Part I](linear_algebra_5.qmd)\n   2. **Advance Linear Algebra with Python - Part II**\n\n\nIn this post I will introduce different types of matrix decompositions, mostly applicable to machine learning or deep learning.\n\nMatrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices.\nFactorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix.\nThese techniques have a wide variety of uses and consequently, there exist several types of decompositions.\n\n\n## Cholesky Decomposition\n\nThe Cholesky Decomposition is the factorization of a given **symmetric** square matrix $A$ into the product of a\nlower triangular matrix, denoted by $L$ and its transpose $L^{T}$. This decomposition is named after French artillery\nofficer [Andre-Louis Cholesky](https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky). The formula is:\n\n$$\nA =\nLL^{T}\n$$\n\nFor rough sense, let $A$ be\n\n$$\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n$$\n\nthen we can represent $A$ as\n\n$$\nA = LL^{T} =\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0 \\\\\nl_{31} & l_{32} & l_{33}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nl_{11} & l_{12} & l_{13} \\\\\n0 & l_{22} & l_{23} \\\\\n0 & 0 & a_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\nl_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\nl_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\nl_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n\\end{bmatrix}\n$$\n\nThe diagonal elements of matrix $L$ can be calculated by the following formulas:\n\n$$\nl_{11} = \\sqrt{a_{11}}\n\\quad \\quad\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n\\quad \\quad\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n$$\n\nand in general, for diagonal elements of the matrix $L$ we have:\n\n$$\nl_{kk} =\n\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n$$\n\nFor the elements below the main diagonal, $l_{ik}$ where $i > k$, the formulas are\n\n$$\nl_{21} = \\frac{1}{l_{11}}a_{21}\n\\quad \\quad\nl_{31} = \\frac{1}{l_{11}}a_{31}\n\\quad \\quad\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n$$\n\nand the general formula is\n\n$$\nl_{ik} =\n\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n$$\n\nMessy formulas! Consider a numerical example to see what happen under the hood. We have a matrix $A$\n\n$$\nA =\n\\begin{bmatrix}\n25 & 15 & -5 \\\\\n15 & 18 & 0 \\\\\n-5 & 0 & 11\n\\end{bmatrix}\n$$\n\nAccording to the above formulas, let find a lower triangular matrix $L$. We have\n\n$$\nl_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n$$\n$$\nl_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n$$\n$$\nl_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n$$\n\nSeems, we have missing non-diagonal elements, which are\n\n$$\nl_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n$$\n$$\nl_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n$$\n$$\nl_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n$$\n\nSo, our matrix $L$ is\n\n$$\nL =\n\\begin{bmatrix}\n5 & 0 & 0 \\\\\n3 & 3 & 0 \\\\\n-1 & 1 & 3\n\\end{bmatrix}\n\\quad \\quad\nL^{T} =\n\\begin{bmatrix}\n5 & 3 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n$$\n\nMultiplication of this matrices is up to you.\n\n```{python}\n\nimport numpy as np\n\n```\n\n```{python}\n\nA = np.array([[25, 15, -5], [15, 18, 0], [-5, 0, 11]])\n\n# Cholesky decomposition, find lower triangular matrix L\nL = np.linalg.cholesky(A)\n\n# Take transpose\nL_T = np.transpose(L)\n\n# Check if it's correct\nA == np.dot(L, L_T)\n\n```\n\n\n## QR Decomposition\n\nQR decomposition is another type of matrix factorization, where a given $m \\times n$ matrix $A$ is decomposed into\ntwo matrices, $Q$ which is orthogonal matrix, which in turn means that $QQ^{T} = Q^{T}Q = I$ and the inverse of $Q$\nequal to its transpose, $Q^{T} = Q^{-1}$, and $R$ which is upper triangular matrix. Hence, the formula is given by\n\n$$\nA =\nQR\n$$\n\nAs $Q$ is an orthogonal matrix, there are three methods to find $Q$, one is [Gramm-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process),\nsecond is [Householder Transformation](https://en.wikipedia.org/wiki/Householder_transformation),\nand third is [Givens Rotation](https://en.wikipedia.org/wiki/Givens_rotation). These methods are out of the scope of this blog post series and hence I'm going to explain all of them in a separate blog post.\n\n```{python}\n\nA = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n\n# QR decomposition\nQ, R = np.linalg.qr(A)\n\nprint(\"Q = \", Q, sep=\"\\n\")\nprint(\"R = \", R, sep=\"\\n\")\nprint(\"A = QR\", np.dot(Q, R), sep=\"\\n\")\n\n```\n\n\n## Eigen Decomposition\n\nHere is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform\nmatrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the\neigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n\n$$\nA = Q \\Lambda Q^{-1}\n$$\n\n$A$ is $n\\times n$ square matrix, $Q$ is the matrix whose columns are the eigenvectors, which in turn are linearly\nindependent and $\\Lambda$ is diagonal matrix of eigenvalues of $A$ and these eigenvalues are not necessarily distinct.\n\nTo see the detailed steps of this decomposition, consider the aforementioned example of the matrix $A$\nfor which we already found eigenvalues and eigenvectors.\n\n$$\nA =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 4 & 9\n\\end{bmatrix}\n$$\n$$\nQ =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n-2 & 0 & 1 \\\\\n1 & 0 & 2\n\\end{bmatrix}\n$$\n$$\n\\Lambda =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 11\n\\end{bmatrix}\n$$\n$$\nQ^{-1} =\n\\begin{bmatrix}\n0 & -0.4 & 0.2 \\\\\n1 & 0 & 0 \\\\\n0 & 0.2 & 0.4\n\\end{bmatrix}\n$$\n\nWe have all the matrices and now take matrix multiplication according to the above formula. Particularly,\nmultiply $Q$ by $\\Lambda$ and by $Q^{-1}$. We have to get original matrix $A$\n\nFurthermore, if matrix $A$ is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n\n$$\nA = Q \\Lambda Q^{T}\n$$\n\nThe only difference between this formula and above formula is that the matrix $A$ is $n\\times n$ real symmetric square\nmatrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real\nsymmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n\n$$\nA =\n\\begin{bmatrix}\n6 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n$$\n\nThe matrix is symmetric because of the original matrix equal to its transpose, $A = A^{T}$\n\nIts eigenvalues are $\\lambda_{1} = 7$ and $\\lambda_{2} = 2$ and corresponding eigenvectors are\n\n$$\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n0.89442719 \\\\\n0.4472136\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n-0.4472136 \\\\\n0.89442719\n\\end{bmatrix}\n$$\n\nAnd in this set up, matrices $Q$, $\\Lambda$ and $Q^{T}$ are the following:\n\n$$\nQ =\n\\begin{bmatrix}\n0.89442719 & -0.4472136 \\\\\n0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n$$\n$$\n\\Lambda = \n\\begin{bmatrix}\n7 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n$$\n$$\nQ^{T} =\n\\begin{bmatrix}\n0.89442719 & 0.4472136 \\\\\n-0.4472136 & 0.89442719 \\\\\n\\end{bmatrix}\n$$\n\nTaking matrix product gives initial matrix $A$.\n\n**Eigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.**\n\n```{python}\n\n# Eigendecomposition for non-symmetric matrix\nA = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\neigenvalues1, eigenvectors1 = np.linalg.eig(A)\n# Form diagonal matrix from eigenvalues\nL1 = np.diag(eigenvalues1)\n\n# Separate eigenvector matrix and take its inverse\nQ1 = eigenvectors1\ninv_Q = np.linalg.inv(Q1)\nB = np.dot(np.dot(Q1, L1), inv_Q)\n\n# Check if B equal to A\nprint(\"Decomposed matrix B: \", B, sep=\"\\n\")\n\n# Numpy produces normalized eigenvectors and don't be confused with my calculations above\n\n\n# Eigendecomposition for symmetric matrix\nC = np.array([[6, 2], [2, 3]])\neigenvalues2, eigenvectors2 = np.linalg.eig(C)\n\n# Eigenvalues and Eigenvectors\nL2 = np.diag(eigenvalues2)\nQ2 = eigenvectors2\nQ2_T = Q2.T\n\nD = np.dot(np.dot(Q2, L2), Q2.T)\n\n# Check if D equal to C\nprint(\"Decomposed matrix D: \", D, sep=\"\\n\")\n\n```\n\n\n## Singular Value Decomposition\n\nSingular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition.\nIn this context, generalization means that eigendecomposition is applicable only for square $n \\times n$ matrices,\nwhile Singular Value Decomposition (SVD) is applicable for any $m \\times n$ matrices.\n\nSVD for a $m \\times n$ matrix $A$ is computed by the following formula:\n\n$$\nA = U \\ D \\ V^{T}\n$$\n\nWhere, $U$'s columns are *left singular vectors* of $A$, $V$'s columns are *right singular vectors* of $A$ and $D$  is a\ndiagonal matrix, not necessarily square matrix, containing **singular values** of $A$ on main diagonal.\nSingular values of $m \\times n$ matrix $A$ are the **square roots of the eigenvalues** of $A^{T}A$, which is a square matrix.\nIf our initial matrix $A$ is square or $n \\times n$ then singular values **coincide** eigenvalues.\nMoreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\n\nMatrices, $U$, $D$, and $V$ can be found by transforming $A$ into a square matrix and computing eigenvalues and\neigenvectors of this transformed matrix. This transformation is done by multiplying $A$ by its transpose $A^{T}$.\nAfter that, matrices $U$, $D$ and $V$ are the following:\n\n* $U$ corresponds to the eigenvectors of $AA^{T}$\n\n* $V$ corresponds to eigenvectors of $A^{T}A$\n\n* $D$ corresponds to eigenvalues, either $AA^{T}$ or $A^{T}A$, which are the same\n\nTheory almost always seems confusing. Consider a numerical example and Python code below for clarification.\n\nLet our initial matrix $A$ be:\n\n$$\nA =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n\\sqrt{2} & 2 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n$$\n\nHere, to use SVD first we need to find $AA^{T}$ and $A^{T}A$.\n\n$$\nAA^{T} =\n\\begin{bmatrix}\n2 & 2 & 2 \\\\\n2 & 6 & 2 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\quad\nA^{T}A =\n\\begin{bmatrix}\n2 & 2\\sqrt{2} & 0 \\\\\n2\\sqrt{2} & 6 & 2 \\\\\n0 & 2 & 2\n\\end{bmatrix}\n$$\n\nIn the next step, we have to find eigenvalues and eigenvectors for $AA^{T}$ and $A^{T}A$. The characteristic polynomial is\n\n$$\n-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n$$\n\nwith roots equal to $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$. Note that these eigenvalues are\nthe same for the $A^{T}A$. We need singular values which are square root from eigenvalues.\nLet denote them by $\\sigma$ such as $\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_{2} = \\sqrt{2}$ and $\\sigma_{3} = \\sqrt{0} = 0$.\nWe now can construct diagonal matrix of singular values:\n\n$$\nD =\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n\nNow we have to find matrices $U$ and $V$. We have everything what we need. First find eigenvectors\nof $AA^{T}$ for $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$, which are the following:\n\n$$\nU_{1} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}}\\\\\n\\frac{2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{6}}\n\\end{bmatrix}\n\\quad\nU_{2} =\n\\begin{bmatrix}\n-\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}} \\\\\n-\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n\\quad\nU_{3} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}\\\\\n0 \\\\\n-\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\n\nNote that eigenvectors are normalized.\n\nAs we have eigenvectors, our $U$ matrix is:\n\n$$\nU =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\n\nIn the same fashion, we can find matrix $V$, which is:\n\n$$\nV =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n$$\n\nAccording to the formula we have\n\n$$\nA = U \\ D \\ V^{T} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n2\\sqrt{2} & 0 & 0 \\\\\n0 & \\sqrt{2} & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n\\end{bmatrix}\n^{T} = A\n$$\n\n```{python}\n\nnp.set_printoptions(suppress=True)  # Suppress scientific notation\n\nA = np.array([[0, 1, 0], [np.sqrt(2), 2, 0], [0, 1, 1]])\nU, D, V = np.linalg.svd(A)\n\nprint(\"U = \", U, sep=\"\\n\")\nprint(\"D = \", D, sep=\"\\n\")\nprint(\"V = \", V, sep=\"\\n\")\n\nB = np.dot(U, np.dot(np.diag(D), V))\nprint(\"B = \", B, sep=\"\\n\")\n\n```\n\n\n## Inverse of a Square Full Rank Matrix\n\nHere, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition.\nLet's get started. If a matrix $A$ can be eigendecomposed and it has no any eigenvalue equal to zero,\nthen this matrix has the inverse and this inverse is given by:\n\n$$\nA^{-1} =\nQ \\Lambda^{-1} Q^{-1}\n$$\n\nMatrices, $Q$, and $\\Lambda$ are already known for us. Consider an example:\n\n$$\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n4 & 3\n\\end{bmatrix}\n$$\n\nIts eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = 5$ and eigenvectors are:\n\n$$\nv_{\\lambda_{1}} =\n\\begin{bmatrix}\n-0.70710678 \\\\\n0.70710678\n\\end{bmatrix}\n\\quad\nv_{\\lambda_{2}} =\n\\begin{bmatrix}\n0.4472136 \\\\\n-0.89442719\n\\end{bmatrix}\n$$\n\nLet calculate the inverse of $A$\n\n$$\nA^{-1} = Q \\Lambda^{-1} Q^{-1} =\n$$\n$$\n=\n\\begin{bmatrix}\n-0.70710678 & -0.4472136 \\\\\n0.70710678 & -0.89442719\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-1 & -0 \\\\\n0 & 0.2\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n-0.94280904 & 0.47140452 \\\\\n-0.74535599 & -0.74535599\n\\end{bmatrix} =\n$$\n$$\n=\n\\begin{bmatrix}\n-0.6 & 0.4 \\\\\n0.8 & -0.2\n\\end{bmatrix}\n$$\n\n```{python}\n\nA = np.array([[1, 2], [4, 3]])\n\n# Eigenvalues and Eigenvectors\nL, Q = np.linalg.eig(A)\n# Diagonal eigenvalues\nL = np.diag(L)\n# Inverse\ninv_L = np.linalg.inv(L)\n# Inverse of igenvector matrix\ninv_Q = np.linalg.inv(Q)\n# Calculate the inverse of A\ninv_A = np.dot(Q, np.dot(inv_L, inv_Q))\n\n# Print the inverse\nprint(\"The inverse of A is: \", inv_A, sep=\"\\n\")\n\n```\n\n\n# Conclusion\n\nIn conclusion, my aim was to make linear algebra tutorials, which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books, which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant topics for machine learning and deep learning.\n\nBased on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should \nbe continuous. Meaning, that you have to practice more and more.\n\n\n#### References\n* [Cholesky Decomposition](https://rosettacode.org/wiki/Cholesky_decomposition)\n* [Matrix Decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition)\n* [Introduction To Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"linear_algebra_6.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.23","theme":["flatly"],"title-block-banner":true,"comments":{"giscus":{"repo":"Okroshiashvili/okrodata","reactions-enabled":true,"input-position":"bottom","theme":"light","language":"en","loading":"lazy"}},"title":"Advance Linear Algebra with Python - Part II","author":"Nodar Okroshiashvili","date":"2021-05-13","categories":["Mathematics"],"tags":["Linear Algebra","Advance Topics"],"keywords":["advance linear algebra","matrix decompositions in python","linear algebra advances in python","advance linear algebra for machine learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}